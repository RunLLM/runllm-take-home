[
    {
        "text": "LLMs 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow Retrieval Augmented Generation (RAG) Deploying Advanced LLMs with Custom PyFuncs in MLflow LLM Evaluation Examples Tutorial: Getting Started with ChatModel Tutorial: Custom GenAI Models using ChatModel Build a tool-calling model with mlflow.pyfunc.ChatModel MLflow Trace UI in Jupyter Notebook Demo MLflow Tracing MLflow Tracing for LLM Observability Tracing Concepts MLflow Tracing Schema Contributing to MLflow Tracing Searching and Retrieving Traces MLflow AI Gateway for LLMs MLflow AI Gateway (Experimental) Benefits of the MLflow AI Gateway Explore the Native Providers of the MLflow AI Gateway LLM Evaluation MLflow LLM Evaluation Benefits of MLflow\u00e2\u0080\u0099s LLM Evaluation Prompt Engineering UI Prompt Engineering UI (Experimental) Benefits of the MLflow Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor MLflow OpenAI Flavor MLflow Sentence-Transformers Flavor MLflow LangChain Flavor MLflow LlamaIndex Flavor MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow\u00e2\u0080\u0099s LLM Tracking Capabilities Benefits of the MLflow LLM Tracking System MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs  LLMs LLMs, or Large Language Models, have rapidly become a cornerstone in the machine learning domain, offering\nimmense capabilities ranging from natural language understanding to code generation and more.\nHowever, harnessing the full potential of LLMs often involves intricate processes, from interfacing with\nmultiple providers to fine-tuning specific models to achieve desired outcomes. Such complexities can easily become a bottleneck for developers and data scientists aiming to integrate LLM\ncapabilities into their applications. MLflow\u00e2\u0080\u0099s Support for LLMs aims to alleviate these challenges by introducing a suite of features and tools designed with the end-user in mind.  Tutorials and Use Case Guides for GenAI applications in MLflow Interested in learning how to leverage MLflow for your GenAI projects? Look in the tutorials and guides below to learn more about interesting use cases that could help to make your journey into leveraging GenAI a bit easier! Note that there are additional tutorials within the \u00e2\u0080\u009cExplore the Native LLM Flavors\u00e2\u0080\u009d section below , so be sure to check those out as well! Custom GenAI Models with ChatModel Learn the basics of creating custom chat models using MLflow's ChatModel by wrapping a local LLM provider. Advanced ChatModel Development Take an in-depth look at the full range of ChatModel features, such as tracing and configuration management. Tool Calling Models Learn how to build a simple tool-calling model using MLflow's ChatModel . Evaluating LLMs Learn how to evaluate LLMs with various metrics including LLM-as-a-Judge using mlflow.evaluate() API. Using Custom PyFunc with LLMs Explore the nuances of packaging, customizing, and deploying advanced LLMs in MLflow using custom PyFuncs. Evaluation for RAG Learn how to evaluate Retrieval Augmented Generation applications by leveraging LLMs to generate a evaluation dataset and evaluate it using the built-in metrics in the MLflow Evaluate API. ",
        "id": "9a6584e4041f69a91fb4d0445eb997e3"
    },
    {
        "text": " MLflow Tracing MLflow offers comprehensive tracing capabilities to monitor and analyze the execution of GenAI applications. This includes automated tracing GenAI frameworks such as\nLangChain, OpenAI, LlamaIndex, manual trace instrumentation using high-level fluent APIs, and low-level client APIs for fine-grained control. This functionality\nallows you to capture detailed trace data, enabling better debugging, performance monitoring, and insights into complex workflows.\nWhether through decorators, context managers, or explicit API calls, MLflow provides the flexibility needed to trace and optimize the operations\nof your GenAI models and retain your traced data within the tracking server for further analysis. Automated tracing with GenAI libraries : Seamless integration with libraries such as LangChain, OpenAI, LlamaIndex, and AutoGen, for automatic trace data collection. Manual trace instrumentation with high-level fluent APIs : Easy-to-use decorators and context managers for adding tracing with minimal code changes. Low-level client APIs for tracing : Thread-safe methods for detailed and explicit control over trace data management. To learn more about what tracing is, see our Tracing Concepts Overview guide. For an in-depth exploration into the structure of\nMLflow traces and their schema, see the Tracing Schema guide. If you\u00e2\u0080\u0099re interested in contributing to the development of MLflow Tracing, please refer to the Contribute to MLflow Tracing guide.  MLflow AI Gateway for LLMs Serving as a unified interface, the MLflow AI Gateway simplifies interactions with multiple LLM providers. In addition to supporting the most popular SaaS LLM providers, the MLflow AI Gateway\nprovides an integration to MLflow model serving, allowing you to serve your own LLM or a fine-tuned foundation model within your own serving infrastructure. Note The MLflow AI Gateway is in active development and has been marked as Experimental .\nAPIs may change as this new feature is refined and its functionality is expanded based on feedback.  Benefits of the MLflow AI Gateway Unified Endpoint : No more juggling between multiple provider APIs. Simplified Integrations : One-time setup, no repeated complex integrations. Secure Credential Management : Centralized storage prevents scattered API keys. No hardcoding or user-handled keys. Consistent API Experience : Uniform API across all providers. Easy-to-use REST endpoints and Client API. Seamless Provider Swapping : Swap providers without touching your code. Zero downtime provider, model, or route swapping.  Explore the Native Providers of the MLflow AI Gateway The MLflow AI Gateway supports a large range of foundational models from popular SaaS model vendors, as well as providing a means of self-hosting your\nown open source model via an integration with MLflow model serving. Please refer to Supported Providers for the full list of supported providers and models. If you\u00e2\u0080\u0099re interested in learning about how to set up the MLflow AI Gateway for a specific provider, follow the links below for our up-to-date\ndocumentation on GitHub. Each link will take you to a README file that will explain how to set up a route for the provider. In the same directory as\nthe README, you will find a runnable example of how to query the routes that the example creates, providing you with a quick reference for getting started\nwith your favorite provider! Note The MLflow and Hugging Face TGI providers are for self-hosted LLM serving of either foundation open-source LLM models, fine-tuned open-source\nLLM models, or your own custom LLM. The example documentation for these providers will show you how to get started with these, using free-to-use open-source\nmodels from the Hugging Face Hub . ",
        "id": "f1f051ca587dcdccce3cac1ecf442cc7"
    },
    {
        "text": " LLM Evaluation Navigating the vast landscape of Large Language Models (LLMs) can be daunting. Determining the right model, prompt, or service that aligns\nwith a project\u00e2\u0080\u0099s needs is no small feat. Traditional machine learning evaluation metrics often fall short when it comes to assessing the\nnuanced performance of generative models. Enter MLflow LLM Evaluation . This feature is designed to simplify the evaluation process,\noffering a streamlined approach to compare foundational models, providers, and prompts.  Benefits of MLflow\u00e2\u0080\u0099s LLM Evaluation Simplified Evaluation : Navigate the LLM space with ease, ensuring the best fit for your project with standard metrics that can be used to compare generated text. Use-Case Specific Metrics : Leverage MLflow\u00e2\u0080\u0099s mlflow.evaluate() API for a high-level, frictionless evaluation experience. Customizable Metrics : Beyond the provided metrics, MLflow supports a plugin-style for custom scoring, enhancing the evaluation\u00e2\u0080\u0099s flexibility. Comparative Analysis : Effortlessly compare foundational models, providers, and prompts to make informed decisions. Deep Insights : Dive into the intricacies of generative models with a comprehensive suite of LLM-relevant metrics. MLflow\u00e2\u0080\u0099s LLM Evaluation is designed to bridge the gap between traditional machine learning evaluation and the unique challenges posed by LLMs.  Prompt Engineering UI Effective utilization of LLMs often hinges on crafting the right prompts.\nThe development of a high-quality prompt is an iterative process of trial and error, where subsequent experimentation is not guaranteed to\nresult in cumulative quality improvements. With the volume and speed of iteration through prompt experimentation, it can quickly become very\noverwhelming to remember or keep a history of the state of different prompts that were tried. Serving as a powerful tool for prompt engineering, the MLflow Prompt Engineering UI revolutionizes the\nway developers interact with and refine LLM prompts.  Benefits of the MLflow Prompt Engineering UI Iterative Development : Streamlined process for trial and error without the overwhelming complexity. UI-Based Prototyping : Prototype, iterate, and refine prompts without diving deep into code. Accessible Engineering : Makes prompt engineering more user-friendly, speeding up experimentation. Optimized Configurations : Quickly hone in on the best model configurations for tasks like question answering or document summarization. Transparent Tracking : Every model iteration and configuration is meticulously tracked. Ensures reproducibility and transparency in your development process. Note The MLflow Prompt Engineering UI is in active development and has been marked as Experimental .\nFeatures and interfaces may evolve as feedback is gathered and the tool is refined.  Native MLflow Flavors for LLMs Harnessing the power of LLMs becomes effortless with flavors designed specifically for working with LLM libraries and frameworks. Native Support for Popular Packages : Standardized interfaces for tasks like saving, logging, and managing inference configurations. PyFunc Compatibility : Load models as PyFuncs for broad compatibility across serving infrastructures. Strengthens the MLOps process for LLMs, ensuring smooth deployments. Utilize the Models From Code feature for simplified GenAI application development. Cohesive Ecosystem : All essential tools and functionalities consolidated under MLflow. Focus on deriving value from LLMs without getting bogged down by interfacing and optimization intricacies. ",
        "id": "59cb0b50d17febda2430502dd4722c40"
    },
    {
        "text": " Explore the Native LLM Flavors Select the integration below to read the documentation on how to leverage MLflow\u00e2\u0080\u0099s native integration with these popular libraries: Learn about MLflow's native integration with the Transformers \u00f0\u009f\u00a4\u0097 library and see example notebooks that leverage\n                    MLflow and Transformers to build Open-Source LLM powered solutions. Learn about MLflow's native integration with the OpenAI SDK and see example notebooks that leverage\n                    MLflow and OpenAI's advanced LLMs to build interesting and fun applications. Learn about MLflow's native integration with the Sentence Transformers library and see example notebooks that leverage\n                    MLflow and Sentence Transformers to perform operations with encoded text such as semantic search, text similarity, and information retrieval. Learn about MLflow's native integration with LangChain and see example notebooks that leverage\n                    MLflow and LangChain to build LLM-backed applications. Learn about MLflow's native integration with LlamaIndex and see example notebooks that leverage\n                    MLflow and LlamaIndex to build advanced QA systems, chatbots, and other AI-driven applications. Learn about MLflow's native integration with DSPy and see example notebooks that leverage\n                    MLflow and DSPy to optimize your GenAI applications.  LLM Tracking in MLflow Empowering developers with advanced tracking capabilities, the MLflow LLM Tracking System stands out as the\npremier solution for managing and analyzing interactions with Large Language Models (LLMs).  Benefits of the MLflow LLM Tracking System Robust Interaction Management : Comprehensive tracking of every LLM interaction for maximum insight. Tailor-Made for LLMs : Unique features specifically designed for LLMs. From logging prompts to tracking dynamic data, MLflow has it covered. Deep Model Insight : Introduces \u00e2\u0080\u0098predictions\u00e2\u0080\u0099 as a core entity, alongside the existing artifacts, parameters, and metrics. Gain unparalleled understanding of text-generating model behavior and performance. Clarity and Repeatability : Ensures consistent and transparent tracking across all LLM interactions. Facilitates informed decision-making and optimization in LLM deployment and utilization. Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "6c35ad23722d1c744bfae761055fe566"
    },
    {
        "text": "Retrieval Augmented Generation (RAG) 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow Retrieval Augmented Generation (RAG) Benefits of RAG Understanding the Power of RAG Explore RAG Tutorials Deploying Advanced LLMs with Custom PyFuncs in MLflow LLM Evaluation Examples Tutorial: Getting Started with ChatModel Tutorial: Custom GenAI Models using ChatModel Build a tool-calling model with mlflow.pyfunc.ChatModel MLflow Trace UI in Jupyter Notebook Demo MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs Retrieval Augmented Generation (RAG)  Retrieval Augmented Generation (RAG) Retrieval Augmented Generation (RAG) is a powerful and efficient approach to natural\nlanguage processing that combines the strength of both pre-trained foundation models and\nretrieval mechanisms. It allows the generative model to access a dataset of documents\nthrough a retrieval mechanism, which enhances generated responses to be more contextually relevant\nand factually accurate. This improvement results in a cost-effective and accessible alternative\nto training custom models for specific use cases. The Retrieval mechanism works by embedding documents and questions in the same latent space, allowing\na user to ask a question and get the most relevant document chunk as a response. This mechanism then passes\nthe contextual chunk to the generative model, resulting in better quality responses with fewer hallucinations.  Benefits of RAG Provides LLM access to external knowledge through documents, resulting in contextually accurate and factual responses. RAG is more cost-effective than fine-tuning, since it doesn\u00e2\u0080\u0099t require the labeled data and computational resources that come with model training.  Understanding the Power of RAG In the realm of artificial intelligence, particularly within natural language processing, the ability to generate coherent\nand contextually relevant responses is paramount. Large language models (LLMs) have shown immense promise in this area,\nbut they often operate based on their internal knowledge, which can sometimes lead to inconsistencies or inaccuracies in\ntheir outputs. This is where RAG comes into play. RAG is a groundbreaking framework designed to enhance the capabilities of LLMs. Instead of solely relying on the vast but\nstatic knowledge embedded during their training, RAG empowers these models to actively retrieve and reference information\nfrom external knowledge bases. This dynamic approach ensures that the generated responses are not only rooted in the most\ncurrent and reliable facts but also transparent in their sourcing. In essence, RAG transforms LLMs from closed-book learners,\nrelying on memorized information, to open-book thinkers, capable of actively seeking out and referencing external knowledge. The implications of RAG are profound. By grounding responses in verifiable external sources, it significantly reduces the\nchances of LLMs producing misleading or incorrect information. Furthermore, it offers a more cost-effective solution for\nbusinesses, as there\u00e2\u0080\u0099s less need for continuous retraining of the model. With RAG, LLMs can provide answers that are not\nonly more accurate but also more trustworthy, paving the way for a new era of AI-driven insights and interactions.  Explore RAG Tutorials View RAG Tutorials Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "203826847759c3225780fd30769c6f8b"
    },
    {
        "text": "Deploying Advanced LLMs with Custom PyFuncs in MLflow 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow Retrieval Augmented Generation (RAG) Deploying Advanced LLMs with Custom PyFuncs in MLflow Explore the Tutorial LLM Evaluation Examples Tutorial: Getting Started with ChatModel Tutorial: Custom GenAI Models using ChatModel Build a tool-calling model with mlflow.pyfunc.ChatModel MLflow Trace UI in Jupyter Notebook Demo MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs Deploying Advanced LLMs with Custom PyFuncs in MLflow  Deploying Advanced LLMs with Custom PyFuncs in MLflow Advanced Large Language Models (LLMs) such as the MPT-7B instruct transformer are intricate and have requirements that don\u00e2\u0080\u0099t align with\ntraditional MLflow flavors. This demands a deeper understanding and the need for custom solutions. Note This tutorial utilizes the mlflow.pyfunc.PythonModel class to create a custom PyFunc model for building custom GenAI solutions.\nIt is recommended to utilize the newer mlflow.pyfunc.ChatModel class for building custom implementations due to a simplified\ndevelopment experience and an easier approach to deployment. To learn more, see the guide to building GenAI applications with ChatModel . Tip MLflow 2.12.2 introduced the feature \u00e2\u0080\u009cmodels from code\u00e2\u0080\u009d, which greatly simplifies the process of serializing and deploying custom models through the use\nof script serialization. While the tutorial here is valuable as a point of reference, we strongly recommend migrating custom model implementations to this\nnew paradigm. You can learn more about models from code within the Models From Code Guide . What\u00e2\u0080\u0099s in this tutorial? This guide is designed to provide insights into the deployment of advanced LLMs with MLflow, with a focus on using custom PyFuncs to address challenges: The World of LLMs : An introduction to LLMs, particularly models like the MPT-7B instruct transformer. We\u00e2\u0080\u0099ll delve into their intricacies, importance, and the challenges associated with their deployment. Why Custom PyFuncs for LLM Deployment? : We\u00e2\u0080\u0099ll explore the reasons behind the need for custom PyFuncs in the context of LLMs. How do they provide a bridge, ensuring that LLMs can be seamlessly deployed while adhering to MLflow\u00e2\u0080\u0099s standards? Managing Complex Behaviors : How custom PyFuncs can help in handling intricate model behaviors and dependencies that aren\u00e2\u0080\u0099t catered to by MLflow\u00e2\u0080\u0099s default flavors. Interface Data Manipulation : Delve into how custom PyFuncs allow the manipulation of interface data to generate prompts, thereby simplifying end-user interactions in a RESTful environment. Crafting Custom PyFuncs for LLM Deployment : A step-by-step walkthrough on how to define, manage, and deploy an LLM using a custom PyFunc. We\u00e2\u0080\u0099ll look at how to design a pyfunc to address LLM requirements and behaviors, and then how to deploy it using MLflow. Challenges with Traditional LLM Deployment : Recognize the issues and limitations when trying to deploy an advanced LLM using MLflow\u00e2\u0080\u0099s built-in capabilities. Understand why custom PyFuncs become essential in such scenarios. By the conclusion of this guide, you\u00e2\u0080\u0099ll possess a deep understanding of how to deploy advanced LLMs in MLflow using custom PyFuncs. You\u00e2\u0080\u0099ll appreciate the role of custom PyFuncs in making complex deployments streamlined and efficient. ",
        "id": "6bccd3e1067c740c727c93fda2a0cb19"
    },
    {
        "text": " Explore the Tutorial Serving LLMs with MLflow: Leveraging Custom PyFunc Learn how to use the MLflow Custom Pyfunc Model to serve Large Language Models (LLMs) in a RESTful environment. Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "e9910bd2b33f383e7b78340ff2b1a535"
    },
    {
        "text": "LLM Evaluation Examples 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow Retrieval Augmented Generation (RAG) Deploying Advanced LLMs with Custom PyFuncs in MLflow LLM Evaluation Examples LLM Evaluation with MLflow Example Notebook LLM RAG Evaluation with MLflow Example Notebook LLM RAG Evaluation with MLflow using llama2-as-judge Example Notebook Create a RAG system Evaluate the RAG system using mlflow.evaluate() Evaluate a Hugging Face LLM with mlflow.evaluate() QA Evaluation Tutorial RAG Evaluation Tutorials Tutorial: Getting Started with ChatModel Tutorial: Custom GenAI Models using ChatModel Build a tool-calling model with mlflow.pyfunc.ChatModel MLflow Trace UI in Jupyter Notebook Demo MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs LLM Evaluation Examples  LLM Evaluation Examples The notebooks listed below contain step-by-step tutorials on how to use MLflow to evaluate LLMs.\nThe first notebook is centered around evaluating an LLM for question-answering with a\nprompt engineering approach. The second notebook is centered around evaluating a RAG system.\nBoth notebooks will demonstrate how to use MLflow\u00e2\u0080\u0099s builtin metrics such as token_count and\ntoxicity as well as LLM-judged intelligent metrics such as answer_relevance. The third notebook\nis the same as the second notebook, but uses Databricks\u00e2\u0080\u0099s served llama2-70b as the judge instead\nof gpt-4.  QA Evaluation Tutorial LLM Question Answering Evaluation with MLflow Learn how to evaluate various LLMs and RAG systems with MLflow, leveraging simple metrics such as toxicity, as well as LLM-judged metrics as relevance, and even custom LLM-judged metrics such as professionalism. Evaluating a \u00f0\u009f\u00a4\u0097 Hugging Face LLMs with MLflow Learn how to evaluate various Open-Source LLMs available in Hugging Face, leveraging MLflow's built-in LLM metrics and experiment tracking to manage models and evaluation results.  RAG Evaluation Tutorials RAG Evaluation with MLflow and GPT-4 as Judge Learn how to evaluate RAG systems with MLflow, leveraging OpenAI GPT-4 model as a judge. RAG Evaluation with MLflow and Llama-2-70B as Judge Learn how to evaluate RAG systems with MLflow, leveraging Llama 2 70B model hosted on Databricks serving endpoint. Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "619de8a37f3e1e07bcd78cd1496d6fc6"
    },
    {
        "text": "Tutorial: Getting Started with ChatModel 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow Retrieval Augmented Generation (RAG) Deploying Advanced LLMs with Custom PyFuncs in MLflow LLM Evaluation Examples Tutorial: Getting Started with ChatModel What You\u00e2\u0080\u0099ll Learn Prerequisites Understanding ChatModel: Input/Output Mapping Building Your First ChatModel Building a ChatModel that Accepts Inference Parameters Comparison to PyFunc Conclusion Tutorial: Custom GenAI Models using ChatModel Build a tool-calling model with mlflow.pyfunc.ChatModel MLflow Trace UI in Jupyter Notebook Demo MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs Tutorial: Getting Started with ChatModel  Tutorial: Getting Started with ChatModel MLflow\u00e2\u0080\u0099s ChatModel class provides a standardized way to create production-ready conversational AI models. The resulting models are fully integrated with MLflow\u00e2\u0080\u0099s tracking, evaluation, and lifecycle management capabilities. They can be shared with others in the MLflow Model Registry, deployed as a REST API, or loaded in a notebook for interactive use. Furthermore, they are compatible with the widely-adopted OpenAI chat API spec, making them easy to integrate with other AI systems and tools. If you\u00e2\u0080\u0099re already familiar with PythonModel , you might wonder why ChatModel is needed. As GenAI applications grow more complex, mapping inputs, outputs, and parameters with a custom PythonModel can be challenging. ChatModel simplifies this by offering a structured, OpenAI-compatible schema for conversational AI models. ChatModel PythonModel When to use Use when you want to develop and deploy a conversational model with standard chat schema compatible with OpenAI spec. Use when you want full control over the model\u00e2\u0080\u0099s interface or customize every aspect of your model\u00e2\u0080\u0099s behavior. Interface Fixed to OpenAI\u00e2\u0080\u0099s chat schema. Full control over the model\u00e2\u0080\u0099s input and output schema. Setup Quick . Works out of the box for conversational applications, with pre-defined model signature and input example. Custom . You need to define model signature or input example yourself. Complexity Low . Standardized interface simplified model deployment and integration. High . Deploying and integrating the custom PythonModel may not be straightforward. E.g., The model needs to handle Pandas DataFrames as MLflow converts input data to DataFrames before passing it to PythonModel.  What You\u00e2\u0080\u0099ll Learn This guide will take you through the basics of using the ChatModel API to define custom conversational AI models. In particular, you will learn: How to map your application logic to the ChatModel \u00e2\u0080\u0099s input/output schema How to use the pre-defined inference parameters supported by ChatModels How to pass custom parameters to a ChatModel using custom_inputs How ChatModel compares to PythonModel for defining custom chat models To illustrate these points, this guide will walk you through building a custom ChatModel , using a locally-hosted Ollama model as our example. There is no built-in Ollama model flavor, so creating a custom ChatModel provides a way to use MLflow\u00e2\u0080\u0099s extensive tracking, evaluation, and lifecycle management capabilities with Ollama models.  Prerequisites Familiarity with MLflow logging APIs and GenAI concepts. MLflow version 2.17.0 or higher installed for use of ChatModel . ",
        "id": "6996c19a26efa0f0756bc3d48d363332"
    },
    {
        "text": " Understanding ChatModel: Input/Output Mapping The mlflow.pyfunc.ChatModel interface sits between your application and MLflow\u00e2\u0080\u0099s ecosystem, providing a layer of standardization that makes it easier to integrate your application with MLflow\u00e2\u0080\u0099s other features and to deploy your model in an accessible, production-ready format. To that end, when defining a custom ChatModel , the key task is to map your application\u00e2\u0080\u0099s logic to the ChatModel \u00e2\u0080\u0099s standardized interface. This mapping exercise is the fundamental part of creating a custom ChatModel . When using a custom ChatModel, the predict method expects standardized inputs that look like this: input = { \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"What is MLflow?\" }], \"max_tokens\" : 25 , } with a messages key containing a list of messages, and optional inference parameters such as max_tokens , temperature , top_p , and stop . You can find details of the full chat request object here . The output is also returned in a standardized format that looks like this: { \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"MLflow is an open-source platform for machine learning (ML) and artificial intelligence (AI). It's designed to manage,\" , }, \"finish_reason\" : \"stop\" , } ], \"model\" : \"llama3.2:1b\" , \"object\" : \"chat.completion\" , \"created\" : 1729190863 , } You can find details of the full chat response object here . These input/output schemas are compatible with the widely-adopted OpenAI spec, making ChatModel s easy to use in a wide variety of contexts. To demonstrate this mapping process, we will show how to use the mlflow.pyfunc.ChatModel class to log Meta\u00e2\u0080\u0099s Llama 3.2 1B model via the Ollama llm client, which does not have a native MLflow flavor. ",
        "id": "f2647c0d3b5868b7963205a1fa3abcf6"
    },
    {
        "text": " Building Your First ChatModel In this section, we will wrap a locally-hosted Ollama model with the ChatModel interface. We will build a simplified version showing how to handle inputs and outputs, and then we will show how to handle inference parameters such as max_tokens and temperature . Setup: Install Ollama and download the model Install Ollama from here . Once Ollama is installed and running, download the Llama 3.2 1B model by running ollama pull llama3.2:1b You can validate that the model is downloaded and available on your system with ollama run llama3.2:1b . > ollama run llama3.2:1b\n\n>>> Hello world!\nHello! It's great to see you're starting the day with a cheerful greeting. How can I assist you today?\n>>> Send a message (/? for help) We will use the ollama-python library to interface with the Ollama model. Install it to your Python environment with pip install ollama . Also, install mlflow with pip install mlflow . Using the Ollama Python library In order to map the Ollama input/output schema to the ChatModel input/output schema, we first need to understand what kinds of inputs and outputs the Ollama model expects and returns. Here\u00e2\u0080\u0099s how to query the model with a simple prompt: import ollama from ollama import Options from rich import print response = ollama . chat ( model = \"llama3.2:1b\" , messages = [ { \"role\" : \"user\" , \"content\" : \"What is MLflow Tracking?\" , } ], options = Options ({ \"num_predict\" : 25 }), ) print ( response ) Which returns the following output: {\n    'model': 'llama3.2:1b',\n    'created_at': '2024-11-04T12:47:53.075714Z',\n    'message': {\n        'role': 'assistant',\n        'content': 'MLflow Tracking is an open-source platform for managing, monitoring, and deploying machine learning (ML) models. It provides a'\n    },\n    'done_reason': 'length',\n    'done': True,\n    'total_duration': 1201354125,\n    'load_duration': 819609167,\n    'prompt_eval_count': 31,\n    'prompt_eval_duration': 41812000,\n    'eval_count': 25,\n    'eval_duration': 337872000\n} Here are a few things to note about the Ollama inputs and outputs: The messages argument expected by the ollama.chat method is a list of dictionaries with role and content keys. We will need to convert the list of ChatMessage objects expected by the ChatModel API to a list of dictionaries. Inference parameters are passed to Ollama via the options argument, which is a dictionary of parameters. Furthermore, as we can see based on num_predict , the parameter names are different from those expected by ChatModel. We will need to map the ChatModel inference parameters to the Ollama options. The output is structured differently from the ChatModel output schema. We will need to map this to the ChatModel output schema. Ollama ChatModel Version 1: Chat only Let\u00e2\u0080\u0099s start with a simple version of a custom ChatModel that handles inputs/output messages but does not yet handle inference parameters. To accomplish this, we need to: Define a class that extends mlflow.pyfunc.ChatModel Implement the load_context method, which will handle the initialization of the Ollama client Implement the predict method, which will handle the input/output mapping Most of the customization, at least in this simple version, will occur in the predict method. When implementing the predict method, we make use of the following standardized inputs: messages : a list of ChatMessage objects params : a ChatParams object, which contains the inference parameters And we need to return a ChatCompletionResponse object, which is a dataclass made up of a list of ChatChoice objects,",
        "id": "58288d0c7b45068356da797233c08856"
    },
    {
        "text": "customization, at least in this simple version, will occur in the predict method. When implementing the predict method, we make use of the following standardized inputs: messages : a list of ChatMessage objects params : a ChatParams object, which contains the inference parameters And we need to return a ChatCompletionResponse object, which is a dataclass made up of a list of ChatChoice objects, along with (optional) usage data and other metadata. These are what we must map to the Ollama inputs and outputs. Here\u00e2\u0080\u0099s a simplified version that, for now, only handles the input/output messages: # if you are using a jupyter notebook # %%writefile ollama_model.py from mlflow.pyfunc import ChatModel from mlflow.types.llm import ChatMessage , ChatCompletionResponse , ChatChoice from mlflow.models import set_model import ollama class SimpleOllamaModel ( ChatModel ): def __init__ ( self ): self . model_name = \"llama3.2:1b\" self . client = None def load_context ( self , context ): self . client = ollama . Client () def predict ( self , context , messages , params = None ): # Prepare the messages for Ollama ollama_messages = [ msg . to_dict () for msg in messages ] # Call Ollama response = self . client . chat ( model = self . model_name , messages = ollama_messages ) # Prepare and return the ChatCompletionResponse return ChatCompletionResponse ( choices = [{ \"index\" : 0 , \"message\" : response [ \"message\" ]}], model = self . model_name , ) set_model ( SimpleOllamaModel ()) In the above code, we mapped the ChatModel inputs to the Ollama inputs, and the Ollama output back to the ChatModel output schema. More specifically: The messages key in the ChatModel input schema is a list of ChatMessage objects. We converted this to a list of dictionaries with role and content keys, which is the expected input format for Ollama. The ChatCompletionResponse that the predict method returns must be created using the ChatCompletionResponse dataclass, but the nested message and choice data can be provided as dictionaries that match the expected schema. MLflow will automatically convert these dictionaries to the appropriate dataclass objects. In our case, we created a ChatCompletionResponse but provided the choices and messages as dictionaries. In a notebook environment, we can save the model to a file called ollama_model.py with the %%writefile magic command and call set_model(SimpleOllamaModel()) . This is the \u00e2\u0080\u009cmodels from code\u00e2\u0080\u009d approach to model logging, which you can read more about here . Now we can log this model to MLflow as follows, passing the path to the file containing the model definition we just created: import mlflow mlflow . set_experiment ( \"chatmodel-quickstart\" ) code_path = \"ollama_model.py\" with mlflow . start_run (): model_info = mlflow . pyfunc . log_model ( \"ollama_model\" , python_model = code_path , input_example = { \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"Hello, how are you?\" }] }, ) Again, we used the models-from-code approach to log the model, so we passed the path to the file containing our model definition to the python_model parameter. Now we can load the model and try it out: loaded_model = mlflow . pyfunc . load_model ( model_info . model_uri ) result = loaded_model . predict ( data = { \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"What is MLflow?\" }], \"max_tokens\" : 25 , } ) print ( result ) { \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"MLflow is an open-source platform for model deployment, monitoring, and tracking. It was created by Databricks, a cloud-based da",
        "id": "94bf89f750c45911e089131c6016480e"
    },
    {
        "text": "d_model ( model_info . model_uri ) result = loaded_model . predict ( data = { \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"What is MLflow?\" }], \"max_tokens\" : 25 , } ) print ( result ) { \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"MLflow is an open-source platform for model deployment, monitoring, and tracking. It was created by Databricks, a cloud-based data analytics company, in collaboration with The Data Science Experience (TDEE), a non-profit organization that focuses on providing high-quality, free machine learning resources. \\n\\n MLflow allows users to build, train, and deploy machine learning models in various frameworks, such as TensorFlow, PyTorch, and scikit-learn. It provides a unified platform for model development, deployment, and tracking across different environments, including local machines, cloud platforms (e.g., AWS), and edge devices. \\n\\n Some key features of MLflow include: \\n\\n 1. **Model versioning**: Each time a model is trained or deployed, it generates a unique version number. This allows users to track changes, identify conflicts, and manage multiple versions. \\n 2. **Model deployment**: MLflow provides tools for deploying models in various environments, including Docker containers, Kubernetes, and cloud platforms (e.g., AWS). \\n 3. **Monitoring and logging**: The platform includes built-in monitoring and logging capabilities to track model performance, errors, and other metrics. \\n 4. **Integration with popular frameworks**: MLflow integrates with popular machine learning frameworks, making it easy to incorporate the platform into existing workflows. \\n 5. **Collaboration and sharing**: MLflow allows multiple users to collaborate on models and tracks changes in real-time. \\n\\n MLflow has several benefits, including: \\n\\n 1. **Improved model management**: The platform provides a centralized view of all models, allowing for better model tracking and management. \\n 2. **Increased collaboration**: MLflow enables team members to work together on machine learning projects more effectively. \\n 3. **Better model performance monitoring**: The platform offers real-time insights into model performance, helping users identify issues quickly. \\n 4. **Simplified model deployment**: MLflow makes it easy to deploy models in various environments, reducing the complexity of model deployment. \\n\\n Overall, MLflow is a powerful tool for managing and deploying machine learning models, providing a comprehensive platform for model development, tracking, and collaboration.\" , }, \"finish_reason\" : \"stop\" , } ], \"model\" : \"llama3.2:1b\" , \"object\" : \"chat.completion\" , \"created\" : 1730739510 , } Now we have received a chat response in a standardized, OpenAI-compatible format. But something is wrong: even though we set max_tokens to 25, the response is well over 25 tokens! Why is this? We have not yet handled the inference parameters in our custom ChatModel: in addition to mapping the input/output messages between the ChatModel and Ollama formats, we also need to map the inference parameters between the two formats. We will address this in the next version of our custom ChatModel. ",
        "id": "f6b16ef20174cdfe00920098a1a88ea0"
    },
    {
        "text": " Building a ChatModel that Accepts Inference Parameters Most LLMs support inference parameters that control how the response is generated, such as max_tokens , which limits the number of tokens in the response, or temperature , which adjusts the \u00e2\u0080\u009ccreativity\u00e2\u0080\u009d of the response. The ChatModel API includes built-in support for many of the most commonly-used inference parameters, and we will see how to configure and use them in this section. Passing Parameters to a ChatModel When using a ChatModel, parameters are passed alongside messages in the input: result = model . predict ( { \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"Write a story\" }], \"max_tokens\" : 100 , \"temperature\" : 0.7 , } ) You can find the full list of supported parameters here . Furthermore, you can pass arbitrary additional parameters to a ChatModel via the custom_inputs key in the input, which we will cover in more detail in the next section. Comparison to Parameter Handling in Custom PyFunc Models If you\u00e2\u0080\u0099re familiar with configuring inference parameters for PyFunc models , you will notice some key differneces in how ChatModel handles parameters: ChatModel PyFunc Parameters are part of the data dictionary passed to predict , which also includes the messages key Parameters are passed to predict as params keyword argument Commonly-used chat model parameters (e.g. max_tokens , temperature , top_p ) are pre-defined in the ChatModel class Parameters are chosen and configured by the developer Model signature is automatically configured to support the common chat model parameters Parameters must be explicitly defined in the model signature In short, ChatModels make it easy to configure and use inference parameters, while also providing a standardized, OpenAI-compatible output format, but at the cost of some flexibility. Now, let\u00e2\u0080\u0099s configure our custom ChatModel to handle inference parameters. Ollama ChatModel Version 2: Chat with inference parameters Setting up a ChatModel with inference parameters is straightforward: just like with the input messages, we need to map the inference parameters to the format expected by the Ollama client. In the Ollama client, inference parameters are passed to the model as an options dictionary. When defining our custom ChatModel, we can access the inference parameters passed to predict via the params keyword argument. Our job is to map the predict method\u00e2\u0080\u0099s params dictionary to the Ollama client\u00e2\u0080\u0099s options dictionary. You can find the list of options supported by Ollama here . # if you are using a jupyter notebook # %%writefile ollama_model.py import mlflow from mlflow.pyfunc import ChatModel from mlflow.types.llm import ChatMessage , ChatCompletionResponse , ChatChoice from mlflow.models import set_model import ollama from ollama import Options class OllamaModelWithMetadata ( ChatModel ): def __init__ ( self ): self . model_name = None self . client = None def load_context ( self , context ): self . model_name = \"llama3.2:1b\" self . client = ollama . Client () def _prepare_options ( self , params ): # Prepare options from params options = {} if params : if params . max_tokens is not None : options [ \"num_predict\" ] = params . max_tokens if params . temperature is not None : options [ \"temperature\" ] = params . temperature if params . top_p is not None : options [ \"top_p\" ] = params . top_p if params . stop is not None : options [ \"stop\" ] = params . stop if params . custom_inputs is not None : options [ \"seed\" ] = int ( params . custom_inputs . get ( \"seed\" , None )) return Options ( options ) def predict ( self , context , messages , params = None ): ollama_messages = [ { \"role\" : msg . role , \"content\" : msg . content } for msg in messages ] options = self . _prepare_options ( params ) # Call Ollama response = self . client . chat ( model = self . model_name , messages = ollama_messages , options = options ) # Prepare the ChatCompletionResponse return ChatCompletionResponse ( choices = [{ \"index\" : 0 , \"message\" : response [ \"message\" ]}], model = self . model_name , ) set_model ( OllamaModelWithMetadata ()) Here\u00e2\u0080\u0099s what we changed from the previous version: We mapped max_tokens , temperature , top_p , and stop from the params dictionary to num_predict , temperature , top_p , and stop in the Ollama client\u00e2\u0080\u0099s options dictionary (note the different parameter name for max_tokens expected by Ollama) We passed the options dictionary to the Ollama client\u00e2\u0080\u0099s chat method. Note that we created a new private method, _prepare_options , to handle the mapping from params to opt",
        "id": "ef1ae81380f9bf903bc82e6db943cdd1"
    },
    {
        "text": "t_model ( OllamaModelWithMetadata ()) Here\u00e2\u0080\u0099s what we changed from the previous version: We mapped max_tokens , temperature , top_p , and stop from the params dictionary to num_predict , temperature , top_p , and stop in the Ollama client\u00e2\u0080\u0099s options dictionary (note the different parameter name for max_tokens expected by Ollama) We passed the options dictionary to the Ollama client\u00e2\u0080\u0099s chat method. Note that we created a new private method, _prepare_options , to handle the mapping from params to options . Additional methods can be added to a custom ChatModel to keep code clean and organized while handling custom logic. We checked the custom_inputs key in the params dictionary for a seed value\u00e2\u0080\u0094we\u00e2\u0080\u0099ll cover this in more detail in the next section. Now we can log this model to MLflow, load it, and try it out in the same way as before: code_path = \"ollama_model.py\" with mlflow . start_run (): model_info = mlflow . pyfunc . log_model ( \"ollama_model\" , python_model = code_path , input_example = { \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"Hello, how are you?\" }] }, ) loaded_model = mlflow . pyfunc . load_model ( model_info . model_uri ) result = loaded_model . predict ( data = { \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"What is MLflow?\" }], \"max_tokens\" : 25 , } ) print ( result ) Which returns: { \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"MLflow is an open-source platform that provides a set of tools for managing and tracking machine learning (ML) model deployments,\" , }, \"finish_reason\" : \"stop\" , } ], \"model\" : \"llama3.2:1b\" , \"object\" : \"chat.completion\" , \"created\" : 1730724514 , } Now that we have appropriately mapped max_tokens from the ChatModel input schema to the Ollama client\u00e2\u0080\u0099s num_predict parameter, we receive a response with the expected number of tokens. Passing Custom Parameters What if we want to pass a custom parameter that is not included in the list of built-in inference parameters? The ChatModel API provides a way to do this via the custom_inputs key, which accepts a dictionary of key-value pairs that are passed through to the model as-is. Both the keys and values must be strings, so it might be necessary to handle type conversions in the predict method. In the above example, we configured the Ollama model to use a custom seed value by adding a seed key to the custom_inputs dictionary: if params . custom_inputs is not None : options [ \"seed\" ] = int ( params . custom_inputs . get ( \"seed\" , None )) Because we included this, we can now pass a seed value via the custom_inputs key in the predict method. If you call predict multiple times with the same seed value, you will always receive the same response. result = loaded_model . predict ( data = { \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"What is MLflow?\" }], \"max_tokens\" : 25 , \"custom_inputs\" : { \"seed\" : \"321\" }, } ) print ( result ) Which returns: { \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"MLflow is an open-source software framework used for machine learning model management, monitoring, and deployment. It's designed to provide\" , }, \"finish_reason\" : \"stop\" , } ], \"model\" : \"llama3.2:1b\" , \"object\" : \"chat.completion\" , \"created\" : 1730724533 , } Tip Using vs. Defining ChatModels There\u00e2\u0080\u0099s an important distinction between how you pass data when using a ChatModel versus how you access that data when defining one. When using an instantiated ChatModel, all the arguments\u00e2\u0080\u0094messages, parameters, etc.\u00e2\u0080\u0094are passed to the predict method as a single dictionary. model . predict ({ \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"Hello\" }], \"temperature\" : 0.7 }) When defining the custom ChatModel\u00e2\u0080\u0099s predict method, on the other hand, we access the data through separate messages and params arguments, where messages is a list of ChatMessage objects and params is a ChatParams object. Understanding this distinction\u00e2\u0080\u0094unified input for users, structured access for developers\u00e2\u0080\u0094is important to working effectively with ChatModels. ",
        "id": "18bba1efebbeb86dd3d17cc2e0fe73eb"
    },
    {
        "text": " Comparison to PyFunc To illustrate some of the benefits and trade-offs of setting up a chat model via the ChatModel API vs. the PythonModel API, let\u00e2\u0080\u0099s see what the above model would look like if we implemented it as a PythonModel . Ollama Model Version 3: Custom PyFunc Model # if you are using a jupyter notebook # %%writefile ollama_pyfunc_model.py import mlflow from mlflow.pyfunc import PythonModel from mlflow.types.llm import ( ChatCompletionRequest , ChatCompletionResponse , ChatMessage , ChatChoice , ) from mlflow.models import set_model import ollama from ollama import Options import pandas as pd from typing import List , Dict class OllamaPyfunc ( PythonModel ): def __init__ ( self ): self . model_name = None self . client = None def load_context ( self , context ): self . model_name = \"llama3.2:1b\" self . client = ollama . Client () def _prepare_options ( self , params ): options = {} if params : if \"max_tokens\" in params : options [ \"num_predict\" ] = params [ \"max_tokens\" ] if \"temperature\" in params : options [ \"temperature\" ] = params [ \"temperature\" ] if \"top_p\" in params : options [ \"top_p\" ] = params [ \"top_p\" ] if \"stop\" in params : options [ \"stop\" ] = params [ \"stop\" ] if \"seed\" in params : options [ \"seed\" ] = params [ \"seed\" ] return Options ( options ) def predict ( self , context , model_input , params = None ): if isinstance ( model_input , ( pd . DataFrame , pd . Series )): messages = model_input . to_dict ( orient = \"records\" )[ 0 ][ \"messages\" ] else : messages = model_input . get ( \"messages\" , []) options = self . _prepare_options ( params ) ollama_messages = [ { \"role\" : msg [ \"role\" ], \"content\" : msg [ \"content\" ]} for msg in messages ] response = self . client . chat ( model = self . model_name , messages = ollama_messages , options = options ) chat_response = ChatCompletionResponse ( choices = [ ChatChoice ( index = 0 , message = ChatMessage ( role = \"assistant\" , content = response [ \"message\" ][ \"content\" ] ), ) ], model = self . model_name , ) return chat_response . to_dict () set_model ( OllamaPyfunc ()) This looks quite similar to how we defined our ChatModel above, and you could in fact use this PythonModel to serve the same Ollama model. However, there are some important differences: We had to handle the input data as a pandas DataFrame, even though the input is ultimately just a list of messages. Instead of receiving the inference parameters as a pre-configured ChatParams object, receive a params dictionary. One consequence of this is that we did not have to treat seed any differently from the other inference parameters: they\u00e2\u0080\u0099re all custom parameters in the PythonModel API. We had to call chat_response.to_dict() to convert the ChatCompletionResponse object to a dictionary rather than a ChatCompletionResponse object. This is handled automatically by ChatModel . Some of the biggest differences come up when it\u00e2\u0080\u0099s t",
        "id": "25c4ff9ef3b0013badd815f982ac3fbe"
    },
    {
        "text": "r inference parameters: they\u00e2\u0080\u0099re all custom parameters in the PythonModel API. We had to call chat_response.to_dict() to convert the ChatCompletionResponse object to a dictionary rather than a ChatCompletionResponse object. This is handled automatically by ChatModel . Some of the biggest differences come up when it\u00e2\u0080\u0099s time to log the model: code_path = \"ollama_pyfunc_model.py\" params = { \"max_tokens\" : 25 , \"temperature\" : 0.5 , \"top_p\" : 0.5 , \"stop\" : [ \" \\n \" ], \"seed\" : 123 , } request = { \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"What is MLflow?\" }]} with mlflow . start_run (): model_info = mlflow . pyfunc . log_model ( \"ollama_pyfunc_model\" , python_model = code_path , input_example = ( request , params ), ) With a custom PythonModel , we need to manually define the input example so that a model signature can be inferred using the example. This is a significant difference from the ChatModel API, which automatically configures a signature that conforms to the standard OpenAI-compatible input/output/parameter schemas.\nTo learn more about auto inference of model signature based on an input example, see the GenAI model signature example section for details. There is also one notable difference in how we call the loaded model\u00e2\u0080\u0099s predict method: parameters are passed as a dictionary via the params keyword argument, rather than in the dictionary containing the messages. loaded_model = mlflow . pyfunc . load_model ( model_info . model_uri ) result = loaded_model . predict ( data = { \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"What is MLflow?\" }]}, params = { \"max_tokens\" : 25 , \"seed\" : 42 }, ) print ( result ) Which returns: { \"choices\" : [ { \"index\" : 0 , \"message\" : { \"role\" : \"assistant\" , \"content\" : \"MLflow is an open-source platform for machine learning (ML) and deep learning (DL) model management, monitoring, and\" , }, \"finish_reason\" : \"stop\" , } ], \"model\" : \"llama3.2:1b\" , \"object\" : \"chat.completion\" , \"created\" : 1731000733 , } In summary, ChatModel provides a more structured approach to defining custom chat models, with a focus on standardized, OpenAI-compatible inputs and outputs. While it requires a bit more setup work to map the input/output schemas between the ChatModel schema and the application it wraps, it can be easier to use than a fully custom PythonModel as it handles the often-challenging task of defining input/output/parameter schemas. The PythonModel approach, on the other hand, provides the most flexibility but requires the developer to manually handle all of the input/output/parameter mapping logic. ",
        "id": "c83b6b0219ee3cc847564d5d2e6da0ce"
    },
    {
        "text": " Conclusion In this guide, you have learned: How to map the input/output schemas between the ChatModel API and your application How to configure commonly-used chat model inference parameters with the ChatModel API How to pass custom parameters to a ChatModel using the custom_inputs key How ChatModel compares to the PythonModel for defining custom chat models You should now have a good sense of what the ChatModel API is and how it can be used to define custom chat models. ChatModel includes some additional functionality that was not covered in this introductory guide, including: Out of the box support for MLflow Tracing, which is useful for debugging and monitoring your chat models, especially in models with multiple components or calls to LLM APIs. Support for customizing the model\u00e2\u0080\u0099s configuration using an external configuration file. To learn more about these and other advanced features of the ChatModel API, you can read this guide . Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "6380156d0aa6452be0a793c273a2934f"
    },
    {
        "text": "Tutorial: Custom GenAI Models using ChatModel 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow Retrieval Augmented Generation (RAG) Deploying Advanced LLMs with Custom PyFuncs in MLflow LLM Evaluation Examples Tutorial: Getting Started with ChatModel Tutorial: Custom GenAI Models using ChatModel Choosing Between ChatModel and PythonModel Purpose of this tutorial Prerequisites Core Concepts Key Classes and Methods in our example Example of a custom ChatModel Setting our model_config values Defining an Input Example Logging and Loading our custom Agent Conclusion Build a tool-calling model with mlflow.pyfunc.ChatModel MLflow Trace UI in Jupyter Notebook Demo MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs Tutorial: Custom GenAI Models using ChatModel  Tutorial: Custom GenAI Models using ChatModel The rapidly evolving landscape of Generative Artificial Intelligence (GenAI) presents exciting opportunities and integration challenges.\nTo leverage the latest GenAI advancements effectively, developers need a framework that balances flexibility with standardization.\nMLflow addresses this need with the mlflow.pyfunc.ChatModel class introduced in version 2.11.0 , providing a\nconsistent interface for GenAI applications while simplifying deployment and testing.  Choosing Between ChatModel and PythonModel When building GenAI applications in MLflow, it\u00e2\u0080\u0099s essential to choose the right model abstraction that balances ease of use with the level of\ncustomization you need. MLflow offers two primary classes for this purpose: mlflow.pyfunc.ChatModel and mlflow.pyfunc.PythonModel . Each has its own strengths and trade-offs, making it crucial to understand which one best suits your use case. ChatModel PythonModel When to use Use when you want to develop and deploy a conversational model with standard chat schema compatible with OpenAI spec. Use when you want full control over the model\u00e2\u0080\u0099s interface or customize every aspect of your model\u00e2\u0080\u0099s behavior. Interface Fixed to OpenAI\u00e2\u0080\u0099s chat schema. Full control over the model\u00e2\u0080\u0099s input and output schema. Setup Quick . Works out of the box for conversational applications, with pre-defined model signature and input example. Custom . You need to define model signature or input example yourself. Complexity Low . Standardized interface simplified model deployment and integration. High . Deploying and integrating the custom PythonModel may not be straightforward. E.g., The model needs to handle Pandas DataFrames as MLflow converts input data to DataFrames before passing it to PythonModel.  Purpose of this tutorial This tutorial will guide you through the process of creating a custom chat agent using MLflow\u00e2\u0080\u0099s mlflow.pyfunc.ChatModel class. By the end of this tutorial you will: Integrate MLflow Tracing into a custom mlflow.pyfunc.ChatModel instance. Customize your model using the model_config parameter within mlflow.pyfunc.log_model() . Leverage standardized signature interfaces for simplified deployment. Recognize and avoid common pitfalls when extending the mlflow.pyfunc.ChatModel class. ",
        "id": "8cf660dc967da204e60e175bef4dab5f"
    },
    {
        "text": " Prerequisites Familiarity with MLflow logging APIs and GenAI concepts. MLflow version 2.11.0 or higher installed for use of mlflow.pyfunc.ChatModel . MLflow version 2.14.0 or higher installed for use of MLflow Tracing . This tutorial uses the Databricks Foundation Model APIs purely as\nan example of interfacing with an external service. You can easily swap the\nprovider example to use any managed LLM hosting service with ease ( Amazon Bedrock , Azure AI Studio , OpenAI , Anthropic , and many others).  Core Concepts  Tracing  Customization  Standardization  Pitfalls  Tracing Customization for GenAI MLflow Tracing allows you to monitor and log the execution of your model\u00e2\u0080\u0099s methods, providing valuable insights during debugging and performance optimization. In our example BasicAgent implementation we utilize two separate APIs for the initiation of trace spans: the decorator API and the fluent API. Decorator API @mlflow . trace def _get_system_message ( self , role : str ) -> Dict : if role not in self . models : raise ValueError ( f \"Unknown role: { role } \" ) instruction = self . models [ role ][ \"instruction\" ] return ChatMessage ( role = \"system\" , content = instruction ) . to_dict () Using the @mlflow.trace tracing decorator is the simplest way to add tracing functionality to functions and methods. By default, a span that is generated from\nthe application of this decorator will utilize the name of the function as the name of the span. It is possible to override this naming, as well as\nother parameters associated with the span, as follows: @mlflow . trace ( name = \"custom_span_name\" , attributes = { \"key\" : \"value\" }, span_type = \"func\" ) def _get_system_message ( self , role : str ) -> Dict : if role not in self . models : raise ValueError ( f \"Unknown role: { role } \" ) instruction = self . models [ role ][ \"instruction\" ] return ChatMessage ( role = \"system\" , content = instruction ) . to_dict () Tip It is always advised to set a human-readable name for any span that you generate, particularly if you are instrumenting private or generically\nnamed functions or methods. The MLflow Trace UI will display the name of the function or method by default, which can be confusing to follow\nif your functions and methods are ambiguously named. Fluent API The fluent APIs context handler implementation for initiating spans is useful when you need full control of the logging of each aspect of the span\u00e2\u0080\u0099s data. The example from our application for ensuring that we\u00e2\u0080\u0099re capturing the parameters that are set when loading the model via the load_context method is\nshown below. We are pulling from the instance attributes self.models_config and self.models to set the attributes of the span. with mlflow . start_span ( \"Audit Agent\" ) as root_span : root_span . set_inputs ( messages ) attributes = { ** params . to_dict (), ** self . models_config , ** self . models } root_span . set_attributes ( attributes ) # More span manipulation... Traces in the MLflow UI After running our example that includes these combined usage patterns for trace span generation and instrumentation, ",
        "id": "410b70968617fbae075f5ad67c2d29e4"
    },
    {
        "text": " Model Customization for GenAI In order to control the behavior of our BasicAgent model without having to hard-code configuration values directly into our model logic, specifying\nconfigurations within the model_config parameter when logging the model gives some flexibility and versatility to our model definition. This functionality allows us to: Rapidly test different configurations without having to make changes to source code See the configuration that was used when logging different iterations directly in the MLflow UI Simplify the model code by decoupling the configuration from the implementation Note In our example model, we set a standard set of configurations that control the behavior of the BasicAgent . The configuration\nstructure expected by the code is a dictionary with the following components: models : Defines the per-agent configurations. (model_name) : Represents the role of the agent. This section contains: endpoint : The specific model type being used by the agent. instruction : The prompt given to the model, describing its role and responsibilities. temperature : The temperature setting controlling response variability. max_tokens : The maximum token limit for generating responses. configuration : Contains miscellaneous settings for the agent application. user_response_instruction : Provides context for the second agent by simulating a user response based on the first agent\u00e2\u0080\u0099s output. This configuration structure definition will be: Defined when logging the model and structured to support the needs of the model\u00e2\u0080\u0099s behavior Used by the load_context method and applied to the model when loading Logged within the MLmodel file and will be visible within the artifact viewer in the MLflow UI The model_config values that are submitted for our BasicAgent example within this tutorial can be seen within the logged model\u00e2\u0080\u0099s MLmodel file in the UI: ",
        "id": "30534ca18c34418243a8cfc7e5d5c601"
    },
    {
        "text": " Standardization for GenAI Models One of the more complex tasks associated with deploying a GenAI application with MLflow arises when attempting to build a custom implementation\nthat is based on subclassing the mlflow.pyfunc.PythonModel abstraction. While PythonModel is recommended for custom Deep Learning and traditional Maching Learning models (such as sklearn or torch models that require\nadditional processing logic apart from that of a base model), there are internal manipulations of the input data that occur\nwhen serving these models that introduce unneccessary complications with GenAI applications. Due to the fact that DL and traditional ML models largely rely on structured data, when input data is passed via a REST interface for model serving,\nthe PythonModel implementation will convert JSON data into pandas.DataFrame or numpy objects. This conversion creates a confusing and difficult to\ndebug scenario when using GenAI models. GenAI implementations generally deal exclusively with JSON-conformant data structures and have no tabular\nrepresentation that makes intuitive sense, thereby creating a frustrating and complex conversion interface needed to make application deployment function\ncorrectly. To simplify this problem, the mlflow.pyfunc.ChatModel class was created to provide a simpler interface for handling of the data\npassed into and returned from a call to the predict() method on custom Python models serving GenAI use cases. In the example tutorial code below, we subclass ChatModel in order to utilize this simplified interface with its immutable input and output\nformats. Because of this immutability, we don\u00e2\u0080\u0099t have to reason about model signatures, and can instead directly use API standards that have\nbeen broadly accepted throughout the GenAI industry. To illustrate why it is preferred to use ChatModel as a super class to custom GenAI implementations in MLflow, here is the signature that\nwould otherwise need to be defined and supplied during model logging to conform to the OpenAI API spec as of September 2024: Input Schema as a dict : [ { \"type\" : \"array\" , \"items\" : { \"type\" : \"object\" , \"properties\" : { \"content\" : { \"type\" : \"string\" , \"required\" : True }, \"name\" : { \"type\" : \"string\" , \"required\" : False }, \"role\" : { \"type\" : \"string\" , \"required\" : True }, }, }, \"name\" : \"messages\" , \"required\" : True , }, { \"type\" : \"double\" , \"name\" : \"temperature\" , \"required\" : False }, { \"type\" : \"long\" , \"name\" : \"max_tokens\" , \"required\" : False }, { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"name\" : \"stop\" , \"required\" : False }, { \"type",
        "id": "e268ad6d0fed68e27afb1a41cf9cdc81"
    },
    {
        "text": " }, }, \"name\" : \"messages\" , \"required\" : True , }, { \"type\" : \"double\" , \"name\" : \"temperature\" , \"required\" : False }, { \"type\" : \"long\" , \"name\" : \"max_tokens\" , \"required\" : False }, { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"name\" : \"stop\" , \"required\" : False }, { \"type\" : \"long\" , \"name\" : \"n\" , \"required\" : False }, { \"type\" : \"boolean\" , \"name\" : \"stream\" , \"required\" : False }, { \"type\" : \"double\" , \"name\" : \"top_p\" , \"required\" : False }, { \"type\" : \"long\" , \"name\" : \"top_k\" , \"required\" : False }, { \"type\" : \"double\" , \"name\" : \"frequency_penalty\" , \"required\" : False }, { \"type\" : \"double\" , \"name\" : \"presence_penalty\" , \"required\" : False }, ] Note Agent-based (tool-calling) schemas are significantly more complex than the simpler chat interface example shown above. As GenAI frameworks and services\nevolve with increasingly sophisticated capabilities and features, the complexity of these interfaces will grow, making manual schema definitions a\nchallenging and time-consuming task. The structured input validation provided by the MLflow mlflow.pyfunc.ChatModel interface removes the burden of defining and\nmanaging these intricate signatures manually. By leveraging these pre-defined schemas, you gain robust input type safety and validation, ensuring your\ndeployed applications handle inputs consistently and correctly without additional effort. This approach not only reduces the risk of errors but also\nstreamlines the development process, allowing you to focus on building impactful GenAI solutions without the overhead of managing complex input specifications. By using mlflow.pyfunc.ChatModel to base a custom implementation off of, we don\u00e2\u0080\u0099t have to reason about this complex signature.\nIt is provided for us. The only two considerations to be aware of when interfacing with the static signatures of ChatModel are: If the service that your custom implementation is interfacing with doesn\u00e2\u0080\u0099t adhere to the OpenAI spec, you will need to extract data from the\nstandard structure of mlflow.types.llm.ChatMessage and mlflow.types.llm.ChatParams and ensure that it conforms to what\nyour service is expecting. The returned response from predict should adhere to the output structure defined within the ChatModel output signature: mlflow.types.llm.ChatCompletionResponse . ",
        "id": "13a5c14c8278579d11be5aa696aa551a"
    },
    {
        "text": " Common GenAI pitfalls in MLflow There are a number of ways that building a custom implementation for a GenAI use case can be frustrating or not intuitive. Here are some of the\nmost common that we\u00e2\u0080\u0099ve heard from our users: Not using a supported flavor If you\u00e2\u0080\u0099re working with a library that is natively supported in MLflow, leveraging the built-in support for logging and loading your implementation\nwill always be easier than implementing a custom model. It is recommended to check the supported GenAI flavors to see if there is a built-in solution that will meet your use case needs in one of the many integrations that are available. Misinterpreting what load_context does While subclassing one of the base model types for a custom model, it may appear that the class definition is a \u00e2\u0080\u009cwhat you see is what you get\u00e2\u0080\u009d standard\nPython class. However, when loading your custom model instance, the load_context method is actually called by another loader object. Because of the implementation, you cannot have direct assignment of undefined instance attributes within load_context . For example, this does not work: from mlflow.pyfunc import ChatModel class MyModel ( ChatModel ): def __init__ ( self ): self . state = [] def load_context ( self , context ): # This will fail on load as the instance attribute self.my_model_config is not defined self . my_model_config = context . get ( \"my_model_config\" ) Instead, ensure that any instance attributes that are set by the load_context method are defined in the class constructor with a\nplaceholder value: from mlflow.pyfunc import ChatModel class MyModel ( ChatModel ): def __init__ ( self ): self . state = [] self . my_model_config = None # Define the attribute here def load_context ( self , context ): self . my_model_config = context . get ( \"my_model_config\" ) Failing to Handle Secrets securely It might be tempting to simplify your model\u00e2\u0080\u0099s deployment by specifying authentication secrets within a configuration. However, any configuration\ndata that is defined within your model_config parameters is directly visible in the MLflow UI and is not stored securely. The recommended approach for handling sensitive configuration data such as API keys or access tokens is to utilize a Secret Manager.\nThe configuration for what to fetch from your secrets management system can be stored within the model_config definition and\nyour deployment environment can utilize a secure means of accessing the key reference for your secrets management service. An effective place to handle secrets assignment (generally set as environment variables or passed as a part of request headers) is to\nhandle the acquisition and per-session setting within load_context . If you have rotating tokens, it is worthwhile to embed the acquisition\nof secrets and re-fetching of them upon expiry as part of a retry mechanism within the call stack of predict . Failing to use input_example While it may seem that providing an input_example when logging a model in MLflow is purely for cosmetic purposes within the artifact view\ndisplay within the MLflow UI, there is an additional bit of functionality that makes providing this data very useful, particularly for GenAI\nuse cases. When an input_example is provided, MLflow will call your model\u00e2\u0080\u0099s predict method with the example data to validate that the input is\ncompatible with the model object that you",
        "id": "30f18e1a1e0542da02e936513e9e29e2"
    },
    {
        "text": "y for cosmetic purposes within the artifact view\ndisplay within the MLflow UI, there is an additional bit of functionality that makes providing this data very useful, particularly for GenAI\nuse cases. When an input_example is provided, MLflow will call your model\u00e2\u0080\u0099s predict method with the example data to validate that the input is\ncompatible with the model object that you are logging. If there are any failures that occur, you will receive an error message detailing\nwhat is wrong with the input syntax. This is very beneficial to ensure that, at the point of logging, you can ensure that your expected\ninput interface structure is what will be allowable for the deployed model, thereby saving you hours of debugging and troubleshooting later\nwhen attempting to deploy your solution. It is highly recommended to supply this example during logging. Failing to handle retries for Rate Limits being hit Nearly all GenAI provider services impose rate limits and token-based usage limits to prevent disruption to their service or to help protect\nusers from unexpected bills. When limits are reached, it is important that your prediction logic is robust to handle these failures to ensure\nthat a user of your deployed application understands why their request was not successful. It can be beneficial to introduce retry logic for certain errors, particularly those involving transient connection issues or per-unit-of-time\nrequest limits. Not validating before deployment The process of deploying a GenAI application can a significant amount of time. When an implementation is finally ready to be submitted to a\nserving environment, the last thing that you want to deal with is a model that is incapable of being served due to some issue with a decoded\nJSON payload being submitted to your model\u00e2\u0080\u0099s predict() method. MLflow offers the mlflow.models.validate_serving_input() API to ensure that the model that you have logged is capable of being interacted\nwith by emulating the data processing that occurs with a deployed model. To use this API, simply navigate to your logged model with the MLflow UI\u00e2\u0080\u0099s artifact viewer. The model display pane on the right side of\nthe artifact viewer contains the code snippet that you can execute in an interactive environment to ensure that your model is ready to\ndeploy. For the example in this tutorial, this is the generated code that is copied from the artifact viewer display: from mlflow.models import validate_serving_input model_uri = \"runs:/8935b7aff5a84f559b5fcc2af3e2ea31/model\" # The model is logged with an input example. MLflow converts # it into the serving payload format for the deployed model endpoint, # and saves it to 'serving_input_payload.json' serving_payload = \"\"\"{ \"messages\": [ { \"role\": \"user\", \"content\": \"What is a good recipe for baking scones that doesn't require a lot of skill?\" } ], \"temperature\": 1.0, \"n\": 1, \"stream\": false }\"\"\" # Validate the serving payload works on the model validate_serving_input ( model_uri , serving_payload ) ",
        "id": "576aee250d48efa1625a819be1ecb1cb"
    },
    {
        "text": " Key Classes and Methods in our example BasicAgent : Our custom chat agent class that extends ChatModel . _get_system_message : Retrieves the system message configuration for a specific role. _get_agent_response` : Sends messages to an endpoint and retrieves responses. _call_agent : Manages the conversation flow between the agent roles. _prepare_message_list` : Prepares the list of messages for sending. load_context : Initializes the model context and configurations. predict` : Handles the prediction logic for the chat model. Of these methods listed above, the methods load_context and predict override the base abstracted implementations of ChatModel . In order to\ndefine a subclass of ChatModel , you must implement (at a minimum), the predict method. The load_context method is only used if you are implementing (as we\nwill be below) custom loading logic where a static configuration needs to be loaded for the model object to work, or additional dependent logic needs\nto execute in order for the object instantiation to function correctly. ",
        "id": "d007836efab00d551ce8bb0a5efad34c"
    },
    {
        "text": " Example of a custom ChatModel In the full example below, we\u00e2\u0080\u0099re creating a custom chat agent by subclassing the mlflow.pyfunc.ChatModel . This agent, named BasicAgent ,\ntakes advantage of several important features that help streamline the development, deployment, and tracking of GenAI applications. By subclassing ChatModel ,\nwe ensure a consistent interface for handling conversational agents, while also avoiding common pitfalls associated with more general-purpose models. The implementation below highlights the following key aspects: Tracing : We leverage MLflow\u00e2\u0080\u0099s tracing functionality to track and log critical operations using both the decorator and fluent API context handler approaches. Decorator API : This is used to easily trace methods such as _get_agent_response and _call_agent for automatic span creation. Fluent API : Provides fine-grained control over span creation, as shown in the predict method for auditing key inputs and outputs during agent interactions. Tip : We ensure human-readable span names for easier debugging in the MLflow Trace UI and when fetching logged traces via the client API. Custom Configuration : Model Configuration : By passing custom configurations during model logging (using the model_config parameter), we decouple model behavior from\nhard-coded values. This allows rapid testing of different agent configurations without modifying the source code. load_context Method : Ensures that configurations are loaded at runtime, initializing the agent with the necessary settings and preventing runtime\nfailures due to missing configurations. Tip : We avoid directly setting undefined instance attributes within load_context . Instead, all attributes are initialized with default\nvalues in the class constructor to ensure proper loading of our model. Conversation Management : We implement a multi-step agent interaction pattern using methods like _get_system_message , _get_agent_response , and _call_agent . These\nmethods manage the flow of communication between multiple agents, such as an \u00e2\u0080\u009coracle\u00e2\u0080\u009d and a \u00e2\u0080\u009cjudge\u00e2\u0080\u009d role, each configured with specific instructions\nand parameters. Static Input/Output Structures : By adhering to the ChatModel \u00e2\u0080\u0099s required input ( List[ChatMessage] ) and output ( ChatCompletionResponse ) formats,\nwe eliminate the complexities associated with converting JSON or tabular data, which is common in more general models like PythonModel . Common Pitfalls Avoided : Model Validation via Input Examples : We provide an input example during model logging, allowing MLflow to validate the input interface and catch\nstructural issues early, reducing debugging time during deployment. import mlflow from mlflow.types.llm import ChatCompletionResponse , ChatMessage , ChatParams , ChatChoice from mlflow.pyfunc import ChatModel from mlflow import deployments from typing import List , Optional , Dict class BasicAgent ( ChatModel ): def __init__ ( self ): \"\"\"Initialize the BasicAgent with placeholder values.\"\"\" self . deploy_client = None self . models = {} self . models_config = {} self .",
        "id": "d781caa45dbac734ad6b8703d64a32c7"
    },
    {
        "text": " , ChatMessage , ChatParams , ChatChoice from mlflow.pyfunc import ChatModel from mlflow import deployments from typing import List , Optional , Dict class BasicAgent ( ChatModel ): def __init__ ( self ): \"\"\"Initialize the BasicAgent with placeholder values.\"\"\" self . deploy_client = None self . models = {} self . models_config = {} self . conversation_history = [] def load_context ( self , context ): \"\"\"Initialize the connectors and model configurations.\"\"\" self . deploy_client = deployments . get_deploy_client ( \"databricks\" ) self . models = context . model_config . get ( \"models\" , {}) self . models_config = context . model_config . get ( \"configuration\" , {}) def _get_system_message ( self , role : str ) -> Dict : \"\"\" Get the system message configuration for the specified role. Args: role (str): The role of the agent (e.g., \"oracle\" or \"judge\"). Returns: dict: The system message for the given role. \"\"\" if role not in self . models : raise ValueError ( f \"Unknown role: { role } \" ) instruction = self . models [ role ][ \"instruction\" ] return ChatMessage ( role = \"system\" , content = instruction ) . to_dict () @mlflow . trace ( name = \"Raw Agent Response\" ) def _get_agent_response ( self , message_list : List [ Dict ], endpoint : str , params : Optional [ dict ] = None ) -> Dict : \"\"\" Call the agent endpoint to get a response. Args: message_list (List[Dict]): List of messages for the agent. endpoint (str): The agent's endpoint. params (Optional[dict]): Additional parameters for the call. Returns: dict: The response from the agent. \"\"\" response = self . deploy_client . predict ( endpoint = endpoint , inputs = { \"messages\" : message_list , ** ( params or {})} ) return response [ \"choices\" ][ 0 ][ \"message\" ] @mlflow . trace ( name = \"Agent Call\" ) def _call_agent ( self , message : ChatMessage , role : str , params : Optional [ dict ] = None ) -> Dict : \"\"\" Prepares and sends the request to a specific agent based on the role. Args: message (ChatMessage): The message to be processed. role (str): The role of the agent (e.g., \"oracle\" or \"judge\"). params (Optional[dict]): Additional parameters for the call. Returns: dict: The response from the agent. \"\"\" system_message = self . _get_system_message ( role ) message_list = self . _prepare_message_list ( system_message , message ) # Fetch agent response agent_config = self . models [ role ] response = self . _get_agent_response ( message_list , agent_config [ \"endpoint\" ], params ) # Update conversation history self . conversation_history . extend ([ message . to_dict (), response ]) return response @mlflow . trace ( name = \"Assemble Conversation\" ) def _prepare_message_list ( self , system_message : Dict , user_message : ChatMessage ) -> List [ Dict ]: \"\"\" Prepare the list of messages to send to the agent. Args: system_message (dict): The system message dictionary. user_message (ChatMessage): The user message. Returns: List[dict]: The complete list of messages to send. \"\"\" user_prompt = { \"role\" : \"user\" , \"content\" : self . models_config . get ( \"user_response_instructi",
        "id": "9ffe20b42798231e247cf6225ffbd7bb"
    },
    {
        "text": "> List [ Dict ]: \"\"\" Prepare the list of messages to send to the agent. Args: system_message (dict): The system message dictionary. user_message (ChatMessage): The user message. Returns: List[dict]: The complete list of messages to send. \"\"\" user_prompt = { \"role\" : \"user\" , \"content\" : self . models_config . get ( \"user_response_instruction\" , \"Can you make the answer better?\" ), } if self . conversation_history : return [ system_message , * self . conversation_history , user_prompt ] else : return [ system_message , user_message . to_dict ()] def predict ( self , context , messages : List [ ChatMessage ], params : Optional [ ChatParams ] = None ) -> ChatCompletionResponse : \"\"\" Predict method to handle agent conversation. Args: context: The MLflow context. messages (List[ChatMessage]): List of messages to process. params (Optional[ChatParams]): Additional parameters for the conversation. Returns: ChatCompletionResponse: The structured response object. \"\"\" # Use the fluent API context handler to have added control over what is included in the span with mlflow . start_span ( name = \"Audit Agent\" ) as root_span : # Add the user input to the root span root_span . set_inputs ( messages ) # Add attributes to the root span attributes = { ** params . to_dict (), ** self . models_config , ** self . models } root_span . set_attributes ( attributes ) # Initiate the conversation with the oracle oracle_params = self . _get_model_params ( \"oracle\" ) oracle_response = self . _call_agent ( messages [ 0 ], \"oracle\" , oracle_params ) # Process the response with the judge judge_params = self . _get_model_params ( \"judge\" ) judge_response = self . _call_agent ( ChatMessage ( ** oracle_response ), \"judge\" , judge_params ) # Reset the conversation history and return the final response self . conversation_history = [] output = ChatCompletionResponse ( choices = [ ChatChoice ( index = 0 , message = ChatMessage ( ** judge_response ))], usage = {}, model = judge_params . get ( \"endpoint\" , \"unknown\" ), ) root_span . set_outputs ( output ) return output def _get_model_params ( self , role : str ) -> dict : \"\"\" Retrieves model parameters for a given role. Args: role (str): The role of the agent (e.g., \"oracle\" or \"judge\"). Returns: dict: A dictionary of parameters for the agent. \"\"\" role_config = self . models . get ( role , {}) return { \"temperature\" : role_config . get ( \"temperature\" , 0.5 ), \"max_tokens\" : role_config . get ( \"max_tokens\" , 500 ), } Now that we have our model defined, the process of logging it has only a single step that is required to be taken before logging:\nwe need to define the configuration for our model to be initialized with. This is done by defining our model_config configuration. ",
        "id": "7f8950d11b0e6e36d8139882fc19a7c6"
    },
    {
        "text": " Setting our model_config values Before logging the model, we need to define the configuration that governs the behavior of our model\u00e2\u0080\u0099s agents. This decoupling of configuration from the core logic of the model allows us to easily test and compare different agent behaviors without needing to modify the model implementation. By using a flexible configuration system, we can efficiently experiment with different settings, making it much easier to iterate and fine-tune our model.  Why Decouple Configuration? In the context of Generative AI (GenAI), agent behavior can vary greatly depending on the instruction sets and parameters (such as temperature or max_tokens ) given to each agent. If we hardcoded these configurations directly into our model\u00e2\u0080\u0099s logic, each new test would require changing the\nmodel\u00e2\u0080\u0099s source code, leading to: Inefficiency : Changing source code for each test slows down the experimentation process. Increased Risk of Errors : Constantly modifying the source increases the chance of introducing bugs or unintended side effects. Lack of Reproducibility : Without a clear separation between code and configuration, tracking and reproducing the exact configuration used for\na particular result becomes challenging. By setting these values externally via the model_config parameter, we make the model flexible and adaptable to different test scenarios.\nThis approach also integrates seamlessly with MLflow\u00e2\u0080\u0099s evaluation tools, such as mlflow.evaluate() , which allows you to compare model\noutputs across different configurations systematically.  Defining the Model Configuration The configuration consists of two main sections: Models : This section defines agent-specific configurations, such as the judge and oracle roles in this example. Each agent has: An endpoint : Specifies the model type or service being used for this agent. An instruction : Defines the role and responsibilities of the agent (e.g., answering questions, evaluating responses). Temperature and Max Tokens : Controls the generation variability ( temperature ) and token limit for responses. General Configuration : Additional settings for the overall behavior of the model, such as how user responses should be framed for subsequent agents. Note There are two options available for setting a model configuration: directly within the logging code (shown below) or by writing a configuration file\nin yaml format to a local location whose path can be specified when defining the model_config argument during logging. To learn more about\nhow the model_config parameter is utilized, see the guide on model_config usage . Here\u00e2\u0080\u0099s how we set the configuration for our agents: model_config = { \"models\" : { \"judge\" : { \"endpoint\" : \"databricks-meta-llama-3-1-405b-instruct\" , \"instruction\" : ( \"You are an evaluator of answers provided by others. Based on the context of both the question and the answer, \" \"provide a corrected answer if it is incorrect; otherwise, enhance the answer with additional context and explanation.\" ), \"temperature\" : 0.5 , \"max_tokens\" : 2000 , }, \"oracle\" : { \"endpoint\" : \"databricks-mixtral-8x7b-instruct\" , \"instruction\" : ( \"You are a knowledgeable source of information that excels at providing detailed, but brief answers to questions. \" \"Provide an answer to the question based on the information provided.\" ), \"temperature\" : 0.9 , \"max_tokens\" : 5000 , }, }, \"configuration\" : { \"user_response_instruction\" : \"Can you evaluate and enhance this answer with the provided contextual history?\" }, } ",
        "id": "4b995b99dbddf30d1d18e3af7474ca2e"
    },
    {
        "text": " Benefits of External Configuration Flexibility : The decoupled configuration allows us to easily switch or adjust model behavior without modifying the core logic. For example, we can\nchange the model\u00e2\u0080\u0099s instructions or adjust the temperature to test different levels of creativity in the responses. Scalability : As more agents are added to the system or new roles are introduced, we can extend this configuration without cluttering the model\u00e2\u0080\u0099s\ncode. This separation keeps the codebase cleaner and more maintainable. Reproducibility and Comparison : By keeping configuration external, we can log the specific settings used in each run with MLflow. This makes it\neasier to reproduce results and compare different experiments, ensuring a robust evaluation and adjudication process to select the best performing\nconfiguration. With the configuration in place, we\u00e2\u0080\u0099re now ready to log the model and run experiments using these settings. By leveraging MLflow\u00e2\u0080\u0099s powerful tracking\nand logging features, we\u00e2\u0080\u0099ll be able to manage the experiments efficiently and extract valuable insights from the agent\u00e2\u0080\u0099s responses.  Defining an Input Example Before logging our model, it\u00e2\u0080\u0099s important to provide an input_example that demonstrates how to interact with the model. This example serves several key purposes: Validation at Logging Time : Including an input_example allows MLflow to execute the predict method using this example during the logging\nprocess. This helps validate that your model can handle the expected input format and catch any issues early. UI Representation : The input_example is displayed in the MLflow UI under the model\u00e2\u0080\u0099s artifacts. This provides a convenient reference for\nusers to understand the expected input structure when interacting with the deployed model. By providing an input example, you ensure that your model is tested with real data, increasing confidence that it will behave as expected when deployed. Tip When defining your GenAI application using the mlflow.pyfunc.ChatModel , a default placeholder input example will be used if none is provided.\nIf you notice an unfamiliar or generic input example in the MLflow UI\u00e2\u0080\u0099s artifact viewer, it\u00e2\u0080\u0099s likely the default placeholder assigned by the system.\nTo avoid this, ensure you specify a custom input example when saving your model. Here\u00e2\u0080\u0099s the input example we\u00e2\u0080\u0099ll use: input_example = { \"messages\" : [ { \"role\" : \"user\" , \"content\" : \"What is a good recipe for baking scones that doesn't require a lot of skill?\" , } ] } This example represents a user asking for an easy scone recipe. It aligns with the input structure expected by our BasicAgent model, which processes a\nlist of messages where each message includes a role and content . Benefits of Providing an Input Example: Execution and Validation : MLflow will pass this input_example to the model\u00e2\u0080\u0099s predict method during logging to ensure that it can process\nthe input without errors. Any issues with input handling, such as incorrect data types or missing fields, will be caught at this stage, saving you time\ndebugging later. User Interface Display : The input_example will be visible in the MLflow UI within the model artifact view section. This helps users understand\nthe format of input data the model expects, making it easier to interact with the model once it\u00e2\u0080\u0099s deployed. Deployment Confidence : By validating the model with an example input upfront, you gain additional assurance that the model will function correctly\nin a production environment, reducing the risk of unexpected behavior after deployment. Including an input_example is a simple yet powerful step to verify that your model is ready for deployment and will behave as expected when\nreceiving input from users. ",
        "id": "e03dc572e8ce7f194c514e73e83f24ce"
    },
    {
        "text": " Logging and Loading our custom Agent To log and load the model using MLflow, use: with mlflow . start_run (): model_info = mlflow . pyfunc . log_model ( \"model\" , python_model = BasicAgent (), model_config = model_config , input_example = input_example , ) loaded = mlflow . pyfunc . load_model ( model_info . model_uri ) response = loaded . predict ( { \"messages\" : [ { \"role\" : \"user\" , \"content\" : \"What is the best material to make a baseball bat out of?\" , } ] } )  Conclusion In this tutorial, you have explored the process of creating a custom GenAI chat agent using MLflow\u00e2\u0080\u0099s mlflow.pyfunc.ChatModel class.\nWe demonstrated how to implement a flexible, scalable, and standardized approach to managing the deployment of GenAI applications, enabling you\nto harness the latest advancements in AI, even for libraries and frameworks that are not yet natively supported with a named flavor in MLflow. By using ChatModel instead of the more generic PythonModel , you can avoid many of the common pitfalls associated with deploying GenAI by\nleveraging the benefits of immutable signature interfaces that are consistent across any of your deployed GenAI interfaces, simplifying the\nuse of all of your solutions by providing a consistent experience. Key takeaways from this tutorial include: Tracing and Monitoring : By integrating tracing directly into the model, you gain valuable insights into the internal workings of your application,\nmaking debugging and optimization more straightforward. Both the decorator and fluent API approaches offer versatile ways to manage tracing for\ncritical operations. Flexible Configuration Management : Decoupling configurations from your model code ensures that you can rapidly test and iterate without\nmodifying source code. This approach not only streamlines experimentation but also enhances reproducibility and scalability as your application evolves. Standardized Input and Output Structures : Leveraging the static signatures of ChatModel simplifies the complexities of deploying and\nserving GenAI models. By adhering to established standards, you reduce the friction typically associated with integrating and validating input/output formats. Avoiding Common Pitfalls : Throughout the implementation, we highlighted best practices to avoid common issues, such as proper handling\nof secrets, validating input examples, and understanding the nuances of loading context. Following these practices ensures that your model\nremains secure, robust, and reliable in production environments. Validation and Deployment Readiness : The importance of validating your model before deployment cannot be overstated. By using tools\nlike mlflow.models.validate_serving_input() , you can catch and resolve potential deployment issues early, saving time and effort\nduring the production deployment process. As the landscape of Generative AI continues to evolve, building adaptable and standardized models will be crucial to leveraging the exciting\nand powerful capabilities that will be unlocked in the months and years ahead. The approach covered in this tutorial equips you with a robust\nframework for integrating and managing GenAI technologies within MLflow, empowering you to develop, track, and deploy sophisticated AI solutions with ease. We encourage you to extend and customize this foundational example to suit your specific needs and explore further enhancements. By leveraging\nMLflow\u00e2\u0080\u0099s growing capabilities, you can continue to refine your GenAI models, ensuring they deliver impactful and reliable results in any application. Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "30c03d8132578a16cd84cffcd23bf207"
    },
    {
        "text": "Build a tool-calling model with mlflow.pyfunc.ChatModel 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow Retrieval Augmented Generation (RAG) Deploying Advanced LLMs with Custom PyFuncs in MLflow LLM Evaluation Examples Tutorial: Getting Started with ChatModel Tutorial: Custom GenAI Models using ChatModel Build a tool-calling model with mlflow.pyfunc.ChatModel Environment setup Step 1: Creating the tool definition Step 2: Implementing the tool Step 3: Implementing the predict method Step 4 (optional, but recommended): Enable tracing for the model Step 5: Logging the model Using the model for inference Serving the model Conclusion MLflow Trace UI in Jupyter Notebook Demo MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs Build a tool-calling model with mlflow.pyfunc.ChatModel  Build a tool-calling model with mlflow.pyfunc.ChatModel Download this Notebook Welcome to the notebook tutorial on building a simple tool calling model using the mlflow.pyfunc.ChatModel wrapper. ChatModel is a subclass of MLflow\u00e2\u0080\u0099s highly customizable PythonModel , which was specifically designed to make creating GenAI workflows easier. Briefly, here are some of the benefits of using ChatModel: No need to define a complex signature! Chat models often accept complex inputs with many levels of nesting, and this can be cumbersome to define yourself. Support for JSON / dict inputs (no need to wrap inputs or convert to Pandas DataFrame) Includes the use of Dataclasses for defining expected inputs / outputs for a simplified development experience For a more in-depth exploration of ChatModel, please check out the detailed guide . In this tutorial, we\u00e2\u0080\u0099ll be building a simple OpenAI wrapper that makes use of the tool calling support (released in MLflow 2.17.0).  Environment setup First, let\u00e2\u0080\u0099s set up the environment. We\u00e2\u0080\u0099ll need the OpenAI Python SDK, as well as MLflow >= 2.17.0. We\u00e2\u0080\u0099ll also need to set our OpenAI API key in order to use the SDK. [16]: % pip install 'mlflow>=2.17.0' 'openai>=1.0' -qq Note: you may need to restart the kernel to use updated packages. [1]: import os from getpass import getpass os . environ [ \"OPENAI_API_KEY\" ] = getpass ( \"Enter your OpenAI API key: \" ) ",
        "id": "d41ceaec2baeb4824833f8b58c704c11"
    },
    {
        "text": " Step 1: Creating the tool definition Let\u00e2\u0080\u0099s begin to define our model! As mentioned in the introduction, we\u00e2\u0080\u0099ll be subclassing mlflow.pyfunc.ChatModel . For this example, we\u00e2\u0080\u0099ll build a toy model that uses a tool to retrieve the weather for a given city. The first step is to create a tool definition that we can pass to OpenAI. We do this by using mlflow.types.llm.FunctionToolDefinition to describe the parameters that our tool accepts. The format of this dataclass is aligned with the OpenAI spec: [2]: import mlflow from mlflow.types.llm import ( FunctionToolDefinition , ParamProperty , ToolParamsSchema , ) class WeatherModel ( mlflow . pyfunc . ChatModel ): def __init__ ( self ): # a sample tool definition. we use the `FunctionToolDefinition` # class to describe the name and expected params for the tool. # for this example, we're defining a simple tool that returns # the weather for a given city. weather_tool = FunctionToolDefinition ( name = \"get_weather\" , description = \"Get weather information\" , parameters = ToolParamsSchema ( { \"city\" : ParamProperty ( type = \"string\" , description = \"City name to get weather information for\" , ), } ), # make sure to call `to_tool_definition()` to convert the `FunctionToolDefinition` # to a `ToolDefinition` object. this step is necessary to normalize the data format, # as multiple types of tools (besides just functions) might be available in the future. ) . to_tool_definition () # OpenAI expects tools to be provided as a list of dictionaries self . tools = [ weather_tool . to_dict ()]  Step 2: Implementing the tool Now that we have a definition for the tool, we need to actually implement it. For the purposes of this tutorial, we\u00e2\u0080\u0099re just going to mock a response, but the implementation can be arbitrary\u00e2\u0080\u0094you might make an API call to an actual weather service, for example. [3]: class WeatherModel ( mlflow . pyfunc . ChatModel ): def __init__ ( self ): weather_tool = FunctionToolDefinition ( name = \"get_weather\" , description = \"Get weather information\" , parameters = ToolParamsSchema ( { \"city\" : ParamProperty ( type = \"string\" , description = \"City name to get weather information for\" , ), } ), ) . to_tool_definition () self . tools = [ weather_tool . to_dict ()] def get_weather ( self , city : str ) -> str : # in a real-world scenario, the implementation might be more complex return f \"It's sunny in { city } , with a temperature of 20C\" ",
        "id": "26b68b63f06a00bb652aeb923a850a5c"
    },
    {
        "text": " Step 3: Implementing the predict method The next thing we need to do is define a predict() function that accepts the following arguments: context : PythonModelContext (not used in this tutorial) messages : List[ ChatMessage ]. This is the chat input that the model uses for generation. params : ChatParams . These are commonly used params used to configure the chat model, e.g.\u00c2 temperature , max_tokens , etc. This is where the tool specifications can be found. This is the function that will ultimately be called during inference. For the implementation, we\u00e2\u0080\u0099ll simply forward the user\u00e2\u0080\u0099s input to OpenAI, and provide the get_weather tool as an option for the LLM to use if it chooses to do so. If we receive a tool call request, we\u00e2\u0080\u0099ll call the get_weather() function and return the response back to OpenAI. We\u00e2\u0080\u0099ll need to use what we\u00e2\u0080\u0099ve defined in the previous two steps in order to do this. [4]: import json from openai import OpenAI import mlflow from mlflow.types.llm import ( ChatMessage , ChatParams , ChatResponse , ) class WeatherModel ( mlflow . pyfunc . ChatModel ): def __init__ ( self ): weather_tool = FunctionToolDefinition ( name = \"get_weather\" , description = \"Get weather information\" , parameters = ToolParamsSchema ( { \"city\" : ParamProperty ( type = \"string\" , description = \"City name to get weather information for\" , ), } ), ) . to_tool_definition () self . tools = [ weather_tool . to_dict ()] def get_weather ( self , city : str ) -> str : return \"It's sunny in {} , with a temperature of 20C\" . format ( city ) # the core method that needs to be implemented. this function # will be called every time a user sends messages to our model def predict ( self , context , messages : list [ ChatMessage ], params : ChatParams ): # instantiate the OpenAI client client = OpenAI () # convert the messages to a format that the OpenAI API expects messages = [ m . to_dict () for m in messages ] # call the OpenAI API response = client . chat . completions . create ( model = \"gpt-4o-mini\" , messages = messages , # pass the tools in the request tools = self . tools , ) # if OpenAI returns a tool_calling response, then we call # our tool. otherwise, we just return the response as is tool_calls = response . choices [ 0 ] . message . tool_calls if tool_calls : print ( \"Received a tool call, calling the weather tool...\" ) # for this example, we only provide the model with one tool, # so we can assume the tool call is for the weather tool. if # we had more, we'd need to check the name of the tool that # was called city = json . loads ( tool_calls [ 0 ] . function . arguments )[ \"city\" ] tool_call_id = tool_calls [ 0 ] . id # call the tool and construct a new chat message tool_response = ChatMessage ( role = \"tool\" , content = self . get_weather ( city ), tool_call_id = tool_call_id ) . to_dict () # send another request to the API, making sure to append # the assistant's tool call along with the tool response. messages . append ( response . choices [ 0 ] . message ) messages . append ( tool_response ) response = client . chat . completions . create ( model = \"gpt-4o-mini\" , messages = messages , tools = self . tools , ) # return the result as a ChatResponse, as this # is the expected output of the predict method return ChatResponse . from_dict ( response . to_dict ()) ",
        "id": "461ef55d64df653ddaa2fb44a53e9faa"
    },
    {
        "text": " Step 4 (optional, but recommended): Enable tracing for the model This step is optional, but highly recommended to improve observability in your app. We\u00e2\u0080\u0099ll be using MLflow Tracing to log the inputs and outputs of our model\u00e2\u0080\u0099s internal functions, so we can easily debug when things go wrong. Agent-style tool calling models can make many layers of function calls during the lifespan of a single request, so tracing is invaluable in helping us understand what\u00e2\u0080\u0099s going on at each step. Integrating tracing is easy, we simply decorate the functions we\u00e2\u0080\u0099re interested in ( get_weather() and predict() ) with @mlflow.trace ! MLflow Tracing also has integrations with many popular GenAI frameworks, such as LangChain, OpenAI, LlamaIndex, and more. For the full list, check out this documentation page . In this tutorial, we\u00e2\u0080\u0099re using the OpenAI SDK to make API calls, so we can enable tracing for this by\ncalling mlflow.openai.autolog() . To view the traces in the UI, run mlflow ui in a separate terminal shell, and navigate to the Traces tab after using the model for inference below. [5]: from mlflow.entities.span import ( SpanType , ) # automatically trace OpenAI SDK calls mlflow . openai . autolog () class WeatherModel ( mlflow . pyfunc . ChatModel ): def __init__ ( self ): weather_tool = FunctionToolDefinition ( name = \"get_weather\" , description = \"Get weather information\" , parameters = ToolParamsSchema ( { \"city\" : ParamProperty ( type = \"string\" , description = \"City name to get weather information for\" , ), } ), ) . to_tool_definition () self . tools = [ weather_tool . to_dict ()] @mlflow . trace ( span_type = SpanType . TOOL ) def get_weather ( self , city : str ) -> str : return \"It's sunny in {} , with a temperature of 20C\" . format ( city ) @mlflow . trace ( span_type = SpanType . AGENT ) def predict ( self , context , messages : list [ ChatMessage ], params : ChatParams ): client = OpenAI () messages = [ m . to_dict () for m in messages ] response = client . chat . completions . create ( model = \"gpt-4o-mini\" , messages = messages , tools = self . tools , ) tool_calls = response . choices [ 0 ] . message . tool_calls if tool_calls : print ( \"Received a tool call, calling the weather tool...\" ) city = json . loads ( tool_calls [ 0 ] . function . arguments )[ \"city\" ] tool_call_id = tool_calls [ 0 ] . id tool_response = ChatMessage ( role = \"tool\" , content = self . get_weather ( city ), tool_call_id = tool_call_id ) . to_dict () messages . append ( response . choices [ 0 ] . message ) messages . append ( tool_response ) response = client . chat . completions . create ( model = \"gpt-4o-mini\" , messages = messages , tools = self . tools , ) return ChatResponse . from_dict ( response . to_dict ()) ",
        "id": "874b304cc6226adf817fbb500be59cc2"
    },
    {
        "text": " Step 5: Logging the model Finally, we need to log the model. This saves the model as an artifact in MLflow Tracking, and allows us to load and serve it later on. (Note: this is a fundamental pattern in MLflow. To learn more, check out the Quickstart guide !) In order to do this, we need to do a few things: Define an input example to inform users about the input we expect Instantiate the model Call mlflow.pyfunc.log_model() with the above as arguments Take note of the Model URI printed out at the end of the cell\u00e2\u0080\u0094we\u00e2\u0080\u0099ll need it when serving the model later! [6]: # messages to use as input examples messages = [ { \"role\" : \"system\" , \"content\" : \"Please use the provided tools to answer user queries.\" }, { \"role\" : \"user\" , \"content\" : \"What's the weather in Singapore?\" }, ] input_example = { \"messages\" : messages , } # instantiate the model model = WeatherModel () # log the model with mlflow . start_run (): model_info = mlflow . pyfunc . log_model ( artifact_path = \"weather-model\" , python_model = model , input_example = input_example , ) print ( \"Successfully logged the model at the following URI: \" , model_info . model_uri ) 2024/10/29 09:30:14 INFO mlflow.pyfunc: Predicting on input example to validate output Received a tool call, calling the weather tool... Received a tool call, calling the weather tool...\nSuccessfully logged the model at the following URI:  runs:/8051850efa194a3b8b2450c4c9f4d42f/weather-model  Using the model for inference Now that the model is logged, our work is more or less done! In order to use the model for inference, let\u00e2\u0080\u0099s load it back using mlflow.pyfunc.load_model() . [7]: import mlflow # Load the previously logged ChatModel tool_model = mlflow . pyfunc . load_model ( model_info . model_uri ) system_prompt = { \"role\" : \"system\" , \"content\" : \"Please use the provided tools to answer user queries.\" , } messages = [ system_prompt , { \"role\" : \"user\" , \"content\" : \"What's the weather in Singapore?\" }, ] # Call the model's predict method response = tool_model . predict ({ \"messages\" : messages }) print ( response [ \"choices\" ][ 0 ][ \"message\" ][ \"content\" ]) messages = [ system_prompt , { \"role\" : \"user\" , \"content\" : \"What's the weather in San Francisco?\" }, ] # Generating another response response = tool_model . predict ({ \"messages\" : messages }) print ( response [ \"choices\" ][ 0 ][ \"message\" ][ \"content\" ]) 2024/10/29 09:30:27 WARNING mlflow.tracing.processor.mlflow: Creating a trace within the default experiment with id '0'. It is strongly recommended to not use the default experiment to log traces due to ambiguous search results and probable performance issues over time due to directory table listing performance degradation with high volumes of directories within a specific path. To avoid performance and disambiguation issues, set the experiment for your environment using `mlflow.set_experiment()` API. Received a tool call, calling the weather tool...\nThe weather in Singapore is sunny, with a temperature of 20\u00c2\u00b0C.\nReceived a tool call, calling the weather tool...\nThe weather in San Francisco is sunny, with a temperature of 20\u00c2\u00b0C. ",
        "id": "4ba0841a8bf3830c8979c45ead0bdc52"
    },
    {
        "text": " Serving the model MLflow also allows you to serve models, using the mlflow models serve CLI tool. In another terminal shell, run the following from the same folder as this notebook: $ export OPENAI_API_KEY = <YOUR OPENAI API KEY>\n$ mlflow models serve -m <MODEL_URI> This will start serving the model on http://127.0.0.1:5000 , and the model can be queried via POST request to the /invocations route. [8]: import requests messages = [ system_prompt , { \"role\" : \"user\" , \"content\" : \"What's the weather in Tokyo?\" }, ] response = requests . post ( \"http://127.0.0.1:5000/invocations\" , json = { \"messages\" : messages }) response . raise_for_status () response . json () [8]: {'choices': [{'index': 0,\n   'message': {'role': 'assistant',\n    'content': 'The weather in Tokyo is sunny, with a temperature of 20\u00c2\u00b0C.'},\n   'finish_reason': 'stop'}],\n 'usage': {'prompt_tokens': 100, 'completion_tokens': 16, 'total_tokens': 116},\n 'id': 'chatcmpl-ANVOhWssEiyYNFwrBPxp1gmQvZKsy',\n 'model': 'gpt-4o-mini-2024-07-18',\n 'object': 'chat.completion',\n 'created': 1730165599}  Conclusion In this tutorial, we covered how to use MLflow\u00e2\u0080\u0099s ChatModel class to create a convenient OpenAI wrapper that supports tool calling. Though the use-case was simple, the concepts covered here can be easily extended to support more complex functionality. If you\u00e2\u0080\u0099re looking to dive deeper into building quality GenAI apps, you might be also be interested in checking out MLflow Tracing , an observability tool you can use to trace the execution of arbitrary functions (such as your tool calls, for example). Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "b0ccb55abae343bcdcfc3ef3f3e30157"
    },
    {
        "text": "MLflow Trace UI in Jupyter Notebook Demo 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow Retrieval Augmented Generation (RAG) Deploying Advanced LLMs with Custom PyFuncs in MLflow LLM Evaluation Examples Tutorial: Getting Started with ChatModel Tutorial: Custom GenAI Models using ChatModel Build a tool-calling model with mlflow.pyfunc.ChatModel MLflow Trace UI in Jupyter Notebook Demo Prerequisites When is the MLflow Trace UI displayed? Disabling the UI Conclusion MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow Trace UI in Jupyter Notebook Demo  MLflow Trace UI in Jupyter Notebook Demo This notebook is a quick showcase of the MLflow Trace UI within Jupyter Notebooks. We begin with some toy examples to explain the display functionality, and end by building a simple RAG demo to showcase more of the UI features. Download this Notebook  Prerequisites Please make sure you have the following packages installed for this demo. mlflow >= 2.20 openai Optionally, for the RAG demo at the end, you\u00e2\u0080\u0099ll need: langchain langchain-community beautifulsoup4 You can run the cell below to install all these packages (make sure to restart the kernel afterwards) [ ]: % pip install mlflow>=2.20 openai langchain langchain-community beautifulsoup4  When is the MLflow Trace UI displayed? The UI is only displayed when the MLflow Tracking URI is set to an HTTP tracking server, as this is where the UI assets are served from. If you don\u00e2\u0080\u0099t use a remote tracking server, you can always start one locally by running the mlflow server CLI command. By default, the tracking server will be running at http://localhost:5000 . For this tutorial, please make sure your tracking URI is set correctly! [ ]: import mlflow # replace with your own URI tracking_uri = \"http://localhost:5000\" mlflow . set_tracking_uri ( tracking_uri ) # set a new experiment to avoid # cluttering the default experiment experiment = mlflow . set_experiment ( \"mlflow-trace-ui-demo\" ) Once that\u00e2\u0080\u0099s set up, the trace UI should automatically show up for the following events. Examples of each will be provided below: When the cell code generates a trace When a mlflow.entities.Trace object is displayed (e.g.\u00c2\u00a0via IPython\u00e2\u0080\u0099s display function, or when it is the last value returned in a cell) When mlflow.search_traces() is called  Example 1: Generating a trace within a cell Traces can be generated by automatic tracing integrations (e.g.\u00c2\u00a0with mlflow.openai.autolog() ), or when you run a manually traced function. For example: [ ]: # Simple manual tracing example import mlflow @mlflow . trace def foo ( input ): return input + 1 # running foo() generates a trace foo ( 1 ) [ ]: # Automatic tracing with OpenAI import os from getpass import getpass if \"OPENAI_API_KEY\" not in os . environ : os . environ [ \"OPENAI_API_KEY\" ] = getpass ( \"Enter your OpenAI API key: \" ) [ ]: from openai import OpenAI import mlflow mlflow . openai . autolog () client = OpenAI () # creating a chat completion will generate a trace client . chat . completions . create ( model = \"gpt-4o-mini\" , messages = [{ \"role\" : \"user\" , \"content\" : \"hello!\" }], ) ",
        "id": "e86498ee20ecd37a83dc4adf527f65c9"
    },
    {
        "text": " Example 2: Displaying a Trace object The trace UI will also show up when an MLflow Trace entity is displayed. This can happen in two ways: Explicitly displaying a trace object with IPython\u00e2\u0080\u0099s display() When a trace object happens to be the last evaluated expression in a cell [ ]: # Explicitly calling `display()` trace = mlflow . get_last_active_trace () display ( trace ) # Even if the last expression does not result in a trace, # display(trace) will still trigger the UI display print ( \"Test\" ) [ ]: # Displaying as a result of the trace being the last expression trace  Example 3: Calling mlflow.search_traces() MLflow provides the mlflow.search_traces() API to conveniently search through all traces in an experiment. When this API is called in a Jupyter notebook, the trace UI will render all the traces in a paginated view. There is a limit to how many traces can be rendered in a single cell output. By default the maximum is 10, but this can be configured by setting the MLFLOW_MAX_TRACES_TO_DISPLAY_IN_NOTEBOOK environment variable. [ ]: mlflow . search_traces ( experiment_ids = [ experiment . experiment_id ])  Disabling the UI The display is enabled by default, but if you\u00e2\u0080\u0099d prefer for it not to be shown, you can run mlflow.tracing.disable_notebook_display() disable it. You will have to rerun the cells (or simply clear the cell outputs) in order to remove the displays that have already rendered. If you\u00e2\u0080\u0099d like to re-enable the auto-display functionality, simply call mlflow.tracing.enable_notebook_display() . [ ]: mlflow . tracing . disable_notebook_display () # no UI will be rendered trace [ ]: mlflow . tracing . enable_notebook_display () # re-enable display trace  Conclusion That\u00e2\u0080\u0099s the basics! We hope you\u00e2\u0080\u0099ll find the Jupyter integration useful. As always, please file an issue at https://github.com/mlflow/mlflow/issues if you find any problems, or if you want to leave any feedback. In the next few cells, we have a short RAG demo that will create a trace with more realistic data, so you can get a better feel of what working with this UI will be like. [ ]: from langchain_core.vectorstores import InMemoryVectorStore from langchain_openai import ChatOpenAI , OpenAIEmbeddings # define necessary RAG entities llm = ChatOpenAI ( model = \"gpt-4o-mini\" ) embeddings = OpenAIEmbeddings ( model = \"text-embedding-3-large\" ) vector_store = InMemoryVectorStore ( embeddings ) [ ]: import bs4 from langchain import hub from langchain_community.document_loaders import WebBaseLoader from langchain_core.runnables import RunnablePassthrough from langchain_text_splitters import RecursiveCharacterTextSplitter # generate sample doc chunks from the MLflow documentation loader = WebBaseLoader ( web_paths = ( \"https://mlflow.org/docs/latest/llms/tracing/index.html\" ,), bs_kwargs = { \"parse_only\" : bs4 . SoupStrainer ( class_ = ( \"document\" ))}, ) docs = loader . load () # add documents to the vector store text_splitter = RecursiveCharacterTextSplitter ( chunk_size = 1000 , chunk_overlap = 200 ) all_splits = text_splitter . split_documents ( docs ) vector_store . add_documents ( documents = all_splits ) retriever = vector_store . as_retriever ( search_type = \"similarity\" , search_kwargs = { \"k\" : 3 }, ) # Define prompt for question-answering prompt = hub . pull ( \"rlm/rag-prompt\" ) [ ]: # define our chain chain = { \"context\" : retriever , \"question\" : RunnablePassthrough ()} | prompt | llm [ ]: import mlflow # call the langchain autolog function so that traces will be generated mlflow . langchain . autolog () response = chain . invoke ( \"What is MLflow Tracing?\" ) Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "a226b1c5b649d8a80e1fc83c7ff54a72"
    },
    {
        "text": "MLflow Tracing for LLM Observability 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow Tracing for LLM Observability Automatic Tracing Jupyter Notebook integration Tracing Fluent APIs Tracing Client APIs Searching and Retrieving Traces Deleting Traces Data Model and Schema Trace Tags Async Logging Using OpenTelemetry Collector for Exporting Traces FAQ Tracing Concepts MLflow Tracing Schema Contributing to MLflow Tracing Searching and Retrieving Traces MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs LLM Tracking in MLflow MLflow Tracing Automatic Tracing Jupyter Notebook integration Tracing Fluent APIs Initiating a Trace Trace Decorator What is captured? Error Handling with Traces Parent-child relationships Span Type Context Handler Function wrapping Tracing Client APIs Starting a Trace Adding a Child Span Ending a Span Ending a Trace Searching and Retrieving Traces Deleting Traces Data Model and Schema Trace Tags Setting Tags on an Active Trace Setting Tags on a Finished Trace Setting Tags via the MLflow UI Async Logging Using OpenTelemetry Collector for Exporting Traces Configurations FAQ Q: Can I disable and re-enable tracing globally? Q: How can I associate a trace with an MLflow Run? Q: Can I use the fluent API and the client API together? Q: How can I add custom metadata to a span? Fluent API Client API Q: I cannot open my trace in the MLflow UI. What should I do? Q. How to group multiple traces within a single conversation session? Q: How to find a particular span within a trace? Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow Tracing for LLM Observability  MLflow Tracing for LLM Observability MLflow Tracing is a feature that enhances LLM observability in your Generative AI (GenAI) applications by capturing detailed information about the execution of your application\u00e2\u0080\u0099s services.\nTracing provides a way to record the inputs, outputs, and metadata associated with each intermediate step of a request, enabling you to easily pinpoint the source of bugs and unexpected behaviors. MLflow offers a number of different options to enable tracing of your GenAI applications. Automated tracing : MLflow provides fully automated integrations with various GenAI libraries such as LangChain, OpenAI, LlamaIndex, DSPy, AutoGen, and more that can be activated by simply enabling mlflow.<library>.autolog() . Manual trace instrumentation with high-level fluent APIs : Decorators, function wrappers and context managers via the fluent API allow you to add tracing functionality with minor code modifications. Low-level client APIs for tracing : The MLflow client API provides a thread-safe way to handle trace implementations, even in aysnchronous modes of operation. If you are new to the tracing or observability concepts, we recommend starting with the Tracing Concepts Overview guide. Note MLflow Tracing support is available with the MLflow 2.14.0 release.  Table of Contents  Automatic Tracing  Jupyter Notebook integration  Tracing Fluent APIs  Tracing Client APIs  Searching and Retrieving Traces  Deleting Traces  Data Model and Schema  Trace Tags  Async Logging  Using OpenTelemetry Collector for Exporting Traces  FAQ ",
        "id": "371fe56b5a07dffc1aa86c42a50c4a68"
    },
    {
        "text": " Automatic Tracing Hint Is your favorite library missing from the list? Consider contributing to MLflow Tracing or submitting a feature request to our Github repository. The easiest way to get started with MLflow Tracing is to leverage the built-in capabilities with MLflow\u00e2\u0080\u0099s integrated libraries. MLflow provides automatic tracing capabilities for some of the integrated libraries such as\nLangChain, OpenAI, LlamaIndex, and AutoGen. For these libraries, you can instrument your code with\njust a single command mlflow.<library>.autolog() and MLflow will automatically log traces\nfor model/API invocations to the active MLflow Experiment.  LangChain / LangGraph  OpenAI  Swarm  Ollama  Instructor  LlamaIndex  DSPy  AutoGen  Gemini  LiteLLM  Anthropic  CrewAI  Groq  LangChain / LangGraph Automatic Tracing As part of the LangChain autologging integration, traces are logged to the active MLflow Experiment when calling invocation APIs on chains. You can enable tracing\nfor LangChain and LangGraph by calling the mlflow.langchain.autolog() function. import mlflow mlflow . langchain . autolog () In the full example below, the model and its associated metadata will be logged as a run, while the traces are logged separately to the active experiment. To learn more, please visit LangChain Autologging documentation . Note This example has been confirmed working with the following requirement versions: pip install mlflow == 2 .18.0 langchain == 0 .3.0 langchain-openai == 0 .2.9 import mlflow import os from langchain.prompts import PromptTemplate from langchain_core.output_parsers import StrOutputParser from langchain_openai import ChatOpenAI mlflow . set_experiment ( \"LangChain Tracing\" ) # Enabling autolog for LangChain will enable trace logging. mlflow . langchain . autolog () llm = ChatOpenAI ( model = \"gpt-4o-mini\" , temperature = 0.7 , max_tokens = 1000 ) prompt_template = PromptTemplate . from_template ( \"Answer the question as if you are {person} , fully embodying their style, wit, personality, and habits of speech. \" \"Emulate their quirks and mannerisms to the best of your ability, embracing their traits\u00e2\u0080\u0094even if they aren't entirely \" \"constructive or inoffensive. The question is: {question} \" ) chain = prompt_template | llm | StrOutputParser () # Let's test another call chain . invoke ( { \"person\" : \"Linus Torvalds\" , \"question\" : \"Can I just set everyone's access to sudo to make things easier?\" , } ) If we navigate to the MLflow UI, we can see not only the model that has been auto-logged, but the traces as well, as shown in the video above.  OpenAI Automatic Tracing The MLflow OpenAI flavor \u00e2\u0080\u0099s autologging feature has a direct integration with MLflow tracing. When OpenAI autologging is enabled with mlflow.openai.autolog() ,\nusage of the OpenAI SDK will automatically record generated traces during interactive development. import mlflow mlflow . openai . autolog () For example, the code below will log traces to the currently active experiment (in this case, the activated experiment \"OpenAI\" , set through the use\nof the mlflow.set_experiment() API).\nTo learn more about OpenAI autologging, you can view the documentation here . import os import openai import mlflow # Calling the autolog API will enable trace logging by default. mlflow . openai . autolog () mlflow . set_experiment ( \"OpenAI\" ) openai_client = openai . OpenAI ( api_key = os . environ . get ( \"OPENAI_API_KEY\" )) messages = [ { \"role\" : \"user\" , \"content\" : \"How can I improve my resting metabolic rate most effectively?\" , } ] response = openai_client . chat . completions . create ( model = \"gpt-4o\" , messages = messages , temperature = 0.99 , ) print ( response ) The logged trace, associated with the OpenAI experiment, can be seen in the MLflow UI, as shown below: ",
        "id": "be2120f7304d70c0d63b2e58b302ca93"
    },
    {
        "text": " OpenAI Swarm Automatic Tracing The MLflow OpenAI flavor supports automatic tracing for Swarm , a multi-agent orchestration\nframework from OpenAI. To enable tracing for Swarm , just call mlflow.openai.autolog() before running your multi-agent interactions. MLflow will trace all LLM interactions,\ntool calls, and agent operations automatically. import mlflow mlflow . openai . autolog () For example, the code below will run the simplest example of multi-agent interaction using OpenAI Swarm. import mlflow from swarm import Swarm , Agent # Calling the autolog API will enable trace logging by default. mlflow . openai . autolog () mlflow . set_experiment ( \"OpenAI Swarm\" ) client = Swarm () def transfer_to_agent_b (): return agent_b agent_a = Agent ( name = \"Agent A\" , instructions = \"You are a helpful agent.\" , functions = [ transfer_to_agent_b ], ) agent_b = Agent ( name = \"Agent B\" , instructions = \"Only speak in Haikus.\" , ) response = client . run ( agent = agent_a , messages = [{ \"role\" : \"user\" , \"content\" : \"I want to talk to agent B.\" }], ) print ( response ) The logged trace, associated with the OpenAI Swarm experiment, can be seen in the MLflow UI, as shown below:  Ollama Automatic Tracing Ollama is an open-source platform that enables users to run large language models (LLMs) locally on their devices, such as Llama 3.2, Gemma 2, Mistral, Code Llama, and more. Since the local LLM endpoint served by Ollama is compatible with the OpenAI API, you can query it via OpenAI SDK and enable tracing for Ollama with mlflow.openai.autolog() . Any LLM interactions via Ollama will be recorded to the active MLflow Experiment. Run the Ollama server with the desired LLM model. ollama run llama3.2:1b Enable auto-tracing for OpenAI SDK. import mlflow mlflow . openai . autolog () # Optional, create an experiment to store traces mlflow . set_experiment ( \"Ollama\" ) Query the LLM and see the traces in the MLflow UI. from openai import OpenAI client = OpenAI ( base_url = \"http://localhost:11434/v1\" , # The local Ollama REST endpoint api_key = \"dummy\" , # Required to instantiate OpenAI client, it can be a random string ) response = client . chat . completions . create ( model = \"llama3.2:1b\" , messages = [ { \"role\" : \"system\" , \"content\" : \"You are a science teacher.\" }, { \"role\" : \"user\" , \"content\" : \"Why is the sky blue?\" }, ], )  Instructor Automatic Tracing Instructor is an open-source Python library built on top of Pydantic, simplifying structured LLM outputs with validation, retries, and streaming. MLflow Tracing works with Instructor by enabling auto-tracing for the underlying LLM libraries.\nFor example, if you use Instructor for OpenAI LLMs, you can enable tracing with mlflow.openai.autolog() and the generated traces will capture the structured outputs from Instructor. Similarly, you can also trace Instructor with other LLM providers, such as Anthropic, Gemini, and LiteLLM, by enabling the corresponding autologging in MLflow. import instructor from pydantic import BaseModel from openai import OpenAI # Use other autologging function e.g., mlflow.anthropic.autolog() if you are using Instructor with different LLM providers mlflow . openai . autolog () # Optional, create an experiment to store traces mlflow . set_experiment ( \"Instructor\" ) # Use Instructor as usual class ExtractUser ( BaseModel ): name : str age : int client = instructor . from_openai ( OpenAI ()) res = client . chat . completions . create ( model = \"gpt-4o-mini\" , response_model = ExtractUser , messages = [{ \"role\" : \"user\" , \"content\" : \"John Doe is 30 years old.\" }], ) print ( f \"Name: { res . name } , Age: { res . age } \" ) ",
        "id": "a9767898ba39eecbf0edefce0083bd61"
    },
    {
        "text": " LlamaIndex Automatic Tracing The MLflow LlamaIndex flavor \u00e2\u0080\u0099s autologging feature has a direct integration with MLflow tracing. When LlamaIndex autologging is enabled with mlflow.llama_index.autolog() , invocation of components\nsuch as LLMs, agents, and query/chat engines will automatically record generated traces during interactive development. import mlflow mlflow . llama_index . autolog () To see the full example of tracing LlamaIndex, please visit LLamaIndex Tracing documentation .  DSPy Automatic Tracing The MLflow DSPy flavor \u00e2\u0080\u0099s autologging feature has a direct integration with MLflow tracing. When DSPy autologging is enabled with mlflow.dspy.autolog() , invocation of components\nsuch as LMs, Adapters and Modules, will automatically record generated traces during interactive development. import mlflow import dspy # Enable tracing for DSPy mlflow . dspy . autolog () # Set an experiment to log the traces to mlflow . set_experiment ( \"DSPy Tracing\" ) # Define a simple ChainOfThought model and run it lm = dspy . LM ( \"openai/gpt-4o-mini\" ) dspy . configure ( lm = lm ) # Define a simple summarizer model and run it class SummarizeSignature ( dspy . Signature ): \"\"\"Given a passage, generate a summary.\"\"\" passage : str = dspy . InputField ( desc = \"a passage to summarize\" ) summary : str = dspy . OutputField ( desc = \"a one-line summary of the passage\" ) class Summarize ( dspy . Module ): def __init__ ( self ): self . summarize = dspy . ChainOfThought ( SummarizeSignature ) def forward ( self , passage : str ): return self . summarize ( passage = passage ) summarizer = Summarize () summarizer ( passage = ( \"MLflow Tracing is a feature that enhances LLM observability in your Generative AI (GenAI) applications \" \"by capturing detailed information about the execution of your application's services. Tracing provides \" \"a way to record the inputs, outputs, and metadata associated with each intermediate step of a request, \" \"enabling you to easily pinpoint the source of bugs and unexpected behaviors.\" ) )  AutoGen Automatic Tracing MLflow Tracing ensures observability for your AutoGen application that involves complex multi-agent interactions. You can enable auto-tracing by calling mlflow.autogen.autolog() , then the internal steps of the agents chat session will be logged to the active MLflow Experiment. import mlflow mlflow . autogen . autolog () To see the full example of tracing AutoGen, please refer to the AutoGen Tracing example .  Gemini Automatic Tracing MLflow Tracing ensures observability for your interactions with Gemini AI models.\nWhen Gemini autologging is enabled with mlflow.gemini.autolog() ,\nusage of the Gemini SDK will automatically record generated traces during interactive development.\nNote that only synchronous calls for text interactions are supported. Asynchronous API is not traced, and full inputs cannnot be recorded for multi-modal inputs. import mlflow mlflow . gemini . autolog () To see the full example of tracing Gemini, please refer to the Gemini Tracing example .  LiteLLM Automatic Tracing LiteLLM allows developers to call all LLM APIs using the OpenAI format. MLflow support auto-tracing for LiteLLM. You can enable it by calling mlflow.litellm.autolog() , then any LLM interactions via LiteLLM will be recorded to the active MLflow Experiment, including various metadata such as token usage, cost, cache hit, and more. import mlflow mlflow . litellm . autolog () # Call Anthropic API via LiteLLM response = litellm . completion ( model = \"claude-3-opus-20240229\" , messages = [{ \"role\" : \"system\" , \"content\" : \"Hey! how's it going?\" }], ) ",
        "id": "17315f4d2a3583961e841620c18afc0f"
    },
    {
        "text": " Anthropic Automatic Tracing MLflow Tracing ensures observability for your interactions with Anthropic AI models.\nWhen Anthropic autologging is enabled with mlflow.anthropic.autolog() ,\nusage of the Anthropic SDK will automatically record generated traces during interactive development.\nNote that only synchronous calls for text interactions are supported.\nAsynchronous API and streaming methods are not traced. import mlflow mlflow . anthropic . autolog () To see the full example of tracing Anthropic, please refer to the Anthropic Tracing example .  CrewAI Automatic Tracing MLflow Tracing ensures observability for the interactions of CrewAI agents.\nWhen CrewAI autologging is enabled with mlflow.crewai.autolog() ,\ntraces are generated for the usage of the CrewAI framework.\nNote that asynchronous task and kickoff are not supported now. import mlflow mlflow . crewai . autolog () To see the full example of tracing CrewAI, please refer to the CrewAI Tracing example .  Groq Automatic Tracing MLflow Tracing ensures observability for your interactions with Groq AI models.\nWhen Groq autologging is enabled with mlflow.groq.autolog() ,\nusage of the Groq SDK will automatically record generated traces during interactive development.\nNote that only synchronous calls are supported.\nAsynchronous API and streaming methods are not traced. import mlflow mlflow . groq . autolog () To see the full example of tracing Groq, please refer to the Groq Tracing example .  Jupyter Notebook integration Note Jupyter integration is available in MLflow 2.20 and above The trace UI is also available within Jupyter notebooks! This feature requires using an MLflow Tracking Server , as\nthis is where the UI assets are fetched from. To get started, simply ensure that the MLflow\nTracking URI is set to your tracking server (e.g. mlflow.set_tracking_uri(\"http://localhost:5000\") ). By default, the trace UI will automatically be displayed for the following events: When the cell code generates a trace (e.g. via automatic tracing , or by running a manually traced function) When mlflow.search_traces() is called When a mlflow.entities.Trace object is displayed (e.g. via IPython\u00e2\u0080\u0099s display function, or when it is the last value returned in a cell) To disable the display, simply call mlflow.tracing.disable_notebook_display() , and rerun the cell\ncontaining the UI. To enable it again, call mlflow.tracing.enable_notebook_display() . For a more complete example, try running this demo notebook !  Tracing Fluent APIs MLflow\u00e2\u0080\u0099s fluent APIs provide a straightforward way to add tracing to your functions and code blocks.\nBy using decorators, function wrappers, and context managers, you can easily capture detailed trace data with minimal code changes. As a comparison between the fluent and the client APIs for tracing, the figure below illustrates the differences in complexity between the two APIs,\nwith the fluent API being more concise and the recommended approach if your tracing use case can support using the higher-level APIs. This section will cover how to initiate traces using these fluent APIs.  Initiating a Trace In this section, we will explore different methods to initiate a trace using MLflow\u00e2\u0080\u0099s fluent APIs. These methods allow you to add tracing\nfunctionality to your code with minimal modifications, enabling you to capture detailed information about the execution of your functions and workflows. ",
        "id": "c1729f3887f55c4a897afff9eebc65ab"
    },
    {
        "text": " Trace Decorator The trace decorator allows you to automatically capture the inputs and outputs of a function by simply adding the @mlflow.trace decorator\nto its definition. This approach is ideal for quickly adding tracing to individual functions without significant changes to your existing code. import mlflow # Create a new experiment to log the trace to mlflow . set_experiment ( \"Tracing Demo\" ) # Mark any function with the trace decorator to automatically capture input(s) and output(s) @mlflow . trace def some_function ( x , y , z = 2 ): return x + ( y - z ) # Invoking the function will generate a trace that is logged to the active experiment some_function ( 2 , 4 ) You can add additional metadata to the tracing decorator as follows: @mlflow . trace ( name = \"My Span\" , span_type = \"func\" , attributes = { \"a\" : 1 , \"b\" : 2 }) def my_func ( x , y ): return x + y When adding additional metadata to the trace decorator constructor, these additional components will be logged along with the span entry within\nthe trace that is stored within the active MLflow experiment. Since MLflow 2.16.0, the trace decorator also supports async functions: from openai import AsyncOpenAI client = AsyncOpenAI () @mlflow . trace async def async_func ( message : str ): return await client . chat . completion . create ( model = \"gpt-4o\" , messages = [{ \"role\" : \"user\" , \"content\" : message }] ) await async_func ( \"What is MLflow Tracing?\" )  What is captured? If we navigate to the MLflow UI, we can see that the trace decorator automatically captured the following information, in addition to the basic\nmetadata associated with any span (start time, end time, status, etc): Inputs : In the case of our decorated function, this includes the state of all input arguments (including the default z value that is applied). Response : The output of the function is also captured, in this case the result of the addition and subtraction operations. Trace Name : The name of the decorated function.  Error Handling with Traces If an Exception is raised during processing of a trace-instrumented operation, an indication will be shown within the UI that the invocation was not\nsuccessful and a partial capture of data will be available to aid in debugging. Additionally, details about the Exception that was raised will be included\nwithin Events of the partially completed span, further aiding the identification of where issues are occurring within your code.  Parent-child relationships When using the trace decorator, each decorated function will be treated as a separate span within the trace. The relationship between dependent function calls\nis handled directly through the native call excecution order within Python. For example, the following code will introduce two \u00e2\u0080\u009cchild\u00e2\u0080\u009d spans to the main\nparent span, all using decorators. import mlflow @mlflow . trace ( span_type = \"func\" , attributes = { \"key\" : \"value\" }) def add_1 ( x ): return x + 1 @mlflow . trace ( span_type = \"func\" , attributes = { \"key1\" : \"value1\" }) def minus_1 ( x ): return x - 1 @mlflow . trace ( name = \"Trace Test\" ) def trace_test ( x ): step1 = add_1 ( x ) return minus_1 ( step1 ) trace_test ( 4 ) If we look at this trace from within the MLflow UI, we can see the relationship of the call order shown in the structure of the trace. ",
        "id": "e6888f879c5222884f00f4905a91f480"
    },
    {
        "text": " Span Type Span types are a way to categorize spans within a trace. By default, the span type is set to \"UNKNOWN\" when using the trace decorator. MLflow provides a set of predefined span types for common use cases, while also allowing you to setting custom span types. The following span types are available: Span Type Description \"LLM\" Represents a call to an LLM endpoint or a local model. \"CHAT_MODEL\" Represents a query to a chat model. This is a special case of an LLM interaction. \"CHAIN\" Represents a chain of operations. \"AGENT\" Represents an autonomous agent operation. \"TOOL\" Represents a tool execution (typically by an agent), such as querying a search engine. \"EMBEDDING\" Represents a text embedding operation. \"RETRIEVER\" Represents a context retrieval operation, such as querying a vector database. \"PARSER\" Represents a parsing operation, transforming text into a structured format. \"RERANKER\" Represents a re-ranking operation, ordering the retrieved contexts based on relevance. \"UNKNOWN\" A default span type that is used when no other span type is specified. To set a span type, you can pass the span_type parameter to the @mlflow.trace decorator or mlflow.start_span context manager. When you are using automatic tracing , the span type is automatically set by MLflow. import mlflow from mlflow.entities import SpanType # Using a built-in span type @mlflow . trace ( span_type = SpanType . RETRIEVER ) def retrieve_documents ( query : str ): ... # Setting a custom span type with mlflow . start_span ( name = \"add\" , span_type = \"MATH\" ) as span : span . set_inputs ({ \"x\" : z , \"y\" : y }) z = x + y span . set_outputs ({ \"z\" : z }) print ( span . span_type ) # Output: MATH  Context Handler The context handler provides a way to create nested traces or spans, which can be useful for capturing complex interactions within your code.\nBy using the mlflow.start_span() context manager, you can group multiple traced functions under a single parent span, making it easier to understand\nthe relationships between different parts of your code. The context handler is recommended when you need to refine the scope of data capture for a given span. If your code is logically constructed such that\nindividual calls to services or models are contained within functions or methods, on the other hand, using the decorator approach is more straight-forward\nand less complex. import mlflow @mlflow . trace def first_func ( x , y = 2 ): return x + y @mlflow . trace def second_func ( a , b = 3 ): return a * b def do_math ( a , x , operation = \"add\" ): # Use the fluent API context handler to create a new span with mlflow . start_span ( name = \"Math\" ) as span : # Specify the inputs and attributes that will be associated with the span span . set_inputs ({ \"a\" : a , \"x\" : x }) span . set_attributes ({ \"mode\" : operation }) # Both of these functions are decorated for tracing and will be associated # as 'children' of the parent 'span' defined with the context handler first = first_func ( x ) second = second_func ( a ) result = None if operation == \"add\" : result = first + second elif operation == \"subtract\" : result = first - second else : raise ValueError ( f \"Unsupported Operation Mode: { operation } \" ) # Specify the output result to the span span . set_outputs ({ \"result\" : result }) return result When calling the do_math function, a trace will be generated that has the root span (parent) defined as the\ncontext handler with mlflow.start_span(): call. The first_func and second_func calls will be associated as child spans\nto this parent span due to the fact that they are both decorated functions (having @mlflow.trace decorated on the function definition). Running the following code will generate a trace. do_math ( 8 , 3 , \"add\" ) This trace can be seen within the MLflow UI: ",
        "id": "54243fce98273d78bef2db85fe367381"
    },
    {
        "text": " Function wrapping Function wrapping provides a flexible way to add tracing to existing functions without modifying their definitions. This is particularly useful when\nyou want to add tracing to third-party functions or functions defined outside of your control. By wrapping an external function with mlflow.trace() , you can\ncapture its inputs, outputs, and execution context. import math import mlflow mlflow . set_experiment ( \"External Function Tracing\" ) def invocation ( x , y = 4 , exp = 2 ): # Initiate a context handler for parent logging with mlflow . start_span ( name = \"Parent\" ) as span : span . set_attributes ({ \"level\" : \"parent\" , \"override\" : y == 4 }) span . set_inputs ({ \"x\" : x , \"y\" : y , \"exp\" : exp }) # Wrap an external function instead of modifying traced_pow = mlflow . trace ( math . pow ) # Call the wrapped function as you would call it directly raised = traced_pow ( x , exp ) # Wrap another external function traced_factorial = mlflow . trace ( math . factorial ) factorial = traced_factorial ( int ( raised )) # Wrap another and call it directly response = mlflow . trace ( math . sqrt )( factorial ) # Set the outputs to the parent span prior to returning span . set_outputs ({ \"result\" : response }) return response for i in range ( 8 ): invocation ( i ) The screenshot below shows our external function wrapping runs within the MLflow UI.  Tracing Client APIs Note Client APIs are advanced features. We recommend using the client APIs only when you have specific requirements that are not met by the other APIs. The MLflow client API provides a comprehensive set of thread-safe methods for manually managing traces. These APIs allow for fine-grained\ncontrol over tracing, enabling you to create, manipulate, and retrieve traces programmatically. This section will cover how to use these APIs\nto manually trace a model, providing step-by-step instructions and examples.  Starting a Trace Unlike with the fluent API, the MLflow Trace Client API requires that you explicitly start a trace before adding child spans. This initial API call\nstarts the root span for the trace, providing a context request_id that is used for associating subsequent spans to the root span. To start a new trace, use the mlflow.client.MlflowClient.start_trace() method. This method creates a new trace and returns the root span object. from mlflow import MlflowClient client = MlflowClient () # Start a new trace root_span = client . start_trace ( \"my_trace\" ) # The request_id is used for creating additional spans that have a hierarchical association to this root span request_id = root_span . request_id  Adding a Child Span Once a trace is started, you can add child spans to it with the mlflow.client.MlflowClient.start_span() API. Child spans allow you to break down the trace into smaller, more manageable segments,\neach representing a specific operation or step within the overall process. # Create a child span child_span = client . start_span ( name = \"child_span\" , request_id = request_id , parent_id = root_span . span_id , inputs = { \"input_key\" : \"input_value\" }, attributes = { \"attribute_key\" : \"attribute_value\" }, ) ",
        "id": "794bb90df793e22853f4111539b7cd73"
    },
    {
        "text": " Ending a Span After performing the operations associated with a span, you must end the span explicitly using the mlflow.client.MlflowClient.end_span() method. Make note of the two required fields\nthat are in the API signature: request_id : The identifier associated with the root span span_id : The identifier associated with the span that is being ended In order to effectively end a particular span, both the root span (returned from calling start_trace ) and the targeted span (returned from calling start_span )\nneed to be identified when calling the end_span API.\nThe initiating request_id can be accessed from any parent span object\u00e2\u0080\u0099s properties. Note Spans created via the Client API will need to be terminated manually. Ensure that all spans that have been started with the start_span API\nhave been ended with the end_span API. # End the child span client . end_span ( request_id = child_span . request_id , span_id = child_span . span_id , outputs = { \"output_key\" : \"output_value\" }, attributes = { \"custom_attribute\" : \"value\" }, )  Ending a Trace To complete the trace, end the root span using the mlflow.client.MlflowClient.end_trace() method. This will also ensure that all associated child\nspans are properly ended. # End the root span (trace) client . end_trace ( request_id = request_id , outputs = { \"final_output_key\" : \"final_output_value\" }, attributes = { \"token_usage\" : \"1174\" }, )   Searching and Retrieving Traces You can search for traces based on various criteria using the mlflow.client.MlflowClient.search_traces() method or the fluent API mlflow.search_traces() .\nSee Searching and Retrieving Traces for the usages of these APIs.  Deleting Traces You can delete traces based on specific criteria using the mlflow.client.MlflowClient.delete_traces() method. This method allows you to delete traces by experiment ID , maximum timestamp , or request IDs . Tip Deleting a trace is an irreversible process. Ensure that the setting provided within the delete_traces API meet the intended range for deletion. import time # Get the current timestamp in milliseconds current_time = int ( time . time () * 1000 ) # Delete traces older than a specific timestamp deleted_count = client . delete_traces ( experiment_id = \"1\" , max_timestamp_millis = current_time , max_traces = 10 )  Data Model and Schema To explore the structure and schema of MLflow Tracing, please see the Tracing Schema guide.  Trace Tags Tags can be added to traces to provide additional metadata at the trace level. For example, you can attach a session ID to a trace to group traces by a conversation session. MLflow provides APIs to set and delete tags on traces. Select the right API based on whether you want to set tags on an active trace or on an already finished trace. API / Method Use Case mlflow.update_current_trace() API. Setting tags on an active trace during the code execution. mlflow.client.MlflowClient.set_trace_tag() API Programmatically setting tags on a finished trace. MLflow UI Setting tags on a finished trace conveniently.  Setting Tags on an Active Trace If you are using automatic tracing or fluent APIs to create traces and want to add tags to the trace during its execution, you can use the mlflow.update_current_trace() function. For example, the following code example adds the \"fruit\": \"apple\" tag to the trace created for the my_func function: @mlflow . trace def my_func ( x ): mlflow . update_current_trace ( tags = { \"fruit\" : \"apple\" }) return x + 1 Note The : mlflow.update_current_trace() function adds the specified tag(s) to the current trace when the key is not already present. If the key is already present, it updates the key with the new value. ",
        "id": "dcb939596267caad356ab3abaf48b113"
    },
    {
        "text": " Setting Tags on a Finished Trace To set tags on a trace that has already been completed and logged in the backend store, use the mlflow.client.MlflowClient.set_trace_tag() method to set a tag on a trace,\nand the mlflow.client.MlflowClient.delete_trace_tag() method to remove a tag from a trace. # Get the request ID fof the most recently created trace trace = mlflow . get_last_active_trace () request_id = trace . info . request_id # Set a tag on a trace client . set_trace_tag ( request_id = request_id , key = \"tag_key\" , value = \"tag_value\" ) # Delete a tag from a trace client . delete_trace_tag ( request_id = request_id , key = \"tag_key\" )  Setting Tags via the MLflow UI Alternatively, you can update or delete tags on a trace from the MLflow UI. To do this, navigate to the trace tab, then click on the pencil icon next to the tag you want to update.  Async Logging By default, MLflow Traces are logged synchronously. This may introduce a performance overhead when logging Traces, especially when your MLflow Tracking Server is running on a remote server. If the performance overhead is a concern for you, you can enable asynchronous logging for tracing in MLflow 2.16.0 and later. To enable async logging for tracing, call mlflow.config.enable_async_logging() in your code. This will make the trace logging operation non-blocking and reduce the performance overhead. import mlflow mlflow . config . enable_async_logging () # Traces will be logged asynchronously with mlflow . start_span ( name = \"foo\" ) as span : span . set_inputs ({ \"a\" : 1 }) span . set_outputs ({ \"b\" : 2 }) # If you don't see the traces in the UI after waiting for a while, you can manually flush the traces # mlflow.flush_trace_async_logging() Note that the async logging does not fully eliminate the performance overhead. Some backend calls still need to be made synchronously and there are other factors such as data serialization. However, async logging can significantly reduce the overall overhead of logging traces, empirically about ~80% for typical workloads.  Using OpenTelemetry Collector for Exporting Traces Traces generated by MLflow are compatible with the OpenTelemetry trace specs .\nTherefore, MLflow Tracing supports exporting traces to an OpenTelemetry Collector, which can then be used to export traces to various backends such as Jaeger, Zipkin, and AWS X-Ray. By default, MLflow exports traces to the MLflow Tracking Server. To enable exporting traces to an OpenTelemetry Collector, set the OTEL_EXPORTER_OTLP_ENDPOINT environment variable (or OTEL_EXPORTER_OTLP_TRACES_ENDPOINT ) to the target URL of the OpenTelemetry Collector before starting any trace . import mlflow import os # Set the endpoint of the OpenTelemetry Collector os . environ [ \"OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\" ] = \"http://localhost:4317/v1/traces\" # Optionally, set the service name to group traces os . environ [ \"OTEL_SERVICE_NAME\" ] = \"<your-service-name>\" # Trace will be exported to the OTel collector at http://localhost:4317/v1/traces with mlflow . start_span ( name = \"foo\" ) as span : span . set_inputs ({ \"a\" : 1 }) span . set_outputs ({ \"b\" : 2 }) Warning MLflow only exports traces to a single destination. When  the OTEL_EXPORTER_OTLP_ENDPOINT environment variable is configured, MLflow will not export traces to the MLflow Tracking Server and you will not see traces in the MLflow UI. Similarly, if you deploy the model to the Databricks Model Serving with tracing enabled , using the OpenTelemetry Collector will result in traces not being recorded in the Inference Table.  Configurations MLflow uses the standard OTLP Exporter for exporting traces to OpenTelemetry Collector instances. Thereby, you can use all of the configurations supported by OpenTelemetry. The following example configures the OTLP Exporter to use HTTP protocol instead of the default gRPC and sets custom headers: export OTEL_EXPORTER_OTLP_TRACES_ENDPOINT = \"http://localhost:4317/v1/traces\" export OTEL_EXPORTER_OTLP_TRACES_PROTOCOL = \"http/protobuf\" export OTEL_EXPORTER_OTLP_TRACES_HEADERS = \"api_key=12345\"  FAQ ",
        "id": "ed971d680f99a47ffb256997db346d91"
    },
    {
        "text": " Q: Can I disable and re-enable tracing globally? Yes. There are two fluent APIs that are used for blanket enablement or disablement of the MLflow Tracing feature in order to support\nusers who may not wish to record interactions with their trace-enabled models for a brief period, or if they have concerns about long-term storage\nof data that was sent along with a request payload to a model in interactive mode. To disable tracing, the mlflow.tracing.disable() API will cease the collection of trace data from within MLflow and will not log\nany data to the MLflow Tracking service regarding traces. To enable tracing (if it had been temporarily disabled), the mlflow.tracing.enable() API will re-enable tracing functionality for instrumented models\nthat are invoked.  Q: How can I associate a trace with an MLflow Run? If a trace is generated within a run context, the recorded traces to an active Experiment will be associated with the active Run. For example, in the following code, the traces are generated within the start_run context. import mlflow # Create and activate an Experiment mlflow . set_experiment ( \"Run Associated Tracing\" ) # Start a new MLflow Run with mlflow . start_run () as run : # Initiate a trace by starting a Span context from within the Run context with mlflow . start_span ( name = \"Run Span\" ) as parent_span : parent_span . set_inputs ({ \"input\" : \"a\" }) parent_span . set_outputs ({ \"response\" : \"b\" }) parent_span . set_attribute ( \"a\" , \"b\" ) # Initiate a child span from within the parent Span's context with mlflow . start_span ( name = \"Child Span\" ) as child_span : child_span . set_inputs ({ \"input\" : \"b\" }) child_span . set_outputs ({ \"response\" : \"c\" }) child_span . set_attributes ({ \"b\" : \"c\" , \"c\" : \"d\" }) When navigating to the MLflow UI and selecting the active Experiment, the trace display view will show the run that is associated with the trace, as\nwell as providing a link to navigate to the run within the MLflow UI. See the below video for an example of this in action. You can also programmatically retrieve the traces associated to a particular Run by using the mlflow.client.MlflowClient.search_traces() method. from mlflow import MlflowClient client = MlflowClient () # Retrieve traces associated with a specific Run traces = client . search_traces ( run_id = run . info . run_id ) print ( traces )  Q: Can I use the fluent API and the client API together? You definitely can. However, the Client API is much more verbose than the fluent API and is designed for more complex use cases where you need\nto control asynchronous tasks for which a context manager will not have the ability to handle an appropriate closure over the context. Mixing the two, while entirely possible, is not generally recommended. For example, the following will work: import mlflow # Initiate a fluent span creation context with mlflow . start_span ( name = \"Testing!\" ) as span : # Use the client API to start a child span child_span = client . start_span ( name = \"Child Span From Client\" , request_id = span . request_id , parent_id = span . span_id , inputs = { \"request\" : \"test input\" }, attributes = { \"attribute1\" : \"value1\" }, ) # End the child span client . end_span ( request_id = span . request_id , span_id = child_span . span_id , outputs = { \"response\" : \"test output\" }, attributes = { \"attribute2\" : \"value2\" }, ) Warning Using the fluent API to manage a child span of a client-initiated root span or child span is not possible.\nAttempting to start a start_span context handler while using the client API will result in two traces being created,\none for the fluent API and one for the client API.  Q: How can I add custom metadata to a span? There are several ways. ",
        "id": "4c85ae9e7c8b13691f672fbf319093be"
    },
    {
        "text": " Fluent API Within the mlflow.start_span() constructor itself. with mlflow . start_span ( name = \"Parent\" , attributes = { \"attribute1\" : \"value1\" , \"attribute2\" : \"value2\" } ) as span : span . set_inputs ({ \"input1\" : \"value1\" , \"input2\" : \"value2\" }) span . set_outputs ({ \"output1\" : \"value1\" , \"output2\" : \"value2\" }) Using the set_attribute or set_attributes methods on the span object returned from the start_span returned object. with mlflow . start_span ( name = \"Parent\" ) as span : # Set multiple attributes span . set_attributes ({ \"attribute1\" : \"value1\" , \"attribute2\" : \"value2\" }) # Set a single attribute span . set_attribute ( \"attribute3\" , \"value3\" )  Client API When starting a span, you can pass in the attributes as part of the start_trace and start_span method calls. parent_span = client . start_trace ( name = \"Parent Span\" , attributes = { \"attribute1\" : \"value1\" , \"attribute2\" : \"value2\" } ) child_span = client . start_span ( name = \"Child Span\" , request_id = parent_span . request_id , parent_id = parent_span . span_id , attributes = { \"attribute1\" : \"value1\" , \"attribute2\" : \"value2\" } ) Utilize the set_attribute or set_attributes APIs directly on the Span objects. parent_span = client . start_trace ( name = \"Parent Span\" , attributes = { \"attribute1\" : \"value1\" , \"attribute2\" : \"value2\" } ) # Set a single attribute parent_span . set_attribute ( \"attribute3\" , \"value3\" ) # Set multiple attributes parent_span . set_attributes ({ \"attribute4\" : \"value4\" , \"attribute5\" : \"value5\" }) Set attributes when ending a span or the entire trace. client . end_span ( request_id = parent_span . request_id , span_id = child_span . span_id , attributes = { \"attribute1\" : \"value1\" , \"attribute2\" : \"value2\" }, ) client . end_trace ( request_id = parent_span . request_id , attributes = { \"attribute3\" : \"value3\" , \"attribute4\" : \"value4\" }, )  Q: I cannot open my trace in the MLflow UI. What should I do? There are multiple possible reasons why a trace may not be viewable in the MLflow UI. The trace is not completed yet : If the trace is still being collected, MLflow cannot display spans in the UI. Ensure that all spans are properly ended with either \u00e2\u0080\u009cOK\u00e2\u0080\u009d or \u00e2\u0080\u009cERROR\u00e2\u0080\u009d status. The browser cache is outdated : When you upgrade MLflow to a new version, the browser cache may contain outdated data and prevent the UI from displaying traces correctly. Clear your browser cache (Shift+F5) and refresh the page. ",
        "id": "519cb6ffdd3f34207f9ad02e87a74daa"
    },
    {
        "text": " Q. How to group multiple traces within a single conversation session? In conversational AI applications, it is common that users interact with the model multiple times within a single conversation session. Since each interaction generates a trace in the typical MLflow setup, it is useful to group these traces together to analyze the conversation as a whole. You can achieve this by attaching the session ID as a tag to each trace. The following example shows how to use session ID in a chat model that has been implemented using the mlflow.pyfunc.ChatModel class. Refer to the Trace Tags section for more information on how to set tags on traces. import mlflow from mlflow.entities import SpanType from mlflow.types.llm import ChatMessage , ChatParams , ChatCompletionResponse import openai from typing import Optional mlflow . set_experiment ( \"Tracing Session ID Demo\" ) class ChatModelWithSession ( mlflow . pyfunc . ChatModel ): @mlflow . trace ( span_type = SpanType . CHAT_MODEL ) def predict ( self , context , messages : list [ ChatMessage ], params : Optional [ ChatParams ] = None ) -> ChatCompletionResponse : if session_id := ( params . custom_inputs or {}) . get ( \"session_id\" ): # Set session ID tag on the current trace mlflow . update_current_trace ( tags = { \"session_id\" : session_id }) response = openai . OpenAI () . chat . completions . create ( messages = [ m . to_dict () for m in messages ], model = \"gpt-4o-mini\" , ) return ChatCompletionResponse . from_dict ( response . to_dict ()) model = ChatModelWithSession () # Invoke the chat model multiple times with the same session ID session_id = \"123\" messages = [ ChatMessage ( role = \"user\" , content = \"What is MLflow Tracing?\" )] response = model . predict ( None , messages , ChatParams ( custom_inputs = { \"session_id\" : session_id }) ) # Invoke again with the same session ID messages . append ( ChatMessage ( role = \"assistant\" , content = response . choices [ 0 ] . message . content ) ) messages . append ( ChatMessage ( role = \"user\" , content = \"How to get started?\" )) response = model . predict ( None , messages , ChatParams ( custom_inputs = { \"session_id\" : session_id }) ) The above code creates two new traces with the same session ID tag. Within the MLflow UI, you can search for these traces that have this defined session ID using tag.session_id = '123' . Alternatively, you can use the mlflow.search_traces() function to get these traces programmatically. traces = mlflow . search_traces ( filter_string = \"tag.session_id = '123456'\" ) ",
        "id": "6adedadd2936a694a184b3fd13e5e67d"
    },
    {
        "text": " Q: How to find a particular span within a trace? When you have a large number of spans in a trace, it can be cumbersome to find a particular span. You can use the Trace.search_spans method to search for spans based on several criteria. import mlflow from mlflow.entities import SpanType @mlflow . trace ( span_type = SpanType . CHAIN ) def run ( x : int ) -> int : x = add_one ( x ) x = add_two ( x ) x = multiply_by_two ( x ) return x @mlflow . trace ( span_type = SpanType . TOOL ) def add_one ( x : int ) -> int : return x + 1 @mlflow . trace ( span_type = SpanType . TOOL ) def add_two ( x : int ) -> int : return x + 2 @mlflow . trace ( span_type = SpanType . TOOL ) def multiply_by_two ( x : int ) -> int : return x * 2 # Run the function and get the trace y = run ( 2 ) trace = mlflow . get_last_active_trace () This will create a Trace object with four spans. run (CHAIN)\n  \u00e2\u0094\u009c\u00e2\u0094\u0080\u00e2\u0094\u0080 add_one (TOOL)\n  \u00e2\u0094\u009c\u00e2\u0094\u0080\u00e2\u0094\u0080 add_two (TOOL)\n  \u00e2\u0094\u0094\u00e2\u0094\u0080\u00e2\u0094\u0080 multiply_by_two (TOOL) Then you can use the Trace.search_spans method to search for a particular spans: # 1. Search by span name (exact match) spans = trace . search_spans ( name = \"add_one\" ) print ( spans ) # Output: [Span(name='add_one', ...)] # Search for a span with the span type \"TOOL\" spans = trace . search_spans ( span_type = SpanType . TOOL ) print ( spans ) # Output: [Span(name='add_one', ...), Span(name='add_two', ...), Span(name='multiply_by_two', ...)] # Search for spans whose name starts with \"add\" spans = trace . search_spans ( name = re . compile ( r \"add.*\" )) print ( spans ) # Output: [Span(name='add_one', ...), Span(name='add_two', ...)] Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "2d61026ffc9cd16ac3fe4158623f1d33"
    },
    {
        "text": "Tracing Concepts 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow Tracing for LLM Observability Tracing Concepts What is tracing? Getting Started with Tracing in MLflow MLflow Tracing Schema Contributing to MLflow Tracing Searching and Retrieving Traces MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs Tracing Concepts  Tracing Concepts In this guide, you can learn about what tracing is as it applies to Generative AI (GenAI) applications and what the main components of tracing are. A good companion to the explanations in this guide is the Tracing Schema guide which will show how MLflow Tracing constructs the\nconcepts discussed here.  What is tracing? Tracing in the context of machine learning (ML) refers to the detailed tracking and recording of the data flow and processing steps during the execution of an ML model.\nIt provides transparency and insights into each stage of the model\u00e2\u0080\u0099s operation, from data input to prediction output. This detailed tracking is crucial for debugging,\noptimizing, and understanding the performance of ML models.  Traditional Machine Learning In traditional ML, the inference process is relatively straightforward. When a request is made, the input data is fed into the model, which processes the data and generates a prediction. The diagram below illustrates the relationship between the input data, the model serving interface, and the model itself. This process is wholly visible, meaning both the input and output are clearly defined and understandable to the end-user. For example, in a spam detection model, the input is an email,\nand the output is a binary label indicating whether the email is spam or not. The entire inference process is transparent, making it easy to determine what data was sent and what prediction was returned,\nrendering full tracing a largely irrelevant process within the context of qualitative model performance. However, tracing might be included as part of a deployment configuration to provide additional insights into the nature of processing the requests made to the server, the latency of the model\u00e2\u0080\u0099s prediction,\nand for logging API access to the system.  For this classical form of trace logging, in which metadata associated with the inference requests from a latency and performance perspective are monitored and logged, these logs\nare not typically used by model developers or data scientists to understand the model\u00e2\u0080\u0099s operation.  Concept of a Span In the context of tracing, a span represents a single operation within the system. It captures metadata such as the start time, end time, and other contextual information about the operation. Along with the metadata, the\ninputs that are provided to a unit of work (such as a call to a GenAI model, a retrieval query from a vector store, or a function call), as well as the output from the operation, are recorded. The diagram below illustrates a call to a GenAI model and the collection of relevant information within a span. The span includes metadata such as the start time, end time, and the request arguments, as well as the input and output of the invocation call. ",
        "id": "65c1ba4c9f20cd955a91e95817f4b28f"
    },
    {
        "text": " Concept of a Trace A trace in the context of GenAI tracing is a collection of Directed Acyclic Graph (DAG)-like Span events that are asynchronously called and recorded in a processor. Each span represents a single operation within\nthe system and includes metadata such as start time, end time, and other contextual information. These spans are linked together to form a trace, which provides a comprehensive view of the end-to-end process. DAG-like Structure : The DAG structure ensures that there are no cycles in the sequence of operations, making it easier to understand the flow of execution. Span Information : Each span captures a discrete unit of work, such as a function call, a database query, or an API request. Spans include metadata that provides context about the operation. Hierarchical Association : Spans mirror the structure of your applications, allowing you to see how different components interact and depend on each other. By collecting and analyzing these spans, one can trace the execution path, identify bottlenecks, and understand the dependencies and interactions between different components of the system. This level of\nvisibility is crucial for diagnosing issues, optimizing performance, and ensuring the robustness of GenAI applications. To illustrate what an entire trace can capture in a RAG application, see the illustration below. The subsystems that are involved in this application are critical to the quality and relevancy of the system. Having no visibility into the paths that data will follow when interacting with the final stage LLM\ncreates an application whose quality could only be achieved by a high degree of monotonouos, tedious, and expensive manual validation of each piece in isolation.  GenAI ChatCompletions Use Case In Generative AI (GenAI) applications, such as chat completions, tracing becomes far more important for the developers of models and GenAI-powered applications. These use cases involve generating human-like text\nbased on input prompts. While not nearly as complex as GenAI applications that involve agents or informational retrieval to augment a GenAI model, a chat interface can benefit from tracing. Enabling tracing on per-interaction interfaces\nwith a GenAI model via a chat session allows for evaluating the entire contextual history, prompt, input, and configuration parameters along with the output, enacpasulating the full context of the request payload that has been\nsubmitted to the GenAI model. As an example, the illustration below shows the nature of a ChatCompletions interface used for connecting a model, hosted in a deployment server, to an external GenAI service. Additional metadata surrounding the inference process is useful for various reasons, including billing, performance evaluation, relevance, evaluation of hallucinations, and general debugging. Key metadata includes: Token Counts : The number of tokens processed, which affects billing. Model Name : The specific model used for inference. Provider Type : The service or platform providing the model. Query Parameters : Settings such as temperature and top-k that influence the generation process. Query Input : The request input (user question). Query Response : The system-generated response to the input query, utilizing the query parameters to adjust generation. This metadata helps in understanding how different settings affect the quality and performance of the generated responses, aiding in fine-tuning and optimization. ",
        "id": "0d4f81cba3c7a424038571a2fa30b676"
    },
    {
        "text": " Advanced Retrieval-Augmented Generation (RAG) Applications In more complex applications like Retrieval-Augmented Generation (RAG), tracing is essential for effective debugging and optimization. RAG involves multiple stages, including document retrieval and interaction with GenAI models.\nWhen only the input and output are visible, it becomes challenging to identify the source of issues or opportunities for improvement. For example, if a GenAI system generates an unsatisfactory response, the problem might lie in: Vector Store Optimization : The efficiency and accuracy of the document retrieval process. Embedding Model : The quality of the model used to encode and search for relevant documents. Reference Material : The content and quality of the documents being queried. Tracing allows each step within the RAG pipeline to be investigated and adjudicated for quality. By providing visibility into every stage, tracing helps pinpoint where adjustments are needed, whether in the\nretrieval process, the embedding model, or the content of the reference material. For example, the diagram below illustrates the complex interactions that form a simple RAG application, wherein the GenAI model is called repeatedly with additional retrieved data that guides the final output generation response. Without tracing enabled on such a complex system, it is challenging to identify the root cause of issues or bottlenecks. The following steps would effectively be a \u00e2\u0080\u009cblack box\u00e2\u0080\u009d: Embedding of the input query The return of the encoded query vector The vector search input The retrieved document chunks from the Vector Database The final input to the GenAI model Diagnosing correctness issues with responses in such a system without these 5 critical steps having instrumentation configured to capture the inputs, outputs, and metadata associated with each request\ncreates a challenging scenario to debug, improve, or refine such an application. When considering performance tuning for responsiveness or cost, not having the visibility into latencies for each of these\nsteps presents an entirely different challenge that would require the configuration and manual instrumentation of each of these services.  Getting Started with Tracing in MLflow To learn how to utilize tracing in MLflow, see the MLflow Tracing Guide . Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "c3d28f443c8d721ebdbeafb768cdfc6d"
    },
    {
        "text": "MLflow Tracing Schema 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow Tracing for LLM Observability Tracing Concepts MLflow Tracing Schema Structure of Traces Trace Trace Info Trace Data Span Schema Schema for specific span types Contributing to MLflow Tracing Searching and Retrieving Traces MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow Tracing Schema  MLflow Tracing Schema This document provides a detailed view of the schema for traces and its ingredients. MLflow traces are compatible to OpenTelemetry specs , but we also define a few additional layers of structure upon the OpenTelemetry Spans to provide additional metadata about the trace.  Structure of Traces TL;DR : Trace = TraceInfo + TraceData where TraceData = List[Span]  Trace  Trace Info  Trace Data  Span  Trace Structure A Trace in MLflow consists of two components: Trace Info and Trace Data . The metadata that aids in explaining the origination\n           of the trace, the status of the trace, and the information about the total execution time is stored within the Trace Info. The Trace\n           Data is comprised entirely of the instrumented Span objects that make up the core of the trace.  Trace Info Structure The Trace Info within MLflow's tracing feature aims to provide a lightweight snapshot of critical data about the overall trace.\n        This includes the logistical information about the trace, such as the experiment_id, providing the storage location for the trace,\n        as well as trace-level data such as start time and total execution time. The Trace Info also includes tags and status information for\n        the trace as a whole.  Trace Data Structure The Trace Data within MLflow's tracing feature provides the core of the trace information. Within this object is a list of Span objects that represent the individual steps of the trace.\n        These spans are associated with one another in a hierarchical relationship, providing a clear order-of-operations linkage of what\n        happened within your application during the trace.  Span Structure The Span object within MLflow's tracing feature provides detailed information about the individual steps of the trace. It complies to the OpenTelemetry Span spec .\n        Each Span object contains information about the step being instrumented, including the span_id, name, start_time, parent_id, status,\n        inputs, outputs, attributes, and events.  Trace A trace is a root object composed of two components: TraceInfo TraceData Tip Check the API documentation for helper methods on these dataclass objects for more information on how to convert or extract data from them. ",
        "id": "8a6200d4805890ad9e17f513e45f82b2"
    },
    {
        "text": " Trace Info Trace Info is a dataclass object that contains metadata about the trace. This metadata includes information about the trace\u00e2\u0080\u0099s origin, status, and\nvarious other data that aids in retrieving and filtering traces when used with mlflow.client.MlflowClient.search_traces() and for\nnavigation of traces within the MLflow UI. To learn more about how TraceInfo metadata is used for searching, you can see examples here . The data that is contained in the TraceInfo object is used to populate the trace view page within the MLflow tracking UI, as shown below. The primary components of MLflow TraceInfo objects are listed below. Property Description Note request_id A unique identifier for the trace. The identifier is used within MLflow and integrated system to resolve the event being captured and to provide associations for external systems to map the logged trace to the originating caller. This value is generated by the tracing backend and is immutable. Within the tracing client APIs, you will need to deliberately pass this value to the span creation API to ensure that a given span is associated with a trace. experiment_id The ID of the experiment in which the trace was logged. All logged traces are associated with the current active experiment when the trace is generated (during invocation of an instrumented object). This value is immutable and is set by the tracing backend. It is a system-controlled value that is very useful when using the Search Traces API. timestamp_ms The time that marks the moment when the root span of the trace was created. This is a Unix timestamp in milliseconds. The time reflected in this property is the time at with the trace was created, not the time at which a request to your application was made. As such, it does not factor into account the time it took to process the request to the environment in which your application is being served, which may introduce additional latency to the total round trip time, depending on network configurations. execution_time_ms The time that marks the moment when the call to end the trace is made. This is a Unix timestamp in milliseconds. This time does not include the networking time associated with sending the response from the environment that generates the trace to the environment that is consuming the application\u00e2\u0080\u0099s invocation result. status An enumerated value that denotes the status of the trace. TraceStatus values are one of: OK - The trace and the instrumented call were successful. ERROR - An error occurred while an application was being instrumented. The error can be seen within the span data for the trace. IN_PROGRESS - The trace has started and is currently running. Temporary state that will update while spans are being logged to a trace. TRACE_STATUS_UNSPECIFIED - internal default state that should not be seen in logged traces. request_metadata The request metadata are additional key-value pairs of information that are associated with the Trace, set and modified by the tracing backend. These are not open for addition or modification by the user, but can provide additional context about the trace, such as an MLflow run_id that is associated with the trace. tags User-defined key-value pairs that can be applied to a trace for applying additional context, aid in search functionality , or to provide additional information during the creation or after the successful logging of a trace. These tags are fully mutable and can be changed at any time, even long after a trace has been logged to an experiment. ",
        "id": "a652d7ba845e9bfad6531b6a601773f8"
    },
    {
        "text": " Trace Data The MLflow TraceData object is a dataclass object that holds the core of the trace data. This object contains\nthe following elements: Property Description Note request The request property is the input data for the entire trace. The input str is a JSON-serialized string that contains the input data for the trace, typically the end-user request that was submitted as a call to the application. Due to the varied structures of inputs that could go to a given application that is being instrumented by MLflow Tracing, all inputs are JSON serialized for compatibility\u00e2\u0080\u0099s sake. This allows for the input data to be stored in a consistent format, regardless of the input data\u00e2\u0080\u0099s structure. spans This property is a list of Span objects that represent the individual steps of the trace. For further information on the structure of Span objects, see the section below. response The response property is the final output data that will be returned to the caller of the invocation of the application. Similar to the request property, this value is a JSON-serialized string to maximize compatibility of disparate formats. ",
        "id": "a6ac250db7a0d58dfafe3c1a64a35997"
    },
    {
        "text": " Span Schema Spans are the core of the trace data. They record key, critical data about each of the steps within your genai application. When you view your traces within the MLflow UI, you\u00e2\u0080\u0099re looking at a collection of spans, as shown below. The sections below provide a detailed view of the structure of a span. Property Description Note inputs The inputs are stored as JSON-serialized strings, representing the input data that is passed into the particular stage (step) of your application. Due to the wide variety of input data that can be passed between specific stages of a GenAI application, this data may be extremely large (such as when using the output of a vector store retrieval step). Reviewing the Inputs, along with the Outputs, of individual stages can dramatically increase the ability to diagnose and debug issues that exist with responses coming from your application. outputs The outputs are stored as JSON-serialized strings, representing the output data that is passed out of the particular stage (step) of your application. Just as with the Inputs, the Outputs can be quite large, depending on the complexity of the data that is being passed between stages. attributes Attributes are metadata that are associated with a given step within your application. These attributes are key-value pairs that can be used to provide insight into behavioral modifications for function and method calls, giving insight into how modification of them can affect the performance of your application. Common examples of attributes that could be associated with a given span include: model temperature document_count These attributes provide additional context and insight into the results that are present in the outputs property for the span. events Events are a system-level property that is optionally applied to a span only if there was an issue during the execution of the span. These events contain information about exceptions that were thrown in the instrumented call, as well as the stack trace. This data is structured within a SpanEvent object, containing the properties: name timestamp attributes The attributes property contains the stack trace of the exception that was thrown during the execution of the span if such an error occurred during execution. parent_id The parent_id property is an identifier that establishes the hierarchical association of a given span with its parent span. This is used to establish an event chain for the spans, helping to determine which step followed another step in the execution of the application. A span must have a parent_id set. span_id The span_id is a unique identifier that is generated for each span within a trace. This identifier is used to disambiguate spans from one another and allow for proper associat",
        "id": "4f0018cd474784fcc273084cc0fdd85d"
    },
    {
        "text": "helping to determine which step followed another step in the execution of the application. A span must have a parent_id set. span_id The span_id is a unique identifier that is generated for each span within a trace. This identifier is used to disambiguate spans from one another and allow for proper association of the span within the sequential execution of other spans within a trace. A span_id is set when a span is created and is immutable. request_id The request_id property is a unique identifier that is generated for each trace and is propogated to each span that is a member of that trace. The request_id is a system-generated propoerty and is immutable. name The name of the trace is either user-defined (optionally when using the fluent and client APIs) or is automatically generated through CallBack integrations or when omitting the name argument when calling the fluent or client APIs. If the name is not overridden, the name will be generated based on the name of the function or method that is being instrumented. It is recommended to provide a name for your span that is unique and relevant to the functionality that is being executed when using manual instumentation via the client or fluent APIs. Generic names for spans or confusing names can make it difficult to diagnose issues when reviewing traces. status The status of a span is reflected in a value from the enumeration object SpanStatusCode . The span status object contains an optional description if the status_code is reflecting an error that occured. The values that the status may have are: OK - The span and the underlying instrumented call were successful. UNSET - The status of the span hasn\u00e2\u0080\u0099t been set yet (this is the default value and should not be seen in logged trace events). ERROR - An issue happened within the call being instrumented. The description property will contain additional information about the error that occurred. Evaluating the status of spans can greatly reduce the amount of time and effort required to diagnose issues with your applications. start_time_ns The unix timestamp (in nanoseconds) when the span was started. The precision of this property is higher than that of the trace start time, allowing for more granular analysis of the execution time of very short-lived spans. end_time_ns The unix timestamp (in nanoseconds) when the span was ended. This precision is higher than the trace timestamps, similar to the start_time_ns timestamp above. ",
        "id": "9aa85138f6f8a3dd3a335d5d5d4773fe"
    },
    {
        "text": " Schema for specific span types MLflow has a set of 10 predefined types of spans (see mlflow.entities.SpanType ), and\ncertain span types have properties that are required in order to enable additional functionality\nwithin the UI and downstream tasks such as evaluation.  Retriever Spans The RETRIEVER span type is used for operations involving retrieving data from a data store (for example, querying\ndocuments from a vector store). The RETRIEVER span type has the following schema: Property Description Note Input There are no restrictions on the span inputs Output The output must be of type List[ mlflow.entities.Document ] , or a dict matching the structure of the dataclass*.\nThe dataclass contains the following properties: id ( Optional[str] ) - An optional unique identifier for the document. page_content ( str ) - The text content of the document. metadata ( Optional[Dict[str,any]] ) - The metadata associated with the document. There are two important metadata keys that are reserved for the MLflow UI and evaluation metrics: \"doc_uri\" (str) : The URI for the document. This is used for rendering a link in the UI. \"chunk_id\" (str) : If your document is broken up into chunks in your data store, this key can be used to\nidentify the chunk that the document is a part of. This is used by some evaluation metrics. This output structure is guaranteed to be provided if the traces are generated via MLflow autologging for the LangChain and LlamaIndex flavors.\nBy conforming to this specification, RETRIEVER spans will be rendered in a more user-friendly manner in the MLflow UI, and downstream tasks\nsuch as evaluation will function as expected. Attributes There are no restrictions on the span attributes * For example, both [Document(page_content=\"Hello world\", metadata={\"doc_uri\": \"https://example.com\"})] and [{\"page_content\": \"Hello world\", \"metadata\": {\"doc_uri\": \"https://example.com\"}}] are valid outputs for a RETRIEVER span. ",
        "id": "719652e68f102bbb0654bcd3a1ac76ae"
    },
    {
        "text": " Chat Completion Spans Spans of type CHAT_MODEL or LLM are used to represent interactions with a chat completions API\n(for example, OpenAI\u00e2\u0080\u0099s chat completions ,\nor Anthropic\u00e2\u0080\u0099s messages API). As providers can have\ndifferent schemas for their API, there are no restrictions on the format of the span\u00e2\u0080\u0099s inputs and\noutputs. However, it is still important to have a common schema in order to enable certain UI features (e.g. rich\nconversation display), and to make authoring evaluation functions easier. To support this, we specify some\ncustom attributes for standardized chat messages and tool defintions: Attribute Name Description Note mlflow.chat.messages This attribute represents the system/user/assistant messages involved in the\nconversation with the chat model. It enables rich conversation rendering in the UI,\nand will also be used in MLflow evaluation in the future. The type must be List[ ChatMessage ] This attribute can be conveniently set using the mlflow.tracing.set_span_chat_messages() function. This function\nwill throw a validation error if the data does not conform to the spec. mlflow.chat.tools This attribute represents the tools that were available for the chat model to call. In the OpenAI\ncontext, this would be equivalent to the tools param in the Chat Completions API. The type must be List[ ChatTool ] This attribute can be conveniently set using the mlflow.tracing.set_span_chat_tools() function. This function\nwill throw a validation error if the data does not conform to the spec. Please refer to the example below for a quick demonstration of how to use the utility functions described above, as well as\nhow to retrieve them using the span.get_attribute() function: import mlflow from mlflow.entities.span import SpanType from mlflow.tracing.constant import SpanAttributeKey from mlflow.tracing import set_span_chat_messages , set_span_chat_tools # example messages and tools messages = [ { \"role\" : \"system\" , \"content\" : \"please use the provided tool to answer the user's questions\" , }, { \"role\" : \"user\" , \"content\" : \"what is 1 + 1?\" }, ] tools = [ { \"type\" : \"function\" , \"function\" : { \"name\" : \"add\" , \"description\" : \"Add two numbers\" , \"parameters\" : { \"type\" : \"object\" , \"properties\" : { \"a\" : { \"type\" : \"number\" }, \"b\" : { \"type\" : \"number\" }, }, \"required\" : [ \"a\" , \"b\" ], }, }, } ] @mlflow . trace ( span_type = SpanType . CHAT_MODEL ) def call_chat_model ( messages , tools ): # mocking a response response = { \"role\" : \"assistant\" , \"tool_calls\" : [ { \"id\" : \"123\" , \"function\" : { \"arguments\" : '{\"a\": 1,\"b\": 2}' , \"name\" : \"add\" }, \"type\" : \"function\" , } ], } combined_messages = messages + [ response ] span = mlflow . get_current_active_span () set_span_chat_messages ( span , combined_messages ) set_span_chat_tools ( span , tools ) return response call_chat_model ( messages , tools ) trace = mlflow . get_last_active_trace () span = trace . data . spans [ 0 ] print ( \"Messages: \" , span . get_attribute ( SpanAttributeKey . CHAT_MESSAGES )) print ( \"Tools: \" , span . get_attribute ( SpanAttributeKey . CHAT_TOOLS )) Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "3159d56e2499e7adc47ec0e4b6306805"
    },
    {
        "text": "Contributing to MLflow Tracing 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow Tracing for LLM Observability Tracing Concepts MLflow Tracing Schema Contributing to MLflow Tracing Step 1. Set up Your Environment Step 2. Familiarize Yourself with MLflow Tracing Step 3. Understand the Integration Library Step 4. Write a Design Document Step 5. Begin Implementation Step 6. Test the Integration Step 7. Document the Integration Step 8. Release\u00f0\u009f\u009a\u0080 Contact Searching and Retrieving Traces MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs Contributing to MLflow Tracing   Contributing to MLflow Tracing Welcome to the MLflow Tracing contribution guide! This step-by-step resource will assist you in implementing additional GenAI library integrations for tracing into MLflow. Tip If you have any questions during the process, try the \u00e2\u0080\u009cAsk AI\u00e2\u0080\u009d feature in the bottom-right corner. It can provide both reference documentation and quick answers to common questions about MLflow.  Step 1. Set up Your Environment Set up a dev environment following the CONTRIBUTING.md . After setup, verify the environment is ready for tracing development by running the unit tests with the pytest tests/tracing command and ensure that all tests pass.  Step 2. Familiarize Yourself with MLflow Tracing First, get a solid understanding of what MLflow Tracing does and how it works. Check out these docs to get up to speed: Tracing Concepts - Understand what tracing is and the specific benefits for MLflow users. Tracing Schema Guide - Details on trace data structure and the information stored. MLflow Tracing API Guide - Practical guide to auto-instrumentation and APIs for manually creating traces. \u00f0\u009f\u0093\u009d Quick Quiz : Before moving on to the next step, let\u00e2\u0080\u0099s challenge your understanding with a few questions. If you are not sure about the answers, revisit the docs for a quick refresh. Q. What is the difference between a Trace and a Span? A. Trace is the main object holding multiple Spans, with each Span capturing different parts of an operation. A Trace has metadata (TraceInfo) and a list of Spans (TraceData). Reference: Tracing Schema Guide Q. What is the easiest way to create a span for a function call? A. Use the @mlflow.trace decorator to capture inputs, outputs, and execution duration automatically. Reference: MLflow Tracing API Guide Q. When would you use the MLflow Client for creating traces? A. The Client API is useful when you want fine-grained control over how you start and end a trace. For example, you can specify a parent span ID when starting a span. Reference: MLflow Tracing API Guide Q. How do you log input data to a span? A. You can log input data with the span.set_inputs() method for a span object returned by the ``mlflow.start_span`` context manager or Client APIs. Reference: Tracing Schema Guide Q. Where is exception information stored in a Trace? A. Exceptions are recorded in the events attribute of the span, including details such as exception type, message, and stack trace. References: MLflow Tracing API Guide ",
        "id": "78c483d5fb2d8dd9c2c9b849e3168a7d"
    },
    {
        "text": " Step 3. Understand the Integration Library From a tracing perspective, GenAI libraries can be categorized into two types: \u00f0\u009f\u00a7\u00a0 LLM Providers or Wrappers Libraries like OpenAI , Anthropic , Ollama , and LiteLLM focus on providing access to LLMs. These libraries often have simple client SDKs, therefore, we often simply use ad-hoc patching to trace those APIs. For this type of library, start with listing up the core APIs to instrument. For example, in Anthropic auto-tracing, we patch the create() method of the Messages class. If the library has multiple APIs (e.g., embeddings, transcription) and you\u00e2\u0080\u0099re not sure which ones to support, consult with the maintainers. Refer to the Anthropic auto-tracing implementation as an example. \u00e2\u009a\u0099\u00ef\u00b8\u008f Orchestration Frameworks Libraries such as LangChain , LlamaIndex , and DSPy offer higher-level workflows, integrating LLMs, embeddings, retrievers, and tools into complex applications. Since these libraries require trace data from multiple components, we do not want to rely on ad-hoc patching. Therefore, auto-tracing for these libraries often leverage available callbacks (e.g., LangChain Callbacks ) for more reliable integration. For this type of library, first check if the library provides any callback mechanism you can make use of. If there isn\u00e2\u0080\u0099t, consider filing a feature request to the library to have this functionality added by the project maintainers, providing comprehensive justification for the request. Having a callback mechanism also benefits the other users of the library, by providing flexibility and allowing integration with many other tools. If there is a certain reason the library cannot provide callbacks, consult with the  MLflow maintainers. We will not likely proceed with a design that relies on ad-hoc patching, but we can discuss alternative approaches if there are any to be had.  Step 4. Write a Design Document Draft a design document for your integration plan, using the design template .  Here are some important considerations: Integration Method : Describe whether you\u00e2\u0080\u0099ll use callbacks, API hooks, or patching. If there are multiple methods, list them as options and explain your choice. Maintainability : LLM frameworks evolve quickly, so avoid relying on internal methods as much as possible. Prefer public APIs such as callbacks. Standardization : Ensure consistency with other MLflow integrations for usability and downstream tasks. For example, retrieval spans should follow the Retriever Schema for UI compatibility. Include a brief overview of the library\u00e2\u0080\u0099s core functionality and use cases to provide context for reviewers. Once the draft is ready, share your design with MLflow maintainers, and if time allows, create a proof of concept to highlight potential challenges early. ",
        "id": "efb90f1932bd7b59ba375e00b6d9b945"
    },
    {
        "text": " Step 5. Begin Implementation With the design approved, start implementation: Create a New Module : If the library isn\u00e2\u0080\u0099t already integrated with MLflow, create a new directory under mlflow/ (e.g., mlflow/llama_index ). Add an __init__.py file to initialize the module. Develop the Tracing Hook : Implement your chosen method (patch, callback, or decorator) for tracing. If you go with patching approach, use the safe_patch function to ensure stable patching (see example ). Define `mlflow.xxx.autolog() function` : This function will be the main entry point for the integration, which enables tracing when called (e.g., mlflow.llama_index.autolog() ). Write Tests : Cover edge cases like asynchronous calls, custom data types, and streaming outputs if the library supports them. Attention There are a few gotchas to watch out for when integrating with MLflow Tracing: Error Handling : Ensure exceptions are captured and logged to spans with type, message, and stack trace. Streaming Outputs : For streaming (iterators), hook into the iterator to assemble and log the full output to the span. Directly logging the iterator object is not only unhelpful but also cause unexpected behavior e.g. exhaust the iterator during serialization. Serialization : MLflow serializes traces to JSON via the custom TraceJsonEncoder implementation, which supports common objects and Pydantic models. If your library uses custom objects, consider extending the serializer, as unsupported types are stringified and may lose useful detail. Timestamp Handling : When using timestamps provided by the library, validate the unit and timezone. MLflow requires timestamps in nanoseconds since the UNIX epoch ; incorrect timestamps will disrupt span duration.  Step 6. Test the Integration Once implementation is complete, run end-to-end tests in a notebook to verify functionality. Ensure: \u00e2\u0097\u00bb\u00ef\u00b8\u008e Traces appear correctly in the MLflow Experiment. \u00e2\u0097\u00bb\u00ef\u00b8\u008e Traces are properly rendered in the MLflow UI. \u00e2\u0097\u00bb\u00ef\u00b8\u008e Errors from MLflow trace creation should not interrupt the original execution of the library. \u00e2\u0097\u00bb\u00ef\u00b8\u008e Edge cases such as asynchronous and streaming calls function as expected. In addition to the local test, there are a few Databricks services that are integrated with MLflow Tracing. Consult with an MLflow maintainer for guidance on how to test those integrations. When you are confident that the implementation works correctly, open a PR with the test result pasted in the PR description.  Step 7. Document the Integration Documentation is a prerequisite for release. Follow these steps to complete the documentation: Add the integrated library icon and example in the main Tracing documentation . If the library is already present in an existing MLflow model flavor, add a Tracing section in the flavor documentation ( example page ). Add a notebook tutorial to demonstrate the integration ( example notebook ) Documentation sources are located in the docs/ folder. Refer to Writing Docs for more details on how to build and preview the documentation.  Step 8. Release\u00f0\u009f\u009a\u0080 Congratulations! Now you\u00e2\u0080\u0099ve completed the journey of adding a new tracing integration to MLflow. The release notes will feature your name, and we will write an SNS or/and a blog post to highlight your contribution. Thank you so much for helping improve MLflow Tracing, and we look forward to working with you again!\u00f0\u009f\u0098\u008a  Contact If you have any questions or need help, feel free to reach out to the maintainers (POC: @B-Step62, @BenWilson2) for further guidance. Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "572c03863b4ea404da270c0284d5153b"
    },
    {
        "text": "Searching and Retrieving Traces 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow Tracing for LLM Observability Tracing Concepts MLflow Tracing Schema Contributing to MLflow Tracing Searching and Retrieving Traces Basic Usage of Search Traces Search Traces with filter_string Order Traces Extract Specific Fields MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs Searching and Retrieving Traces  Searching and Retrieving Traces This page describes various ways to search and retrieve traces in MLflow. MLflow provides two methods for this purpose: MlflowClient.search_traces() and mlflow.search_traces() . MlflowClient.search_traces() : This method allows you to filter traces using experiment IDs,\nfilter strings, and other parameters. mlflow.search_traces() : A higher-level fluent API that returns a pandas DataFrame, with each row representing\na trace. It supports the same filtering capabilities as MlflowClient.search_traces and additionally allows you to specify\nfields to extract from traces. See Extract Specific Fields for details. The pandas Dataframe returned by the mlflow.search_traces() API consists of the following columns by default: request_id: A primary identifier of a trace trace: A trace object. timestamp_ms: The start time of the trace in milliseconds. status: The status of the trace. execution_time_ms: The duration of the trace in milliseconds. request: The input to the traced logic. response: The output of the traced logic. request_metadata: Key-value pairs associated with the trace. spans: Spans in the trace. tags: Tags associated with the trace. ",
        "id": "a787bc505a95b51fcc6391a7f189941c"
    },
    {
        "text": " Basic Usage of Search Traces First, create several traces using the following code: import time import mlflow from mlflow.entities import SpanType # Define methods to be traced @mlflow . trace ( span_type = SpanType . TOOL , attributes = { \"time\" : \"morning\" }) def morning_greeting ( name : str ): time . sleep ( 1 ) mlflow . update_current_trace ( tags = { \"person\" : name }) return f \"Good morning { name } .\" @mlflow . trace ( span_type = SpanType . TOOL , attributes = { \"time\" : \"evening\" }) def evening_greeting ( name : str ): time . sleep ( 1 ) mlflow . update_current_trace ( tags = { \"person\" : name }) return f \"Good evening { name } .\" @mlflow . trace ( span_type = SpanType . TOOL ) def goodbye (): raise Exception ( \"Cannot say goodbye\" ) # Execute the methods within different experiments morning_experiment = mlflow . set_experiment ( \"Morning Experiment\" ) morning_greeting ( \"Tom\" ) # Get the timestamp in milliseconds morning_time = int ( time . time () * 1000 ) evening_experiment = mlflow . set_experiment ( \"Evening Experiment\" ) experiment_ids = [ morning_experiment . experiment_id , evening_experiment . experiment_id ] evening_greeting ( \"Mary\" ) goodbye () The code above creates the following traces: Experiment Name Tags.person Status Morning Experiment morning_greeting Tom OK Evening Experiment evening_greeting Mary OK Evening Experiment goodbye N/A ERROR Then, you can search traces by experiment_ids using either mlflow.search_traces() or MlflowClient.search_traces() . Note The experiment_ids parameter is required for MlflowClient.search_traces() ,\nwhile it is optional for mlflow.search_traces() and it defaults to the currently active experiment. from mlflow import MlflowClient client = MlflowClient () client . search_traces ( experiment_ids = [ morning_experiment . experiment_id ]) # [Trace #1] mlflow . search_traces ( experiment_ids = [ morning_experiment . experiment_id ]) #     request_id     status          ...    response # 0   [trace #1 ID]  TraceStatus.OK  ...    Good morning Tom.  Search Traces with filter_string The filter_string argument provides a flexible way to query traces using a SQL-like Domain-Specific Language (DSL) .\nThe syntax supports searching traces with various metadata and allows for combining multiple conditions.  Filter Traces by Name Search for traces by the attributes.name keyword: client . search_traces ( experiment_ids = experiment_ids , filter_string = \"attributes.name = 'morning_greeting'\" , ) # [Trace #1]  Filter Traces by Timestamp Search traces created after a specific timestamp: client . search_traces ( experiment_ids = experiment_ids , filter_string = f \"attributes.timestamp > { morning_time } \" , ) # [Trace #2, Trace #3]  Filter Traces by Tags Filter traces by specific tag values using tag.[tag name] : client . search_traces ( experiment_ids = experiment_ids , filter_string = \"tag.person = 'Tom'\" , ) # [Trace #1]  Filter Traces by Status Search for traces by their status: client . search_traces ( experiment_ids = experiment_ids , filter_string = \"attributes.status = 'OK'\" , ) # [Trace #1, Trace #2]  Combine Multiple Conditions The filter_string DSL allows you to combine multiple filters together by using AND . client . search_traces ( experiment_ids = experiment_ids , filter_string = f \"attributes.status = 'OK' AND attributes.timestamp > { morning_time } \" , ) # [Trace #2]  Order Traces The order_by argument allows you to sort traces based on one or more fields. Each order_by clause follows\nthe format [attribute name] [ASC or DESC] . client . search_traces ( experiment_ids = experiment_ids , order_by = [ \"timestamp DESC\" ], ) # [Trace #3, Trace #2, Trace #1] ",
        "id": "a81ddeee9c39415a960b126671f7584e"
    },
    {
        "text": "  Extract Specific Fields In addition to the search functionalities mentioned above, the fluent API mlflow.search_traces() enables you\nto extract specific fields from traces using the format \"span_name.[inputs|outputs]\" or \"span_name.[inputs|outputs].field_name\" . This feature is useful for generating evaluation datasets or analyzing\nmodel performance. Refer to MLFlow LLM Evaluation for more details. traces = mlflow . search_traces ( extract_fields = [ \"morning_greeting.inputs\" , \"morning_greeting.outputs\" ], experiment_ids = [ morning_experiment . experiment_id ], ) print ( traces ) The output Pandas DataFrame contains the additional columns for the extracted span fields: request_id                              ...     morning_greeting.inputs        morning_greeting.outputs\n0   053adf2f5f5e4ad68d432e06e254c8a4        ...     {'name': 'Tom'}                'Good morning Tom.' Lastly, you can convert the pandas DataFrame to the MLflow LLM evaluation dataset format and evaluate your language model. eval_data = traces . rename ( columns = { \"morning_greeting.inputs\" : \"inputs\" , \"morning_greeting.outputs\" : \"ground_truth\" , } ) results = mlflow . evaluate ( model , eval_data , targets = \"ground_truth\" , model_type = \"question-answering\" , ) Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "5ebc641b93069c823ba4ab574e49bb57"
    },
    {
        "text": "MLflow AI Gateway (Experimental) 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs MLflow AI Gateway (Experimental) Getting Started with MLflow Deployments for LLMs Tutorials and Guides Quickstart Concepts Configuring the gateway server Querying the AI Gateway server Plugin LLM Provider (Experimental) MLflow AI Gateway API Documentation OpenAI Compatibility Unity Catalog Integration gateway server Security Considerations Benefits of the MLflow AI Gateway Explore the Native Providers of the MLflow AI Gateway LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow AI Gateway (Experimental)   MLflow AI Gateway (Experimental) Warning MLflow AI Gateway does not support Windows. The MLflow AI Gateway is a powerful tool designed to streamline the usage and management of\nvarious large language model (LLM) providers, such as OpenAI and Anthropic, within an organization.\nIt offers a high-level interface that simplifies the interaction with these services by providing\na unified endpoint to handle specific LLM related requests. A major advantage of using the MLflow AI Gateway is its centralized management of API keys.\nBy storing these keys in one secure location, organizations can significantly enhance their\nsecurity posture by minimizing the exposure of sensitive API keys throughout the system. It also\nhelps to prevent exposing these keys within code or requiring end-users to manage keys safely. The gateway server is designed to be flexible and adaptable, capable of easily defining and managing endpoints by updating the\nconfiguration file. This enables the easy incorporation\nof new LLM providers or provider LLM types into the system without necessitating changes to\napplications that interface with the gateway server. This level of adaptability makes the MLflow AI Gateway\nService an invaluable tool in environments that require agility and quick response to changes. This simplification and centralization of language model interactions, coupled with the added\nlayer of security for API key management, make the MLflow AI Gateway an ideal choice for\norganizations that use LLMs on a regular basis.  Tutorials and Guides If you\u00e2\u0080\u0099re interested in diving right in to a step by step guide that will get you up and running with the MLflow AI Gateway\nas fast as possible, the guides below will be your best first stop. View the gateway server Getting Started Guide   Quickstart The following guide will assist you in getting up and running, using a 3-endpoint configuration to\nOpenAI services for chat, completions, and embeddings.  Step 1: Install the MLflow AI Gateway First, you need to install the MLflow AI Gateway on your machine. You can do this using pip from PyPI or from the MLflow repository.  Installing from PyPI pip install 'mlflow[genai]'  Step 2: Set the OpenAI API Key(s) for each provider The gateway server needs to communicate with the OpenAI API. To do this, it requires an API key.\nYou can create an API key from the OpenAI dashboard. For this example, we\u00e2\u0080\u0099re only connecting with OpenAI. If there are additional providers within the\nconfiguration, these keys will need to be set as well. Once you have the key, you can set it as an environment variable in your terminal: export OPENAI_API_KEY = your_api_key_here This sets a temporary session-based environment variable. For production use cases, it is advisable\nto store this key in the .bashrc or .zshrc files so that the key doesn\u00e2\u0080\u0099t have to be re-entered upon\nsystem restart. ",
        "id": "74b94fd54ad549508bb8ffffcb2ca1eb"
    },
    {
        "text": " Step 3: Create a gateway server Configuration File Next, you need to create a gateway server configuration file. This is a YAML file where you specify the\nendpoints that the MLflow AI Gateway should expose. Let\u00e2\u0080\u0099s create a file with three endpoints using OpenAI as a provider: completions, chat, and embeddings. For details about the configuration file\u00e2\u0080\u0099s parameters (including parameters for other providers besides OpenAI), see the AI Gateway server Configuration Details section below. endpoints : - name : completions endpoint_type : llm/v1/completions model : provider : openai name : gpt-4o-mini config : openai_api_key : $OPENAI_API_KEY limit : renewal_period : minute calls : 10 - name : chat endpoint_type : llm/v1/chat model : provider : openai name : gpt-4o-mini config : openai_api_key : $OPENAI_API_KEY - name : embeddings endpoint_type : llm/v1/embeddings model : provider : openai name : text-embedding-ada-002 config : openai_api_key : $OPENAI_API_KEY Save this file to a location on the system that is going to be running the MLflow AI Gateway.  Step 4: Start the gateway server You\u00e2\u0080\u0099re now ready to start the gateway server! Use the MLflow AI Gateway start-server command and specify the path to your configuration file: mlflow gateway start --config-path config.yaml --port { port } --host { host } --workers { worker count } The configuration file can also be set using the MLFLOW_DEPLOYMENTS_CONFIG environment variable: export MLFLOW_DEPLOYMENTS_CONFIG = /path/to/config.yaml If you do not specify the host, a localhost address will be used. If you do not specify the port, port 5000 will be used. The worker count for gunicorn defaults to 2 workers.  Step 5: Access the Interactive API Documentation The MLflow AI Gateway provides an interactive API documentation endpoint that you can use to explore\nand test the exposed endpoints. Navigate to http://{host}:{port}/ (or http://{host}:{port}/docs ) in your browser to access it. The docs endpoint allow for direct interaction with the endpoints and permits submitting actual requests to the\nprovider services by click on the \u00e2\u0080\u009ctry it now\u00e2\u0080\u009d option within the endpoint definition entry.  Step 6: Send Requests Using the Client API See the Client API section for further information.  Step 7: Send Requests to Endpoints via REST API You can now send requests to the exposed endpoints.\nSee the REST examples for guidance on request formatting.  Step 8: Compare Provider Models Here\u00e2\u0080\u0099s an example of adding a new model from a provider to determine which model instance is better for a given use case. Firstly, update the MLflow AI Gateway config YAML file with the additional endpoint definition to test: endpoints : - name : completions endpoint_type : llm/v1/completions model : provider : openai name : gpt-4o-mini config : openai_api_key : $OPENAI_API_KEY - name : completions-gpt4 endpoint_type : llm/v1/completions model : provider : openai name : gpt-4 config : openai_api_key : $OPENAI_API_KEY This updated configuration adds a new completions endpoint completions-gpt4 while still preserving the original completions endpoint that was configured with the gpt-4o-mini model. Once the configuration file is updated, simply save your changes. The gateway server will automatically create the new endpoint with zero downtime. If you no longer need an endpoint, you can delete it from the configuration YAML and save your changes. The gateway server will automatically remove the endpoint.  Step 9: Use gateway server endpoints for model development Now that you have created several gateway server endpoints, you can create MLflow Models that query these\nendpoints to build application-specific logic using techniques like prompt engineering. For more\ninformation, see gateway server and MLflow Models .   Concepts There are several concepts that are referred to within the MLflow AI Gateway APIs, the configuration definitions, examples, and documentation.\nBecoming familiar with these terms will help to simplify both configuring new endpoints and using the MLflow AI Gateway APIs. ",
        "id": "4c1537cf4294593f8498a4b3ec9ad8f9"
    },
    {
        "text": "  Providers The MLflow AI Gateway is designed to support a variety of model providers.\nA provider represents the source of the machine learning models, such as OpenAI, Anthropic, and so on.\nEach provider has its specific characteristics and configurations that are encapsulated within the model part of an endpoint in the MLflow AI Gateway.  Supported Providers The table below presents supported corresponding endpoint type for each LLM provider within the MLflow AI Gateway.\nNote that \u00e2\u009c\u0085 mark does not mean all models from the provider are compatible with the endpoint types. For example, OpenAI provider supports all three endpoint types, but the model gpt-4 is only compatible with the llm/v1/chat endpoint types. Provider llm/v1/completions llm/v1/chat llm/v1/embeddings OpenAI \u00c2\u00a7 \u00e2\u009c \u00e2\u009c \u00e2\u009c Azure OpenAI \u00e2\u009c \u00e2\u009c \u00e2\u009c MosaicML \u00e2\u009c \u00e2\u009c \u00e2\u009c Anthropic \u00e2\u009c \u00e2\u009c \u00e2\u009d\u008c Cohere \u00e2\u009c \u00e2\u009c \u00e2\u009c PaLM \u00e2\u009c \u00e2\u009c \u00e2\u009c MLflow \u00e2\u009c\u0085* \u00e2\u009c\u0085* \u00e2\u009c\u0085** HuggingFace TGI \u00e2\u009d\u008c \u00e2\u009c \u00e2\u009d\u008c AI21 Labs \u00e2\u009c \u00e2\u009d\u008c \u00e2\u009d\u008c Amazon Bedrock \u00e2\u009c \u00e2\u009c \u00e2\u009d\u008c Mistral \u00e2\u009c \u00e2\u009c \u00e2\u009c TogetherAI \u00e2\u009c \u00e2\u009c \u00e2\u009c \u00c2\u00a7 For full compatibility references for OpenAI , see the OpenAI Model Compatibility Matrix . Within each model block in the configuration file, the provider field is used to specify the name\nof the provider for that model. This is a string value that needs to correspond to a provider the MLflow AI Gateway supports. Note * MLflow Model Serving will only work for chat or completions if the output return is in an endpoint-compatible format. The\nresponse must conform to either an output of {\"predictions\": str} or {\"predictions\": {\"candidates\": str}} . Any complex return type from a model that\ndoes not conform to these structures will raise an exception at query time. ** Embeddings support is only available for models whose response signatures conform to the structured format of {\"predictions\": List[float]} or {\"predictions\": List[List[float]]} . Any other return type will raise an exception at query time. FeatureExtractionPipeline in transformers and\nmodels using the sentence_transformers flavor will return the correct data structures for the embeddings endpoint. Here\u00e2\u0080\u0099s an example of a provider configuration within an endpoint: endpoints : - name : chat endpoint_type : llm/v1/chat model : provider : openai name : gpt-4 config : openai_api_key : $OPENAI_API_KEY limit : renewal_period : minute calls : 10 In the above configuration, openai is the provider for the model. As of now, the MLflow AI Gateway supports the following providers: mosaicml : This is used for models offered by MosaicML . openai : This is used for models offered by OpenAI and the Azure integrations for Azure OpenAI and Azure OpenAI with AAD. anthropic : This is used for models offered by Anthropic . cohere : This is used for models offered by Cohere . palm : This is used for models offered by PaLM . huggingface text generation inference : This is used for models deployed using Huggingface Text Generation Inference . ai21labs : This is used for models offered by AI21 Labs . bedrock : This is used for models offered by Amazon Bedrock . mistral : This is used for models offered by Mistral . togetherai : This is used for models offered by TogetherAI . More providers are being added continually. Check the latest version of the MLflow AI Gateway Docs for the\nmost up-to-date list of supported providers. If you would like to use a LLM model that is not offered by the above providers, or if you\nwould like to integrate a private LLM model, you can create a provider plugin to integrate with the MLflow AI Gateway. ",
        "id": "bbedc5a76f17ff77ef184709220a7419"
    },
    {
        "text": "  Endpoints Endpoints are central to how the MLflow AI Gateway functions. Each endpoint acts as a proxy endpoint for the\nuser, forwarding requests to the underlying Models and Providers specified in the configuration file. an endpoint in the MLflow AI Gateway consists of the following fields: name : This is the unique identifier for the endpoint. This will be part of the URL when making API calls via the MLflow AI Gateway. type : The type of the endpoint corresponds to the type of language model interaction you desire. For instance, llm/v1/completions for text completion operations, llm/v1/embeddings for text embeddings, and llm/v1/chat for chat operations. model : Defines the model to which this endpoint will forward requests. The model contains the following details: provider : Specifies the name of the provider for this model. For example, openai for OpenAI\u00e2\u0080\u0099s GPT-4o models. name : The name of the model to use. For example, gpt-4o-mini for OpenAI\u00e2\u0080\u0099s GPT-4o-Mini model. config : Contains any additional configuration details required for the model. This includes specifying the API base URL and the API key. limit : Specify the rate limit setting this endpoint will follow. The limit field contains the following fields: renewal_period : The time unit of the rate limit, one of [second|minute|hour|day|month|year]. calls : The number of calls this endpoint will accept within the specified time unit. Here\u00e2\u0080\u0099s an example of an endpoint configuration: endpoints : - name : completions endpoint_type : llm/v1/chat model : provider : openai name : gpt-4o-mini config : openai_api_key : $OPENAI_API_KEY limit : renewal_period : minute calls : 10 In the example above, a request sent to the completions endpoint would be forwarded to the gpt-4o-mini model provided by openai . The endpoints in the configuration file can be updated at any time, and the MLflow AI Gateway will\nautomatically update its available endpoints without requiring a restart. This feature provides you\nwith the flexibility to add, remove, or modify endpoints as your needs change. It enables \u00e2\u0080\u0098hot-swapping\u00e2\u0080\u0099\nof endpoints, providing a seamless experience for any applications or services that interact with the MLflow AI Gateway. When defining endpoints in the configuration file, ensure that each name is unique to prevent conflicts.\nDuplicate endpoint names will raise an MlflowException . ",
        "id": "d586c587dfef8d55abac7826eb9664aa"
    },
    {
        "text": "  Models The model section within an endpoint specifies which model to use for generating responses.\nThis configuration block needs to contain a name field which is used to specify the exact model instance to be used.\nAdditionally, a provider needs to be specified, one that you have an authenticated access api key for. Different endpoint types are often associated with specific models.\nFor instance, the llm/v1/chat and llm/v1/completions endpoints are generally associated with\nconversational models, while llm/v1/embeddings endpoints would typically be associated with\nembedding or transformer models. The model you choose should be appropriate for the type of endpoint specified. Here\u00e2\u0080\u0099s an example of a model name configuration within an endpoint: endpoints : - name : embeddings endpoint_type : llm/v1/embeddings model : provider : openai name : text-embedding-ada-002 config : openai_api_key : $OPENAI_API_KEY In the above configuration, text-embedding-ada-002 is the model used for the embeddings endpoint. When specifying a model, it is critical that the provider supports the model you are requesting.\nFor instance, openai as a provider supports models like text-embedding-ada-002 , but other providers\nmay not. If the model is not supported by the provider, the MLflow AI Gateway will return an HTTP 4xx error\nwhen trying to route requests to that model. Important Always check the latest documentation of the specified provider to ensure that the model you want\nto use is supported for the type of endpoint you\u00e2\u0080\u0099re configuring. Remember, the model you choose directly affects the results of the responses you\u00e2\u0080\u0099ll get from the\nAPI calls. Therefore, choose a model that fits your use-case requirements. For instance,\nfor generating conversational responses, you would typically choose a chat model.\nConversely, for generating embeddings of text, you would choose an embedding model.   Configuring the gateway server The MLflow AI Gateway relies on a user-provided configuration file, written in YAML,\nthat defines the endpoints and providers available to the server. The configuration file dictates\nhow the gateway server interacts with various language model providers and determines the end-points that\nusers can access. ",
        "id": "02a139c31fed357cc2cbd63ccfbd1b75"
    },
    {
        "text": " AI Gateway server Configuration The configuration file includes a series of sections, each representing a unique endpoint.\nEach endpoint section has a name, a type, and a model specification, which includes the model\nprovider, name, and configuration details. The configuration section typically contains the base\nURL for the API and an environment variable for the API key. Here is an example of a single-endpoint configuration: endpoints : - name : chat endpoint_type : llm/v1/chat model : provider : openai name : gpt-4o-mini config : openai_api_key : $OPENAI_API_KEY limit : renewal_period : minute calls : 10 In this example, we define an endpoint named chat that corresponds to the llm/v1/chat type, which\nwill use the gpt-4o-mini model from OpenAI to return query responses from the OpenAI service, and accept up to 10 requests per minute. The MLflow AI Gateway configuration is very easy to update.\nSimply edit the configuration file and save your changes, and the MLflow AI Gateway will automatically\nupdate the endpoints with zero disruption or down time. This allows you to try out new providers or model types while keeping your applications steady and reliable. In order to define an API key for a given provider, there are three primary options: Directly include it in the YAML configuration file. Use an environment variable to store the API key and reference it in the YAML configuration file. Define your API key in a file and reference the location of that key-bearing file within the YAML configuration file. If you choose to include the API key directly, replace $OPENAI_API_KEY in the YAML file with your\nactual API key. Warning The MLflow AI Gateway provides direct access to billed external LLM services. It is strongly recommended to restrict access to this server. See the section on security for guidance. If you prefer to use an environment variable (recommended), you can define it in your shell\nenvironment. For example: export OPENAI_API_KEY = \"your_openai_api_key\" Note: Replace \u00e2\u0080\u009cyour_openai_api_key\u00e2\u0080\u009d with your actual OpenAI API key.   AI Gateway server Configuration Details The MLflow AI Gateway relies on a user-provided configuration file. It defines how the gateway server interacts with various language model providers and dictates the endpoints that users can access. The configuration file is written in YAML and includes a series of sections, each representing a unique endpoint. Each endpoint section has a name, a type, and a model specification, which includes the provider, model name, and provider-specific configuration details. Here are the details of each configuration parameter:  General Configuration Parameters endpoints : This is a list of endpoint configurations. Each endpoint represents a unique endpoint that maps to a particular language model service. Each endpoint has the following configuration parameters: name : This is the name of the endpoint. It needs to be a unique name without spaces or any non-alphanumeric characters other than hyphen and underscore. endpoint_type : This specifies the type of service offered by this endpoint. This determines the interface for inputs to an endpoint and the returned outputs. Current supported endpoint types are: \u00e2\u0080\u009cllm/v1/completions\u00e2\u0080\u009d \u00e2\u0080\u009cllm/v1/chat\u00e2\u0080\u009d \u00e2\u0080\u009cllm/v1/embeddings\u00e2\u0080\u009d model : This defines the provider-specific details of the language model. It contains the following fields: provider : This indicates the provider of the AI model. It accepts the following values: \u00e2\u0080\u009copenai\u00e2\u0080\u009d \u00e2\u0080\u009cmosaicml\u00e2\u0080\u009d \u00e2\u0080\u009canthropic\u00e2\u0080\u009d \u00e2\u0080\u009ccohere\u00e2\u0080\u009d \u00e2\u0080\u009cpalm\u00e2\u0080\u009d \u00e2\u0080\u009cazure\u00e2\u0080\u009d / \u00e2\u0080\u009cazuread\u00e2\u0080\u009d \u00e2\u0080\u009cmlflow-model-serving\u00e2\u0080\u009d \u00e2\u0080\u009chuggingface-text-generation-inference\u00e2\u0080\u009d \u00e2\u0080\u009cai21labs\u00e2\u0080\u009d \u00e2\u0080\u009cbedrock\u00e2\u0080\u009d \u00e2\u0080\u009cmistral\u00e2\u0080\u009d \u00e2\u0080\u009ctogetherai\u00e2\u0080\u009d name : This is an optional field to specify the name of the model. config : This contains provider-specific configuration details.  Provider-Specific Configuration Parameters ",
        "id": "df9169b97d95c94a8e32de235106b887"
    },
    {
        "text": " OpenAI Configuration Parameter Required Default Description openai_api_key Yes This is the API key for the OpenAI service. openai_api_type No This is an optional field to specify the type of OpenAI API\nto use. openai_api_base No https://api.openai.com/v1 This is the base URL for the OpenAI API. openai_api_version No This is an optional field to specify the OpenAI API\nversion. openai_organization No This is an optional field to specify the organization in\nOpenAI.  MosaicML Configuration Parameter Required Default Description mosaicml_api_key Yes N/A This is the API key for the MosaicML service.  Cohere Configuration Parameter Required Default Description cohere_api_key Yes N/A This is the API key for the Cohere service.  HuggingFace Text Generation Inference Configuration Parameter Required Default Description hf_server_url Yes N/A This is the url of the Huggingface TGI Server.  PaLM Configuration Parameter Required Default Description palm_api_key Yes N/A This is the API key for the PaLM service.  AI21 Labs Configuration Parameter Required Default Description ai21labs_api_key Yes N/A This is the API key for the AI21 Labs service.  Anthropic Configuration Parameter Required Default Description anthropic_api_key Yes N/A This is the API key for the Anthropic service.  Amazon Bedrock Top-level model configuration for Amazon Bedrock endpoints must be one of the following two supported authentication modes: key-based or role-based . Configuration Parameter Required Default Description aws_config No An object with either the key-based or role-based\nschema below. To use key-based authentication, define an Amazon Bedrock endpoint with the required fields below.\n.. note: If using a configured endpoint purely for development or testing , utilizing an IAM User role or a temporary short - lived standard IAM role are recommended ; while for production deployments , a standard long - expiry IAM role is recommended to ensure that the endpoint is capable of handling authentication for a long period . If the authentication expires and a new set of keys need to be supplied , the endpoint must be recreated in order to persist the new keys . Configuration Parameter Required Default Description aws_region No AWS_REGION/AWS_DEFAULT_REGION The AWS Region to use for bedrock access. aws_secret_access_key Yes AWS secret access key for the IAM user/role\nauthorized to use bedrock aws_access_key_id Yes AWS access key ID for the IAM user/role\nauthorized to use Bedrock aws_session_token No None Optional session token, if required Alternatively, for role-based authentication, an Amazon Bedrock endpoint can be defined and initialized with an a IAM Role  ARN that is authorized to access Bedrock.  The MLflow AI Gateway will attempt to assume this role with using the standard credential provider chain and will renew the role credentials if they have expired. Configuration Parameter Required Default Description aws_region No AWS_REGION/AWS_DEFAULT_REGION The AWS Region to use for bedrock access. aws_role_arn Yes An AWS role authorized to use Bedrock.  The standard\ncredential provider chain must be able to find\ncredentials authorized to assume this role. session_length_seconds No 900 The length of session to request.  MLflow Model Serving Configuration Parameter Required Default Description model_server_url Yes N/A This is the url of the MLflow Model Server. Note that with MLflow model serving, the name parameter for the model definition is not used for validation and is only present for reference purposes. This alias can be\nuseful for understanding a particular version or endpoint definition that was used that can be referenced back to a deployed model. You may choose any name that you wish, provided that\nit is JSON serializable. ",
        "id": "a529a57d6eb04704a88cc658ae0a6b59"
    },
    {
        "text": " Azure OpenAI Azure provides two different mechanisms for integrating with OpenAI, each corresponding to a different type of security validation. One relies on an access token for validation, referred to as azure , while the other uses Azure Active Directory (Azure AD) integration for authentication, termed as azuread . To match your user\u00e2\u0080\u0099s interaction and security access requirements, adjust the openai_api_type parameter to represent the preferred security validation model. This will ensure seamless interaction and reliable security for your Azure-OpenAI integration. Configuration Parameter Required Default Description openai_api_key Yes This is the API key for the Azure OpenAI service. openai_api_type Yes This field must be either azure or azuread depending on the security access protocol. openai_api_base Yes This is the base URL for the Azure OpenAI API service provided by Azure. openai_api_version Yes The version of the Azure OpenAI service to utilize, specified by a date. openai_deployment_name Yes This is the name of the deployment resource for the Azure OpenAI service. openai_organization No This is an optional field to specify the organization in OpenAI.  Mistral Configuration Parameter Required Default Description mistral_api_key | Yes      | N/A                      | This is the API key for the Mistral service.  TogetherAI Configuration Parameter  | Required | Default  | Description togetherai_api_key Yes N/A This is the API key for the TogetherAI service. An example configuration for Azure OpenAI is: endpoints : - name : completions endpoint_type : llm/v1/completions model : provider : openai name : gpt-35-turbo config : openai_api_type : \"azuread\" openai_api_key : $AZURE_AAD_TOKEN openai_deployment_name : \"{your_deployment_name}\" openai_api_base : \"https://{your_resource_name}-azureopenai.openai.azure.com/\" openai_api_version : \"2023-05-15\" limit : renewal_period : minute calls : 10 Note Azure OpenAI has distinct features as compared with the direct OpenAI service. For an overview, please see the comparison documentation . For specifying an API key, there are three options: (Preferred) Use an environment variable to store the API key and reference it in the YAML configuration file. This is denoted by a $ symbol before the name of the environment variable. (Preferred) Define the API key in a file and reference the location of that key-bearing file within the YAML configuration file. Directly include it in the YAML configuration file. Important The use of environment variables or file-based keys is recommended for better security practices. If the API key is directly included in the configuration file, it should be ensured that the file is securely stored and appropriately access controlled.\nPlease ensure that the configuration file is stored in a secure location as it contains sensitive API keys.   Querying the AI Gateway server Once the MLflow AI Gateway has been configured and started, it is ready to receive traffic from users.   Standard Query Parameters The MLflow AI Gateway defines standard parameters for chat, completions, and embeddings that can be\nused when querying any endpoint regardless of its provider. Each parameter has a standard range and\ndefault value. When querying an endpoint with a particular provider, the MLflow AI Gateway automatically\nscales parameter values according to the provider\u00e2\u0080\u0099s value ranges for that parameter. ",
        "id": "3d0b38677a338a65fd142f6b4402fd7c"
    },
    {
        "text": " Completions The standard parameters for completions endpoints with type llm/v1/completions are: Query Parameter Type Required Default Description prompt string Yes N/A The prompt for which to generate completions. n integer No 1 The number of completions to generate for the\nspecified prompt, between 1 and 5. temperature float No 0.0 The sampling temperature to use, between 0 and 1.\nHigher values will make the output more random, and\nlower values will make the output more deterministic. max_tokens integer No None The maximum completion length, between 1 and infinity\n(unlimited). stop array[string] No None Sequences where the model should stop generating\ntokens and return the completion.  Chat The standard parameters for chat endpoints with type llm/v1/chat are: Query Parameter Type Required Default Description messages array[message] Yes N/A A list of messages in a conversation from which to\na new message (chat completion). For information\nabout the message structure, see Messages . n integer No 1 The number of chat completions to generate for the\nspecified prompt, between 1 and 5. temperature float No 0.0 The sampling temperature to use, between 0 and 1.\nHigher values will make the output more random, and\nlower values will make the output more deterministic. max_tokens integer No None The maximum completion length, between 1 and infinity\n(unlimited). stop array[string] No None Sequences where the model should stop generating\ntokens and return the chat completion.   Messages Each chat message is a string dictionary containing the following fields: Field Name Required Default Description role Yes N/A The role of the conversation participant who sent the\nmessage. Must be one of: \"system\" , \"user\" , or \"assistant\" . content Yes N/A The message content.  Embeddings The standard parameters for completions endpoints with type llm/v1/embeddings are: Query Parameter Type Required Default Description input string\nor\narray[string] Yes N/A A string or list of strings for which to generate\nembeddings.  Additional Query Parameters In addition to the Standard Query Parameters , you can pass any additional parameters supported by the endpoint\u00e2\u0080\u0099s provider as part of your query. For example: logit_bias (supported by OpenAI, Cohere) top_k (supported by MosaicML, Anthropic, PaLM, Cohere) frequency_penalty (supported by OpenAI, Cohere, AI21 Labs) presence_penalty (supported by OpenAI, Cohere, AI21 Labs) stream (supported by OpenAI, Cohere) Below is an example of submitting a query request to an MLflow AI Gateway endpoint using additional parameters: from mlflow.deployments import get_deploy_client client = get_deploy_client ( \"http://my.deployments:8888\" ) data = { \"prompt\" : ( \"What would happen if an asteroid the size of \" \"a basketball encountered the Earth traveling at 0.5c? \" \"Please provide your answer in .rst format for the purposes of documentation.\" ), \"temperature\" : 0.5 , \"max_tokens\" : 1000 , \"n\" : 1 , \"frequency_penalty\" : 0.2 , \"presence_penalty\" : 0.2 , } client . predict ( endpoint = \"completions-gpt4\" , inputs = data ) The results of the query are: { \"id\" : \"chatcmpl-8Pr33fsCAtD2L4oZHlyfOkiYHLapc\" , \"object\" : \"text_completion\" , \"created\" : 1701172809 , \"model\" : \"gpt-4-0613\" , \"choices\" : [ { \"index\" : 0 , \"text\" : \"If an asteroid the size of a basketball ...\" , } ], \"usage\" : { \"prompt_tokens\" : 43 , \"completion_tokens\" : 592 , \"total_tokens\" : 635 , }, } ",
        "id": "d231121934dafc6cc24b9dffba399a3a"
    },
    {
        "text": " Streaming Some providers support streaming responses. Streaming responses are useful when you want to\nreceive responses as they are generated, rather than waiting for the entire response to be\ngenerated before receiving it. Streaming responses are supported by the following providers: Provider Endpoints llm/v1/completions llm/v1/chat OpenAI \u00e2\u009c\u0093 \u00e2\u009c\u0093 Cohere \u00e2\u009c\u0093 \u00e2\u009c\u0093 Anthropic \u00e2\u009c\u0098 \u00e2\u009c\u0093 To enable streaming responses, set the stream parameter to true in your request. For example: curl -X POST http://my.deployments:8888/endpoints/chat/invocations \\ -H \"Content-Type: application/json\" \\ -d '{\"messages\": [{\"role\": \"user\", \"content\": \"hello\"}], \"stream\": true}' The results of the query follow the OpenAI schema .  Chat data: {\"choices\": [{\"delta\": {\"content\": null, \"role\": \"assistant\"}, \"finish_reason\": null, \"index\": 0}], \"created\": 1701161926, \"id\": \"chatcmpl-8PoDWSiVE8MHNsUZF2awkW5gNGYs3\", \"model\": \"gpt-35-turbo\", \"object\": \"chat.completion.chunk\"}\n\ndata: {\"choices\": [{\"delta\": {\"content\": \"Hello\", \"role\": null}, \"finish_reason\": null, \"index\": 0}], \"created\": 1701161926, \"id\": \"chatcmpl-8PoDWSiVE8MHNsUZF2awkW5gNGYs3\", \"model\": \"gpt-35-turbo\", \"object\": \"chat.completion.chunk\"}\n\ndata: {\"choices\": [{\"delta\": {\"content\": \" there\", \"role\": null}, \"finish_reason\": null, \"index\": 0}], \"created\": 1701161926, \"id\": \"chatcmpl-8PoDWSiVE8MHNsUZF2awkW5gNGYs3\", \"model\": \"gpt-35-turbo\", \"object\": \"chat.completion.chunk\"}\n\ndata: {\"choices\": [{\"delta\": {\"content\": null, \"role\": null}, \"finish_reason\": \"stop\", \"index\": 0}], \"created\": 1701161926, \"id\": \"chatcmpl-8PoDWSiVE8MHNsUZF2awkW5gNGYs3\", \"model\": \"gpt-35-turbo\", \"object\": \"chat.completion.chunk\"}  Completions data: {\"choices\": [{\"delta\": {\"role\": null, \"content\": null}, \"finish_reason\": null, \"index\": 0}], \"created\": 1701161629, \"id\": \"chatcmpl-8Po8jVXzljc245k1Ah4UsAcm2zxQ2\", \"model\": \"gpt-35-turbo\", \"object\": \"text_completion_chunk\"}\n\ndata: {\"choices\": [{\"delta\": {\"role\": null, \"content\": \"If\"}, \"finish_reason\": null, \"index\": 0}], \"created\": 1701161629, \"id\": \"chatcmpl-8Po8jVXzljc245k1Ah4UsAcm2zxQ2\", \"model\": \"gpt-35-turbo\", \"object\": \"text_completion_chunk\"}\n\ndata: {\"choices\": [{\"delta\": {\"role\": null, \"content\": \" an\"}, \"finish_reason\": null, \"index\": 0}], \"created\": 1701161629, \"id\": \"chatcmpl-8Po8jVXzljc245k1Ah4UsAcm2zxQ2\", \"model\": \"gpt-35-turbo\", \"object\": \"text_completion_chunk\"}\n\ndata: {\"choices\": [{\"delta\": {\"role\": null, \"content\": \" asteroid\"}, \"finish_reason\": null, \"index\": 0}], \"created\": 1701161629, \"id\": \"chatcmpl-8Po8jVXzljc245k1Ah4UsAcm2zxQ2\", \"model\": \"gpt-35-turbo\", \"object\": \"text_completion_chunk\"}\n\ndata: {\"choices\": [{\"delta\": {\"role\": null, \"content\": null}, \"finish_reason\": \"length\", \"index\": 0}], \"created\": 1701161629, \"id\": \"chatcmpl-8Po8jVXzljc245k1Ah4UsAcm2zxQ2\", \"model\": \"gpt-35-turbo\", \"object\": \"text_completion_chunk\"}  FastAPI Documentation (\u00e2\u0080\u009c/docs\u00e2\u0080\u009d) FastAPI, the framework used for building the MLflow AI Gateway, provides an automatic interactive API\ndocumentation interface, which is accessible at the \u00e2\u0080\u009c/docs\u00e2\u0080\u009d endpoint (e.g., http://my.deployments:9000/docs ).\nThis interactive interface is very handy for exploring and testing the available API endpoints. As a convenience, accessing the root URL (e.g., http://my.deployments:9000 ) redirects to this \u00e2\u0080\u009c/docs\u00e2\u0080\u009d endpoint.  MLflow Python Client APIs MlflowDeploymentClient is the user-facing client API that is used to interact with the MLflow AI Gateway.\nIt abstracts the HTTP requests to the gateway server via a simple, easy-to-use Python API. ",
        "id": "77180519199ef47632458b56b08d2398"
    },
    {
        "text": "  Client API To use the MlflowDeploymentClient API, see the below examples for the available API methods: Create an MlflowDeploymentClient from mlflow.deployments import get_deploy_client client = get_deploy_client ( \"http://my.deployments:8888\" ) List all endpoints: The list_endpoints() method returns a list of all endpoints. endpoint = client . list_endpoints () for endpoint in endpoints : print ( endpoint ) Query an endpoint: The predict() method submits a query to a configured provider endpoint.\nThe data structure you send in the query depends on the endpoint. response = client . predict ( endpoint = \"chat\" , inputs = { \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"Tell me a joke about rabbits\" }]}, ) print ( response )  LangChain Integration LangChain supports an integration for MLflow Deployments .\nThis integration enable users to use prompt engineering, retrieval augmented generation, and other techniques with LLMs in the gateway server.  Example import mlflow from langchain import LLMChain , PromptTemplate from langchain.llms import Mlflow llm = Mlflow ( target_uri = \"http://127.0.0.1:5000\" , endpoint = \"completions\" ) llm_chain = LLMChain ( llm = llm , prompt = PromptTemplate ( input_variables = [ \"adjective\" ], template = \"Tell me a {adjective} joke\" , ), ) result = llm_chain . run ( adjective = \"funny\" ) print ( result ) with mlflow . start_run (): model_info = mlflow . langchain . log_model ( llm_chain , \"model\" ) model = mlflow . pyfunc . load_model ( model_info . model_uri ) print ( model . predict ([{ \"adjective\" : \"funny\" }]))   MLflow Models Interfacing with MLflow Models can be done in two ways. With the use of a custom PyFunc Model, a query can be issued directly to a gateway server endpoint and used in a broader context within a model.\nData may be augmented, manipulated, or used in a mixture of experts paradigm. The other means of utilizing the MLflow AI Gateway along with MLflow Models is to define a served MLflow model directly as\nan endpoint within a gateway server.  Using the gateway server to Query a served MLflow Model For a full walkthrough and example of using the MLflow serving integration to query a model directly through the MLflow AI Gateway, please see the full example .\nWithin the guide, you will see the entire end-to-end process of serving multiple models from different servers and configuring an MLflow AI Gateway instance to provide a single unified point to handle queries from. ",
        "id": "626a402ef40f426c94c20d9f511c29d5"
    },
    {
        "text": " Using an MLflow Model to Query the gateway server You can also build and deploy MLflow Models that call the MLflow AI Gateway.\nThe example below demonstrates how to use a gateway server from within a custom pyfunc model. Note The custom Model shown in the example below is utilizing environment variables for the gateway server\u00e2\u0080\u0099s uri. These values can also be set manually within the\ndefinition or can be applied via mlflow.deployments.get_deployments_target() after the uri has been set. For the example below, the value for MLFLOW_DEPLOYMENTS_TARGET is http://127.0.0.1:5000/ . For an actual deployment use case, this value would be set to the configured and production deployment server. import os import pandas as pd import mlflow def predict ( data ): from mlflow.deployments import get_deploy_client client = get_deploy_client ( os . environ [ \"MLFLOW_DEPLOYMENTS_TARGET\" ]) payload = data . to_dict ( orient = \"records\" ) return [ client . predict ( endpoint = \"completions\" , inputs = query )[ \"choices\" ][ 0 ][ \"text\" ] for query in payload ] input_example = pd . DataFrame . from_dict ( { \"prompt\" : [ \"Where is the moon?\" , \"What is a comet made of?\" ]} ) signature = mlflow . models . infer_signature ( input_example , [ \"Above our heads.\" , \"It's mostly ice and rocks.\" ] ) with mlflow . start_run (): model_info = mlflow . pyfunc . log_model ( python_model = predict , registered_model_name = \"anthropic_completions\" , artifact_path = \"anthropic_completions\" , input_example = input_example , signature = signature , ) df = pd . DataFrame . from_dict ( { \"prompt\" : [ \"Tell me about Jupiter\" , \"Tell me about Saturn\" ], \"temperature\" : 0.6 , \"max_records\" : 500 , } ) loaded_model = mlflow . pyfunc . load_model ( model_info . model_uri ) print ( loaded_model . predict ( df )) This custom MLflow model can be used in the same way as any other MLflow model. It can be used within a spark_udf , used with mlflow.evaluate() , or deploy like any other model.   REST API The REST API allows you to send HTTP requests directly to the MLflow AI Gateway. This is useful if you\u00e2\u0080\u0099re not using Python or if you prefer to interact with a gateway server using HTTP directly. Here are some examples for how you might use curl to interact with the MLflow AI Gateway: Get information about a particular endpoint: GET /api/2.0/endpoints/{name} This route returns a serialized representation of the endpoint data structure.\nThis provides information about the name and type, as well as the model details for the requested endpoint. curl -X GET http://my.deployments:8888/api/2.0/endpoints/embeddings List all endpoints: GET /api/2.0/endpoints/ This route returns a list of all endpoints. curl -X GET http://my.deployments:8888/api/2.0/endpoints/ Query an endpoint: POST /endpoints/{name}/invocations This route allows you to submit a query to a configured provider endpoint. The data structure you send in the query depends on the endpoint. Here are examples for the \u00e2\u0080\u009ccompletions\u00e2\u0080\u009d, \u00e2\u0080\u009cchat\u00e2\u0080\u009d, and \u00e2\u0080\u009cembeddings\u00e2\u0080\u009d endpoints: Completions curl -X POST http://my.deployments:8888/endpoints/completions/invocations \\ -H \"Content-Type: application/json\" \\ -d '{\"prompt\": \"Describe the probability distribution of the decay chain of U-235\"}' Chat curl -X POST http://my.deployments:8888/endpoints/chat/invocations \\ -H \"Content-Type: application/json\" \\ -d '{\"messages\": [{\"role\": \"user\", \"content\": \"Can you write a limerick about orange flavored popsicles?\"}]}' Embeddings curl -X POST http://my.deployments:8888/endpoints/embeddings/invocations \\ -H \"Content-Type: application/json\" \\ -d '{\"input\": [\"I would like to return my shipment of beanie babies, please\", \"Can I please speak to a human now?\"]}' Note: Remember to replace my.deployments:8888 with the URL of your actual MLflow AI Gateway. ",
        "id": "f94fe8521268f6cd7a083467f24511d5"
    },
    {
        "text": "  Plugin LLM Provider (Experimental) Attention This feature is in active development and is marked as Experimental. It may change in a future release without warning. The MLflow AI Gateway supports the use of custom language model providers through the use of plugins.\nA plugin is a Python package that provides a custom implementation of a language model provider.\nThis allows users to integrate their own language model services with the MLflow AI Gateway. To create a custom plugin, you need to implement a provider class that inherits from mlflow.gateway.providers.BaseProvider ,\nand a config class that inherits from mlflow.gateway.base_models.ConfigModel .  Example import os from typing import AsyncIterable from pydantic import validator from mlflow.gateway.base_models import ConfigModel from mlflow.gateway.config import RouteConfig from mlflow.gateway.providers import BaseProvider from mlflow.gateway.schemas import chat , completions , embeddings class MyLLMConfig ( ConfigModel ): # This model defines the configuration for the provider such as API keys my_llm_api_key : str @validator ( \"my_llm_api_key\" , pre = True ) def validate_my_llm_api_key ( cls , value ): return os . environ [ value . lstrip ( \"$\" )] class MyLLMProvider ( BaseProvider ): # Define the provider name. This will be displayed in log and error messages. NAME = \"my_llm\" # Define the config model for the provider. # This must be a subclass of ConfigModel. CONFIG_TYPE = MyLLMConfig def __init__ ( self , config : RouteConfig ) -> None : super () . __init__ ( config ) if config . model . config is None or not isinstance ( config . model . config , MyLLMConfig ): raise TypeError ( f \"Unexpected config type { config . model . config } \" ) self . my_llm_config : MyLLMConfig = config . model . config # You can implement one or more of the following methods # depending on the capabilities of your provider. # Implementing `completions`, `chat` and `embeddings` will enable the respective endpoints. # Implementing `completions_stream` and `chat_stream` will enable the `stream=True` # option for the respective endpoints. # Unimplemented methods will return a 501 Not Implemented HTTP response upon invocation. async def completions_stream ( self , payload : completions . RequestPayload ) -> AsyncIterable [ completions . StreamResponsePayload ]: ... async def completions ( self , payload : completions . RequestPayload ) -> completions . ResponsePayload : ... async def chat_stream ( self , payload : chat . RequestPayload ) -> AsyncIterable [ chat . StreamResponsePayload ]: ... async def chat ( self , payload : chat . RequestPayload ) -> chat . ResponsePayload : ... async def embeddings ( self , payload : embeddings . RequestPayload ) -> embeddings . ResponsePayload : ... Then, you need to create a Python package that contains the plugin implementation.\nYou must specify an entry point under the mlflow.gateway.providers group, so that your plugin can be detected by MLflow.\nThe entry point should be in the format <name> = <module>:<class> . ",
        "id": "de0a8825bde014d2b2a7f07c8fc5d23b"
    },
    {
        "text": " pyproject.toml [project] name = \"my_llm\" version = \"1.0\" [project.entry-points. \"mlflow.gateway.providers\" ] my_llm = \"my_llm.providers:MyLLMProvider\" [tool.setuptools.packages.find] include = [ \"my_llm*\" ] namespaces = false You can specify more than one entry point in the same package if you have multiple providers.\nNote that entry point names must be globally unique. If two plugins specify the same entry point name,\nMLflow will raise an error at startup time. MLflow already provides a number of providers by default. Your plugin name cannot be the same as any one\nof them. See AI Gateway server Configuration Details for a complete list of default providers. Finally, you need to install the plugin package in the same environment as the MLflow AI Gateway. Important Only install plugin packages from sources that you trust. Starting a server with a plugin provider will\nexecute any arbitrary code that is defined within the plugin package. Then, you can specify the plugin provider according to the entry point name\nin the MLflow AI Gateway configuration file. endpoints : - name : chat endpoint_type : llm/v1/chat model : provider : my_llm name : my-model-0.1.2 config : my_llm_api_key : $MY_LLM_API_KEY  Example A working example can be found in the MLflow repository at examples/deployments/deployments_server/plugin .  MLflow AI Gateway API Documentation API documentation  OpenAI Compatibility MLflow AI Gateway is compatible with OpenAI API and supports the chat , completions , and embeddings APIs.\nThe OpenAI client can be used to query the server as shown in the example below: Create a configuration file: endpoints : - name : my-chat endpoint_type : llm/v1/chat model : provider : openai name : gpt-4o-mini config : openai_api_key : $OPENAI_API_KEY Start the server with the configuration file: mlflow gateway start --config-path /path/to/config.yaml --port 7000 Once the server is up and running, query the server using the OpenAI client: from openai import OpenAI client = OpenAI ( base_url = \"http://localhost:7000/v1\" ) completion = client . chat . completions . create ( model = \"my-chat\" , messages = [{ \"role\" : \"user\" , \"content\" : \"Hello\" }], ) print ( completion . choices [ 0 ] . message . content )  Unity Catalog Integration See Unity Catalog Integration for how to integrate the MLflow AI Gateway with Unity Catalog. ",
        "id": "d7157d6cc0707fa9ba02c9e0da68517c"
    },
    {
        "text": "  gateway server Security Considerations Remember to ensure secure access to the system that the MLflow AI Gateway is running in to protect access to these keys. An effective way to secure your gateway server is by placing it behind a reverse proxy. This will allow the reverse proxy to handle incoming requests and forward them to the MLflow AI Gateway. The reverse proxy effectively shields your application from direct exposure to Internet traffic. A popular choice for a reverse proxy is Nginx . In addition to handling the traffic to your application, Nginx can also serve static files and load balance the traffic if you have multiple instances of your application running. Furthermore, to ensure the integrity and confidentiality of data between the client and the server, it\u00e2\u0080\u0099s highly recommended to enable HTTPS on your reverse proxy. In addition to the reverse proxy, it\u00e2\u0080\u0099s also recommended to add an authentication layer before the requests reach the MLflow AI Gateway. This could be HTTP Basic Authentication, OAuth, or any other method that suits your needs. For example, here\u00e2\u0080\u0099s a simple configuration for Nginx with Basic Authentication: http { server { listen 80 ; location / { auth_basic \"Restricted Content\" ; auth_basic_user_file /etc/nginx/.htpasswd ; proxy_pass http://localhost:5000 ; # Replace with the MLflow AI Gateway port } } } In this example, /etc/nginx/.htpasswd is a file that contains the username and password for authentication. These measures, together with a proper network setup, can significantly improve the security of your system and ensure that only authorized users have access to submit requests to your LLM services. Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "de9625345985e3bf7f7c2d7673b25a0d"
    },
    {
        "text": "MLflow LLM Evaluation 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation MLflow LLM Evaluation Full Notebook Guides and Examples Quickstart LLM Evaluation Metrics Prepare Your Target Models Viewing Evaluation Results Benefits of MLflow\u00e2\u0080\u0099s LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow LLM Evaluation   MLflow LLM Evaluation With the emerging of ChatGPT, LLMs have shown its power of text generation in various fields, such as\nquestion answering, translating and text summarization. Evaluating LLMs\u00e2\u0080\u0099 performance is slightly different\nfrom traditional ML models, as very often there is no single ground truth to compare against.\nMLflow provides an API mlflow.evaluate() to help evaluate your LLMs. MLflow\u00e2\u0080\u0099s LLM evaluation functionality consists of 3 main components: A model to evaluate : it can be an MLflow pyfunc model, a URI pointing to one registered\nMLflow model, or any python callable that represents your model, e.g, a HuggingFace text summarization pipeline. Metrics : the metrics to compute, LLM evaluate will use LLM metrics. Evaluation data : the data your model is evaluated at, it can be a pandas Dataframe, a python list, a\nnumpy array or an mlflow.data.dataset.Dataset() instance.  Full Notebook Guides and Examples If you\u00e2\u0080\u0099re interested in thorough use-case oriented guides that showcase the simplicity and power of MLflow\u00e2\u0080\u0099s evaluate\nfunctionality for LLMs, please navigate to the notebook collection below: View the Notebook Guides ",
        "id": "64b308c6dd2947e062a79a8915783b41"
    },
    {
        "text": " Quickstart Below is a simple example that gives an quick overview of how MLflow LLM evaluation works. The example builds\na simple question-answering model by wrapping \u00e2\u0080\u009copenai/gpt-4\u00e2\u0080\u009d with custom prompt. You can paste it to\nyour IPython or local editor and execute it, and install missing dependencies as prompted. Running the code\nrequires OpenAI API key, if you don\u00e2\u0080\u0099t have an OpenAI key, you can set it up by following the OpenAI guide . export OPENAI_API_KEY = 'your-api-key-here' import mlflow import openai import os import pandas as pd from getpass import getpass eval_data = pd . DataFrame ( { \"inputs\" : [ \"What is MLflow?\" , \"What is Spark?\" , ], \"ground_truth\" : [ \"MLflow is an open-source platform for managing the end-to-end machine learning (ML) \" \"lifecycle. It was developed by Databricks, a company that specializes in big data and \" \"machine learning solutions. MLflow is designed to address the challenges that data \" \"scientists and machine learning engineers face when developing, training, and deploying \" \"machine learning models.\" , \"Apache Spark is an open-source, distributed computing system designed for big data \" \"processing and analytics. It was developed in response to limitations of the Hadoop \" \"MapReduce computing model, offering improvements in speed and ease of use. Spark \" \"provides libraries for various tasks such as data ingestion, processing, and analysis \" \"through its components like Spark SQL for structured data, Spark Streaming for \" \"real-time data processing, and MLlib for machine learning tasks\" , ], } ) with mlflow . start_run () as run : system_prompt = \"Answer the following question in two sentences\" # Wrap \"gpt-4\" as an MLflow model. logged_model_info = mlflow . openai . log_model ( model = \"gpt-4\" , task = openai . chat . completions , artifact_path = \"model\" , messages = [ { \"role\" : \"system\" , \"content\" : system_prompt }, { \"role\" : \"user\" , \"content\" : \" {question} \" }, ], ) # Use predefined question-answering metrics to evaluate our model. results = mlflow . evaluate ( logged_model_info . model_uri , eval_data , targets = \"ground_truth\" , model_type = \"question-answering\" , ) print ( f \"See aggregated evaluation results below: \\n { results . metrics } \" ) # Evaluation result for each data record is available in `results.tables`. eval_table = results . tables [ \"eval_results_table\" ] print ( f \"See evaluation table below: \\n { eval_table } \" )  LLM Evaluation Metrics There are two types of LLM evaluation metrics in MLflow: Heuristic-based metrics : These metrics calculate a score for each data record (row in terms of Pandas/Spark dataframe), based on certain functions, such as: Rouge ( rougeL() ), Flesch Kincaid ( flesch_kincaid_grade_level() ) or Bilingual Evaluation Understudy (BLEU) ( bleu() ). These metrics are similar to traditional continuous value metrics. For the list of built-in heuristic metrics and how to define a custom metric with your own function definition, see the Heuristic-based Metrics section. LLM-as-a-Judge metrics : LLM-as-a-Judge is a new type of metric that uses LLMs to score the quality of model outputs. It overcomes the limitations of heuristic-based metrics, which often miss nuances like context and semantic accuracy. LLM-as-a-Judge metrics provides a more human-like evaluation for complex language tasks while being more scalable and cost-effective than human evaluation. MLflow provides various built-in LLM-as-a-Judge metrics and supports creating custom metrics with your own prompt, grading criteria, and reference examples. See the LLM-as-a-Judge Metrics section for more details.  Heuristic-based Metrics  Built-in Heuristic Metrics See this page for the full list of the built-in heuristic metrics. ",
        "id": "b2465c8a5387cd1a219f5648357ea2be"
    },
    {
        "text": "  Default Metrics with Pre-defined Model Types MLflow LLM evaluation includes default collections of metrics for pre-selected tasks, e.g, \u00e2\u0080\u009cquestion-answering\u00e2\u0080\u009d. Depending on the\nLLM use case that you are evaluating, these pre-defined collections can greatly simplify the process of running evaluations. To use\ndefaults metrics for pre-selected tasks, specify the model_type argument in mlflow.evaluate() , as shown by the example\nbelow: results = mlflow . evaluate ( model , eval_data , targets = \"ground_truth\" , model_type = \"question-answering\" , ) The supported LLM model types and associated metrics are listed below: question-answering : model_type=\"question-answering\" : exact-match toxicity 1 ari_grade_level 2 flesch_kincaid_grade_level 2 text-summarization : model_type=\"text-summarization\" : ROUGE 3 toxicity 1 ari_grade_level 2 flesch_kincaid_grade_level 2 text models : model_type=\"text\" : toxicity 1 ari_grade_level 2 flesch_kincaid_grade_level 2 retrievers : model_type=\"retriever\" : precision_at_k 4 recall_at_k 4 ndcg_at_k 4 1 Requires packages evaluate , torch , and transformers 2 Requires package textstat 3 Requires packages evaluate , nltk , and rouge-score 4 All retriever metrics have a default retriever_k value of 3 that can be overridden by specifying retriever_k in the evaluator_config argument.   Use a Custom List of Metrics Using the pre-defined metrics associated with a given model type is not the only way to generate scoring metrics\nfor LLM evaluation in MLflow. You can specify a custom list of metrics in the extra_metrics argument in mlflow.evaluate : To add additional metrics to the default metrics list of pre-defined model type, keep the model_type and add your metrics to extra_metrics : results = mlflow . evaluate ( model , eval_data , targets = \"ground_truth\" , model_type = \"question-answering\" , extra_metrics = [ mlflow . metrics . latency ()], ) The above code will evaluate your model using all metrics for \u00e2\u0080\u009cquestion-answering\u00e2\u0080\u009d model plus mlflow.metrics.latency() . To disable default metric calculation and only calculate your selected metrics, remove the model_type argument and define the desired metrics. results = mlflow . evaluate ( model , eval_data , targets = \"ground_truth\" , extra_metrics = [ mlflow . metrics . toxicity (), mlflow . metrics . latency ()], ) The full reference for supported evaluation metrics can be found here . ",
        "id": "e8b57169e9abc1d2fb65a1c1118b4c29"
    },
    {
        "text": " Create Custom heuristic-based LLM Evaluation Metrics This is very similar to creating custom traditional metrics, with the exception of returning a mlflow.metrics.MetricValue() instance.\nBasically you need to: Implement An eval_fn to define your scoring logic. This function must take in 2 args: predictions and targets . eval_fn must return a mlflow.metrics.MetricValue() instance. Pass eval_fn and other arguments to the mlflow.metrics.make_metric API to create the metric. The following code creates a dummy per-row metric called \"over_10_chars\" ; if the model output is greater than 10,\nthe score is \u00e2\u0080\u009cyes\u00e2\u0080\u009d, otherwise it is \u00e2\u0080\u009cno\u00e2\u0080\u009d. def eval_fn ( predictions , targets ): scores = [ \"yes\" if len ( pred ) > 10 else \"no\" for pred in predictions ] return MetricValue ( scores = scores , aggregate_results = standard_aggregations ( scores ), ) # Create an EvaluationMetric object. passing_code_metric = make_metric ( eval_fn = eval_fn , greater_is_better = False , name = \"over_10_chars\" ) To create a custom metric that is dependent on other metrics, include those other metrics\u00e2\u0080\u0099 names as an argument after predictions and targets . This can be the name of a builtin metric or another custom metric.\nEnsure that you do not accidentally have any circular dependencies in your metrics, or the evaluation will fail. The following code creates a dummy per-row metric called \"toxic_or_over_10_chars\" : if the model output is greater than 10 or the toxicity score is greater than 0.5, the score is \u00e2\u0080\u009cyes\u00e2\u0080\u009d, otherwise it is \u00e2\u0080\u009cno\u00e2\u0080\u009d. def eval_fn ( predictions , targets , toxicity , over_10_chars ): scores = [ \"yes\" if toxicity . scores [ i ] > 0.5 or over_10_chars . scores [ i ] else \"no\" for i in len ( toxicity . scores ) ] return MetricValue ( scores = scores ) # Create an EvaluationMetric object. toxic_and_over_10_chars_metric = make_metric ( eval_fn = eval_fn , greater_is_better = False , name = \"toxic_or_over_10_chars\" )  LLM-as-a-Judge Metrics LLM-as-a-Judge is a new type of metric that uses LLMs to score the quality of model outputs, providing a more human-like evaluation for complex language tasks while being more scalable and cost-effective than human evaluation. MLflow supports several builtin LLM-as-a-judge metrics, as well as allowing you to create your own LLM-as-a-judge metrics with custom configurations and prompts.  Built-in LLM-as-a-Judge metrics To use built-in LLM-as-a-Judge metrics in MLflow, pass the list of metrics definitions to the extra_metrics argument in the mlflow.evaluate() function. The following example uses the built-in answer correctness metric for evaluation, in addition to the latency metric (heuristic): from mlflow.metrics import latency from mlflow.metrics.genai import answer_correctness results = mlflow . evaluate ( eval_data , targets = \"ground_truth\" , extra_metrics = [ answer_correctness (), latency (), ], ) Here is the list of built-in LLM-as-a-Judge metrics. Click on the link to see the full documentation for each metric: answer_similarity() : Evaluate how similar a model\u00e2\u0080\u0099s generated output is compared to the information in the ground truth data. answer_correctness() : Evaluate how factually correct a model\u00e2\u0080\u0099s generated output is based on the information within the ground truth data. answer_relevance() : Evaluate how relevant the model generated output is to the input (context is ignored). relevance() : Evaluate how relevant the model generated output is with respect to both the input and the context. faithfulness() : Evaluate how faithful the model generated output is based on the context provided.  Selecting the Judge Model By default, MLflow will use OpenAI\u00e2\u0080\u0099s GPT-4 model as the judge model that scores metrics. You can change the judge model by passing an override to the model argument within the metric definition. ",
        "id": "69e495dde720873dadb970d56414c167"
    },
    {
        "text": " 1. SaaS LLM Providers To use SaaS LLM providers, such as OpenAI or Anthropic, set the model parameter in the metrics definition, in the format of <provider>:/<model-name> . Currently, MLflow supports [\"openai\", \"anthropic\", \"bedrock\", \"mistral\", \"togetherai\"] as viable LLM providers for any judge model.  OpenAI / Azure OpenAI  Anthropic  Bedrock  Mistral  TogetherAI  OpenAI models can be accessed via the openai:/<model-name> URI. import mlflow import os os . environ [ \"OPENAI_API_KEY\" ] = \"<your-openai-api-key>\" answer_correctness = mlflow . metrics . genai . answer_correctness ( model = \"openai:/gpt-4o\" ) # Test the metric definition answer_correctness ( inputs = \"What is MLflow?\" , predictions = \"MLflow is an innovative full self-driving airship.\" , targets = \"MLflow is an open-source platform for managing the end-to-end ML lifecycle.\" , ) Azure OpenAI endpoints can be accessed via the same openai:/<model-name> URI, by setting the environment variables, such as OPENAI_API_BASE , OPENAI_API_TYPE , etc. os . environ [ \"OPENAI_API_TYPE\" ] = \"azure\" os . environ [ \"OPENAI_API_BASE\" ] = \"https:/my-azure-openai-endpoint.azure.com/\" os . environ [ \"OPENAI_DEPLOYMENT_NAME\" ] = \"gpt-4o-mini\" os . environ [ \"OPENAI_API_VERSION\" ] = \"2024-08-01-preview\" os . environ [ \"OPENAI_API_KEY\" ] = \"<your-api-key-for-azure-openai-endpoint>\"  Anthropic models can be accessed via the anthropic:/<model-name> URI. Note that the default judge parameters <#overriding-default-judge-parameters> need to be overridden by passing the parameters argument to the metrics definition, since the default parameters violates the Anthropic endpoint requirement ( temperature and top_p cannot be specified together). import mlflow import os os . environ [ \"ANTHROPIC_API_KEY\" ] = \"<your-anthropic-api-key>\" answer_correctness = mlflow . metrics . genai . answer_correctness ( model = \"anthropic:/claude-3-5-sonnet-20241022\" , # Override default judge parameters to meet Claude endpoint requirements. parameters = { \"temperature\" : 0 , \"max_tokens\" : 256 }, ) # Test the metric definition answer_correctness ( inputs = \"What is MLflow?\" , predictions = \"MLflow is an innovative full self-driving airship.\" , targets = \"MLflow is an open-source platform for managing the end-to-end ML lifecycle.\" , )  Bedrock models can be accessed via the bedrock:/<model-name> URI. Make sure you have set the authentication information via the environment variables. You can use both role-based or API key-based authentication for accessing Bedrock models. import mlflow import os os . environ [ \"AWS_REGION\" ] = \"<your-aws-region>\" # Option 1. Role-based authentication os . environ [ \"AWS_ROLE_ARN\" ] = \"<your-aws-role-arn>\" # Option 2. API key-based authentication os . environ [ \"AWS_ACCESS_KEY_ID\" ] = \"<your-aws-access-key-id>\" os . environ [ \"AWS_SECRET_ACCESS_KEY\" ] = \"<your-aws-secret-access-key>\" # You can also use session token for temporary credentials. # os.environ[\"AWS_SESSION_TOKEN\"] = \"<your-aws-session-token>\" answer_correctness = mlflow . metrics . genai . answer_correctness ( model = \"bedrock:/anthropic.claude-3-5-sonnet-20241022-v2:0\" , parameters = { \"temperature\" : 0 , \"max_tokens\" : 256 , \"anthropic_version\" : \"bedrock-2023-05-31\" , }, ) # Test the metric definition answer_correctness ( inputs = \"What is MLflow?\" , predictions = \"MLflow is an innovative full self-driving airship.\" , targets = \"MLflow is an open-source platform for managing the end-to-end ML lifecycle.\" , )  Mistral models can be accessed via the mistral:/<model-name> URI. import mlflow import os os . environ [ \"MISTRAL_API_KEY\" ] = \"<your-mistral-api-key>\" answer_correctness = mlflow . metrics . genai . answer_correctness ( model = \"mistral:/mistral-small-latest\" , ) # Test the metric definition answer_correctness ( inputs = \"What is MLflow?\" , predictions = \"MLflow is an innovative full self-driving airship.\" , targets = \"MLflow is an open-source platform for managing the end-to-end ML lifecycle.\" , ) ",
        "id": "460a01e5081bd9f961f35cd2c563fb55"
    },
    {
        "text": " TogetherAI models can be accessed via the togetherai:/<model-name> URI. import mlflow import os os . environ [ \"TOGETHERAI_API_KEY\" ] = \"<your-togetherai-api-key>\" answer_correctness = mlflow . metrics . genai . answer_correctness ( model = \"togetherai:/togetherai-small-latest\" , ) # Test the metric definition answer_correctness ( inputs = \"What is MLflow?\" , predictions = \"MLflow is an innovative full self-driving airship.\" , targets = \"MLflow is an open-source platform for managing the end-to-end ML lifecycle.\" , ) Note Your use of a third party LLM service (e.g., OpenAI) for evaluation may be subject to and governed by the LLM service\u00e2\u0080\u0099s terms of use.  2. Self-hosted Proxy Endpoints If you are accessing SaaS LLM providers via a proxy endpoint (e.g., for security compliance), you can set the proxy_url parameter in the metrics definition. Additionally, use the extra_headers parameters  to pass extra headers for the endpoint for authentication. answer_similarity = mlflow . metrics . genai . answer_similarity ( model = \"openai:/gpt-4o\" , proxy_url = \"https://my-proxy-endpoint/chat\" , extra_headers = { \"Group-ID\" : \"my-group-id\" }, )  3. MLflow AI Gateway Endpoints MLflow AI Gateway is a self-hosted solution that allows you to query various LLM providers in a unified interface. To use an endpoint hosted by MLflow AI Gateway: Start the MLflow AI Gateway server with your LLM setting by following these steps . Set the MLflow deployment client to target the server address by using set_deployments_target() . Set endpoints:/<endpoint-name> to the model parameter in the metrics definition. from mlflow.deployments import set_deployments_target # When the MLflow AI Gateway server is running at http://localhost:5000 set_deployments_target ( \"http://localhost:5000\" ) my_answer_similarity = mlflow . metrics . genai . answer_similarity ( model = \"endpoints:/my-endpoint\" )  4. Databricks Model Serving If you have a model hosted on Databricks, you can use it as a judge model by setting endpoints:/<endpoint-name> to the model parameter in the metrics definition. The following code uses a Llama 3.1 405B model available via the Foundation Model API . from mlflow.deployments import set_deployments_target set_deployments_target ( \"databricks\" ) llama3_answer_similarity = mlflow . metrics . genai . answer_similarity ( model = \"endpoints:/databricks-llama-3-1-405b-instruct\" )  Overriding Default Judge Parameters By default, MLflow queries the judge LLM model with the following parameters: temperature : 0.0 max_tokens : 200 top_p : 1.0 However, this might not be suitable for all LLM providers. For example, accessing Anthropic\u00e2\u0080\u0099s Claude models on Amazon Bedrock requires an anthropic_version parameter to be specified in the request payload. You can override these default parameters by passing the parameters argument to the metrics definition. my_answer_similarity = mlflow . metrics . genai . answer_similarity ( model = \"bedrock:/anthropic.claude-3-5-sonnet-20241022-v2:0\" , parameters = { \"temperature\" : 0 , \"max_tokens\" : 256 , \"anthropic_version\" : \"bedrock-2023-05-31\" , }, ) Note that the parameters dictionary you pass in the parameters argument will replace the default parameters , instead of being merged with them. For example, top_p will not be sent to the model in the above code example. ",
        "id": "47e2057b6379fec11103456905c84b04"
    },
    {
        "text": " Creating Custom LLM-as-a-Judge Metrics You can also create your own LLM-as-a-Judge evaluation metrics with mlflow.metrics.genai.make_genai_metric() API, which needs the following information: name : the name of your custom metric. definition : describe what\u00e2\u0080\u0099s the metric doing. grading_prompt : describe the scoring criteria. examples (Optional): a few input/output examples with scores provided; used as a reference for the LLM judge. See the API documentation for the full list of the configurations. Under the hood, definition , grading_prompt , examples together with evaluation data and model output will be\ncomposed into a long prompt and sent to LLM. If you are familiar with the concept of prompt engineering,\nSaaS LLM evaluation metric is basically trying to compose a \u00e2\u0080\u009cright\u00e2\u0080\u009d prompt containing instructions, data and model\noutput so that LLM, e.g., GPT4 can output the information we want. Now let\u00e2\u0080\u0099s create a custom GenAI metrics called \u00e2\u0080\u009cprofessionalism\u00e2\u0080\u009d, which measures how professional our model output is. Let\u00e2\u0080\u0099s first create a few examples with scores, these will be the reference samples LLM judge uses. To create such examples,\nwe will use mlflow.metrics.genai.EvaluationExample() class, which has 4 fields: input: input text. output: output text. score: the score for output in the context of input. justification: why do we give the score for the data. professionalism_example_score_2 = mlflow . metrics . genai . EvaluationExample ( input = \"What is MLflow?\" , output = ( \"MLflow is like your friendly neighborhood toolkit for managing your machine learning projects. It helps \" \"you track experiments, package your code and models, and collaborate with your team, making the whole ML \" \"workflow smoother. It's like your Swiss Army knife for machine learning!\" ), score = 2 , justification = ( \"The response is written in a casual tone. It uses contractions, filler words such as 'like', and \" \"exclamation points, which make it sound less professional. \" ), ) professionalism_example_score_4 = mlflow . metrics . genai . EvaluationExample ( input = \"What is MLflow?\" , output = ( \"MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was \" \"developed by Databricks, a company that specializes in big data and machine learning solu",
        "id": "22e4799e75617a508b40bed2c9bb4e2c"
    },
    {
        "text": ". EvaluationExample ( input = \"What is MLflow?\" , output = ( \"MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was \" \"developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is \" \"designed to address the challenges that data scientists and machine learning engineers face when \" \"developing, training, and deploying machine learning models.\" , ), score = 4 , justification = ( \"The response is written in a formal language and a neutral tone. \" ), ) Now let\u00e2\u0080\u0099s define the professionalism metric, you will see how each field is set up. professionalism = mlflow . metrics . genai . make_genai_metric ( name = \"professionalism\" , definition = ( \"Professionalism refers to the use of a formal, respectful, and appropriate style of communication that is \" \"tailored to the context and audience. It often involves avoiding overly casual language, slang, or \" \"colloquialisms, and instead using clear, concise, and respectful language.\" ), grading_prompt = ( \"Professionalism: If the answer is written using a professional tone, below are the details for different scores: \" \"- Score 0: Language is extremely casual, informal, and may include slang or colloquialisms. Not suitable for \" \"professional contexts.\" \"- Score 1: Language is casual but generally respectful and avoids strong informality or slang. Acceptable in \" \"some informal professional settings.\" \"- Score 2: Language is overall formal but still have casual words/phrases. Borderline for professional contexts.\" \"- Score 3: Language is balanced and avoids extreme informality or formality. Suitable for most professional contexts. \" \"- Score 4: Language is noticeably formal, respectful, and avoids casual elements. Appropriate for formal \" \"business or academic settings. \" ), examples = [ professionalism_example_score_2 , professionalism_example_score_4 ], model = \"openai:/gpt-4o-mini\" , parameters = { \"temperature\" : 0.0 }, aggregations = [ \"mean\" , \"variance\" ], greater_is_better = True , ) ",
        "id": "44bd0d5258eb8075d39029ea6701df01"
    },
    {
        "text": " Prepare Your Target Models In order to evaluate your model with mlflow.evaluate() , your model has to be one of the following types: A mlflow.pyfunc.PyFuncModel() instance or a URI pointing to a logged mlflow.pyfunc.PyFuncModel model. In\ngeneral we call that MLflow model. The A python function that takes in string inputs and outputs a single string. Your callable must match the signature of mlflow.pyfunc.PyFuncModel.predict() (without params argument), briefly it should: Has data as the only argument, which can be a pandas.Dataframe , numpy.ndarray , python list, dictionary or scipy matrix. Returns one of pandas.DataFrame , pandas.Series , numpy.ndarray or list. An MLflow Deployments endpoint URI pointing to a local MLflow AI Gateway , Databricks Foundation Models API , and External Models in Databricks Model Serving . Set model=None , and put model outputs in data . Only applicable when the data is a Pandas dataframe.  Evaluating with an MLflow Model For detailed instruction on how to convert your model into a mlflow.pyfunc.PyFuncModel instance, please read this doc . But in short,\nto evaluate your model as an MLflow model, we recommend following the steps below: Log your model to MLflow server by log_model . Each flavor ( opeanai , pytorch , \u00e2\u0080\u00a6)\nhas its own log_model API, e.g., mlflow.openai.log_model() : with mlflow . start_run (): system_prompt = \"Answer the following question in two sentences\" # Wrap \"gpt-4o-mini\" as an MLflow model. logged_model_info = mlflow . openai . log_model ( model = \"gpt-4o-mini\" , task = openai . chat . completions , artifact_path = \"model\" , messages = [ { \"role\" : \"system\" , \"content\" : system_prompt }, { \"role\" : \"user\" , \"content\" : \" {question} \" }, ], ) Use the URI of logged model as the model instance in mlflow.evaluate() : results = mlflow . evaluate ( logged_model_info . model_uri , eval_data , targets = \"ground_truth\" , model_type = \"question-answering\" , )   Evaluating with a Custom Function As of MLflow 2.8.0, mlflow.evaluate() supports evaluating a python function without requiring\nlogging the model to MLflow. This is useful when you don\u00e2\u0080\u0099t want to log the model and just want to evaluate\nit. The following example uses mlflow.evaluate() to evaluate a function. You also need to set\nup OpenAI authentication to run the code below. import mlflow import openai import pandas as pd from typing import List eval_data = pd . DataFrame ( { \"inputs\" : [ \"What is MLflow?\" , \"What is Spark?\" , ], \"ground_truth\" : [ \"MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.\" , \"Apache Spark is an open-source, distributed computing system designed for big data processing and analytics. It was developed in response to limitations of the Hadoop MapReduce computing model, offering improvements in speed and ease of use. Spark provides libraries for various tasks such as data ingestion, processing, and analysis through its components like Spark SQL for structured data, Spark Streaming for real-time data processing, and MLlib for machine learning tasks\" , ], } ) def openai_qa ( inputs : pd . DataFrame ) -> List [ str ]: predictions = [] system_prompt = \"Please answer the following question in formal language.\" for _ , row in inputs . iterrows (): completion = openai . chat . completions . create ( model = \"gpt-4o-mini\" , messages = [ { \"role\" : \"system\" , \"content\" : system_prompt }, { \"role\" : \"user\" , \"content\" : row [ \"inputs\" ]}, ], ) predictions . append ( completion . choices [ 0 ] . message . content ) return predictions with mlflow . start_run (): results = mlflow . evaluate ( model = openai_qa , data = eval_data , targets = \"ground_truth\" , model_type = \"question-answering\" , ) print ( results . metrics ) ",
        "id": "ccde35e7355e85a8e12bc4b66d269004"
    },
    {
        "text": " Output { \"flesch_kincaid_grade_level/v1/mean\" : 14.75 , \"flesch_kincaid_grade_level/v1/variance\" : 0.5625 , \"flesch_kincaid_grade_level/v1/p90\" : 15.35 , \"ari_grade_level/v1/mean\" : 18.15 , \"ari_grade_level/v1/variance\" : 0.5625 , \"ari_grade_level/v1/p90\" : 18.75 , \"exact_match/v1\" : 0.0 , }   Evaluating with an MLflow Deployments Endpoint For MLflow >= 2.11.0, mlflow.evaluate() supports evaluating a model endpoint by directly passing the MLflow Deployments endpoint URI to the model argument.\nThis is particularly useful when you want to evaluate a deployed model hosted by a local MLflow AI Gateway , Databricks Foundation Models API , and External Models in Databricks Model Serving , without implementing custom prediction logic to wrap it as an MLflow model or a python function. Please don\u00e2\u0080\u0099t forget to set the target deployment client by using mlflow.deployments.set_deployments_target() before calling mlflow.evaluate() with the endpoint URI, as shown in the example below. Otherwise, you will see an error message like MlflowException: No deployments target has been set... . Hint When you want to use an endpoint not hosted by an MLflow AI Gateway or Databricks, you can create a custom Python function following the Evaluating with a Custom Function guide and use it as the model argument.  Supported Input Data Formats The input data can be either of the following format when using an URI of the MLflow Deployment Endpoint as the model: Data Format Example Additional Notes A pandas DataFrame with a string column. pd . DataFrame ( { \"inputs\" : [ \"What is MLflow?\" , \"What is Spark?\" , ] } ) For this input format, MLflow will construct the appropriate request payload to the model endpoint type. For example, if your model is a chat endpoint ( llm/v1/chat ), MLflow will wrap your input string with the chat messages format like {\"messages\": [{\"role\": \"user\", \"content\": \"What is MLflow?\"}]} . If you want to customize the request payload e.g. including system prompt, please use the next format. A pandas DataFrame with a dictionary column. pd . DataFrame ( { \"inputs\" : [ { \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"Please answer.\" }, { \"role\" : \"user\" , \"content\" : \"What is MLflow?\" }, ], \"max_tokens\" : 100 , }, # ... more dictionary records ] } ) In this format, the dictionary should have the correct request format for your model endpoint. Please refer to the MLflow Deployments documentation for more information about the request format for different model endpoint types. A list of input strings. [ \"What is MLflow?\" , \"What is Spark?\" , ] The mlflow.evaluate() also accepts a list input. A list of request payload (dictionary). [ { \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"Please answer.\" }, { \"role\" : \"user\" , \"content\" : \"What is MLflow?\" }, ], \"max_tokens\" : 100 , }, # ... more dictionary records ] Similarly to Pandas DataFrame input, the dictionary should have the correct request format for your model endpoint.  Passing Inference Parameters You can pass additional inference parameters such as max_tokens , temperature , n , to the model endpoint by setting the inference_params argument in mlflow.evaluate() . The inference_params argument is a dictionary that contains the parameters to be passed to the model endpoint. The specified parameters are used for all the input record in the evaluation dataset. Note When your input is a dictionary format that represents request payload, it can also include the parameters like max_tokens . If there are overlapping parameters in both the inference_params and the input data, the values in the inference_params will take precedence. ",
        "id": "912c2541130c9e0d44c755245972e3d1"
    },
    {
        "text": " Examples Chat Endpoint hosted by a local MLflow AI Gateway import mlflow from mlflow.deployments import set_deployments_target import pandas as pd # Point the client to the local MLflow AI Gateway set_deployments_target ( \"http://localhost:5000\" ) eval_data = pd . DataFrame ( { # Input data must be a string column and named \"inputs\". \"inputs\" : [ \"What is MLflow?\" , \"What is Spark?\" , ], # Additional ground truth data for evaluating the answer \"ground_truth\" : [ \"MLflow is an open-source platform ....\" , \"Apache Spark is an open-source, ...\" , ], } ) with mlflow . start_run () as run : results = mlflow . evaluate ( model = \"endpoints:/my-chat-endpoint\" , data = eval_data , targets = \"ground_truth\" , inference_params = { \"max_tokens\" : 100 , \"temperature\" : 0.0 }, model_type = \"question-answering\" , ) Completion Endpoint hosted on Databricks Foundation Models API import mlflow from mlflow.deployments import set_deployments_target import pandas as pd # Point the client to Databricks Foundation Models API set_deployments_target ( \"databricks\" ) eval_data = pd . DataFrame ( { # Input data must be a string column and named \"inputs\". \"inputs\" : [ \"Write 3 reasons why you should use MLflow?\" , \"Can you explain the difference between classification and regression?\" , ], } ) with mlflow . start_run () as run : results = mlflow . evaluate ( model = \"endpoints:/databricks-mpt-7b-instruct\" , data = eval_data , inference_params = { \"max_tokens\" : 100 , \"temperature\" : 0.0 }, model_type = \"text\" , ) Evaluating External Models in Databricks Model Serving can be done in the same way, you just need to specify the different URI that points to the serving endpoint like \"endpoints:/your-chat-endpoint\" .   Evaluating with a Static Dataset For MLflow >= 2.8.0, mlflow.evaluate() supports evaluating a static dataset without specifying a model.\nThis is useful when you save the model output to a column in a Pandas DataFrame or an MLflow PandasDataset, and\nwant to evaluate the static dataset without re-running the model. If you are using a Pandas DataFrame, you must specify the column name that contains the model output using the\ntop-level predictions parameter in mlflow.evaluate() : import mlflow import pandas as pd eval_data = pd . DataFrame ( { \"inputs\" : [ \"What is MLflow?\" , \"What is Spark?\" , ], \"ground_truth\" : [ \"MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. \" \"It was developed by Databricks, a company that specializes in big data and machine learning solutions. \" \"MLflow is designed to address the challenges that data scientists and machine learning engineers \" \"face when developing, training, and deploying machine learning models.\" , \"Apache Spark is an open-source, distributed computing system designed for big data processing and \" \"analytics. It was developed in response to limitations of the Hadoop MapReduce computing model, \" \"offering improvements in speed and ease of use. Spark provides libraries for various tasks such as \" \"data ingestion, processing, and analysis through its components like Spark SQL for structured data, \" \"Spark Streaming for real-time data processing, and MLlib for machine learning tasks\" , ], \"predictions\" : [ \"MLflow is an open-source platform that provides handy tools to manage Machine Learning workflow \" \"lifecycle in a simple way\" , \"Spark is a popular open-source distributed computing system designed for big data processing and analytics.\" , ], } ) with mlflow . start_run () as run : results = mlflow . evaluate ( data = eval_data , targets = \"ground_truth\" , predictions = \"predictions\" , extra_metrics = [ mlflow . metrics . genai . answer_similarity ()], evaluators = \"default\" , ) print ( f \"See aggregated evaluation results below: \\n { results . metrics } \" ) eval_table = results . tables [ \"eval_results_table\" ] print ( f \"See evaluation table below: \\n { eval_table } \" )  Viewing Evaluation Results ",
        "id": "6b7cd2942d975984a3e0dd3bb52c1664"
    },
    {
        "text": " View Evaluation Results via Code mlflow.evaluate() returns the evaluation results as an mlflow.models.EvaluationResult() instance.\nTo see the score on selected metrics, you can check: metrics : stores the aggregated results, like average/variance across the evaluation dataset. Let\u00e2\u0080\u0099s take a second\npass on the code example above and focus on printing out the aggregated results. with mlflow . start_run () as run : results = mlflow . evaluate ( data = eval_data , targets = \"ground_truth\" , predictions = \"predictions\" , extra_metrics = [ mlflow . metrics . genai . answer_similarity ()], evaluators = \"default\" , ) print ( f \"See aggregated evaluation results below: \\n { results . metrics } \" ) tables[\"eval_results_table\"] : stores the per-row evaluation results. with mlflow . start_run () as run : results = mlflow . evaluate ( data = eval_data , targets = \"ground_truth\" , predictions = \"predictions\" , extra_metrics = [ mlflow . metrics . genai . answer_similarity ()], evaluators = \"default\" , ) print ( f \"See per-data evaluation results below: \\n { results . tables [ 'eval_results_table' ] } \" )  View Evaluation Results via the MLflow UI Your evaluation result is automatically logged into MLflow server, so you can view your evaluation results directly from the\nMLflow UI. To view the evaluation results on MLflow UI, please follow the steps below: Go to the experiment view of your MLflow experiment. Select the \u00e2\u0080\u009cEvaluation\u00e2\u0080\u009d tab. Select the runs you want to check evaluation results. Select the metrics from the dropdown menu on the right side. Please see the screenshot below for clarity: Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "638d3ffe4d4f53e5e33431b26e47f8fc"
    },
    {
        "text": "Prompt Engineering UI (Experimental) 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Prompt Engineering UI (Experimental) Quickstart Deployment for real-time serving Benefits of the MLflow Prompt Engineering UI Native MLflow Flavors for LLMs LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs Prompt Engineering UI (Experimental)   Prompt Engineering UI (Experimental) Starting in MLflow 2.7, the MLflow Tracking UI provides a best-in-class experience for prompt\nengineering. With no code required, you can try out multiple LLMs from the MLflow AI Gateway , parameter configurations, and prompts to build a variety of models for\nquestion answering, document summarization, and beyond. Using the embedded Evaluation UI, you can\nalso evaluate multiple models on a set of inputs and compare the responses to select the best one.\nEvery model created with the prompt engineering UI is stored in the MLflow Model format and can be deployed for batch or real time inference. All configurations (prompt templates,\nchoice of LLM, parameters, etc.) are tracked as MLflow Runs .   Quickstart The following guide will get you started with MLflow\u00e2\u0080\u0099s UI for prompt engineering.  Step 1: Create an MLflow AI Gateway Completions or Chat Endpoint To use the prompt engineering UI, you need to create one or more MLflow AI Gateway completions or chat Endpoints . Follow the MLflow AI Gateway Quickstart guide to easily create an endpoint in less than five\nminutes. If you already have access to an MLflow AI Gateway endpoint of type llm/v1/completions or llm/v1/chat , you can skip this step. mlflow gateway start --config-path config.yaml --port 7000  Step 2: Connect the MLflow AI Gateway to your MLflow Tracking Server The prompt engineering UI also requires a connection between the MLflow AI Gateway and the MLflow\nTracking Server. To connect the MLflow AI Gateway with the MLflow Tracking Server, simply set the MLFLOW_DEPLOYMENTS_TARGET environment variable in the environment where the server is running and\nrestart the server. For example, if the MLflow AI Gateway is running at http://localhost:7000 , you\ncan start an MLflow Tracking Server in a shell on your local machine and connect it to the\nMLflow AI Gateway using the mlflow server command as follows: export MLFLOW_DEPLOYMENTS_TARGET = \"http://127.0.0.1:7000\" mlflow server --port 5000  Step 3: Create or find an MLflow Experiment Next, open an existing MLflow Experiment in the MLflow UI, or create a new experiment.  Step 4: Create a run with prompt engineering Once you have opened the Experiment, click the New Run button and select using Prompt Engineering . This will open the prompt engineering playground where you can try\nout different LLMs, parameters, and prompts.  Step 5: Select your endpoint and evaluate the example prompt Next, click the Select endpoint dropdown and select the MLflow AI Gateway completions endpoint you created in\nStep 1. Then, click the Evaluate button to test out an example prompt engineering use case\nfor generating product advertisements. MLflow will embed the specified stock_type input\nvariable value - \"books\" - into the specified prompt  template and send it to the LLM\nassociated with the MLflow AI Gateway endpoint with the configured temperature (currently 0.01 )\nand max_tokens (currently 1000). The LLM response will appear in the Output section. ",
        "id": "30da82ffe2c8232a5d950c8a12df6aa3"
    },
    {
        "text": " Step 6: Try a prompt of your choosing Replace the prompt template from the previous step with a prompt template of your choosing.\nPrompts can define multiple variables. For example, you can use the following prompt template\nto instruct the LLM to answer questions about the MLflow documentation: Read the following article from the MLflow documentation that appears between triple\nbackticks. Then, answer the question about the documentation that appears between triple quotes.\nInclude relevant links and code examples in your answer.\n\n```{{article}}```\n\n\"\"\"\n{{question}}\n\"\"\" Then, fill in the input variables. For example, in the MLflow documentation\nuse case, the article input variable can be set to the contents of https://mlflow.org/docs/latest/tracking.html#logging-data-to-runs and the question input variable\ncan be set to \"How do I create a new MLflow Run using the Python API?\" . Finally, click the Evaluate button to see the new output. You can also try choosing a larger\nvalue of temperature to observe how the LLM\u00e2\u0080\u0099s output changes.  Step 7: Capture your choice of LLM, prompt template, and parameters as an MLflow Run Once you\u00e2\u0080\u0099re satisfied with your chosen prompt template and parameters, click the Create Run button to store this information, along with your choice of LLM, as an MLflow Run. This will\ncreate a new Run with the prompt template, parameters, and choice of LLM stored as Run params.\nIt will also automatically create an MLflow Model with this information that can be used for batch\nor real-time inference. To view this information, click the Run name to open the Run page: You can also see the parameters and compare them with other configurations by opening the Table view tab: After your Run is created, MLflow will open the Evaluation tab where you can see your latest\nplayground input & output and try out additional inputs:  Step 8: Try new inputs To test the behavior of your chosen LLM, prompt template, and parameters on a new inputs: Click the Add Row button and fill in a value(s) your prompt template\u00e2\u0080\u0099s input variable(s).\nFor example, in the MLflow documentation use case, you can try asking a question\nunrelated to MLflow to see how the LLM responds. This is important to ensure that the application\nis robust to irrelevant inputs. Then, click the Evaluate button to see the output. Finally, click the Save button to store the new inputs and output.  Step 9: Adjust your prompt template and create a new Run As you try additional inputs, you might discover scenarios where your choice of LLM, prompt\ntemplate, and parameters doesn\u00e2\u0080\u0099t perform as well as you would like. For example, in the\nMLflow documentation use case, the LLM still attempts to answer irrelevant\nquestions about MLflow Projects even if the answer does not appear in the\nspecified article. To improve performance, create a new Run by selecting the Duplicate run option from the context\nmenu. For example, in the MLflow documentation use case, adding the following text to\nthe prompt template helps improve robustness to irrelevant questions: If the question does not relate to the article, respond exactly with the phrase\n\"I do not know how to answer that question.\" Do not include any additional text in your\nresponse. Then, from the prompt engineering playground, adjust the prompt template (and / or choice of\nLLM and parameters), evaluate an input, and click the Create Run button to create a new Run.  Step 10: Evaluate the new prompt template on previous inputs Now that you\u00e2\u0080\u0099ve made an adjustment to your prompt template, it\u00e2\u0080\u0099s important to make sure that\nthe new template performs well on the previous inputs and compare the outputs with older\nconfigurations. From the Evaluation tab, click the Evaluate all button next to the new Run to evaluate\nall of the previous inputs. Click the Save button to store the results. ",
        "id": "cc203762c3cace15c142914a64e839f0"
    },
    {
        "text": " Step 11: Load evaluation data programmatically All of the inputs and outputs produced by the MLflow prompt engineering UI and Evaluation UI are stored\nas artifacts in MLflow Runs. They can be accessed programmatically using the mlflow.load_table() API\nas follows: import mlflow mlflow . set_experiment ( \"/Path/to/your/prompt/engineering/experiment\" ) # Load input and output data across all Runs (configurations) as a Pandas DataFrame inputs_outputs_pdf = mlflow . load_table ( # All inputs and outputs created from the MLflow UI are stored in an artifact called # \"eval_results_table.json\" artifact_file = \"eval_results_table.json\" , # Include the run ID as a column in the table to distinguish inputs and outputs # produced by different runs extra_columns = [ \"run_id\" ], ) # Optionally convert the Pandas DataFrame to Spark where it can be stored as a Delta # table or joined with existing Delta tables inputs_outputs_sdf = spark . createDataFrame ( inputs_outputs_pdf )   Step 12: Generate predictions programmatically Once you have found a configuration of LLM, prompt template, and parameters that performs well, you\ncan generate predictions using the corresponding MLflow Model in a Python environment of your choosing,\nor you can deploy it for real-time serving . To load the MLflow Model in a notebook for batch inference, click on the Run\u00e2\u0080\u0099s name to open the Run Page and select the model directory in the Artifact Viewer . Then, copy the first\nfew lines of code from the Predict on a Pandas DataFrame section and run them in a Python\nenvironment of your choosing, for example: import mlflow logged_model = \"runs:/8451075c46964f82b85fe16c3d2b7ea0/model\" # Load model as a PyFuncModel. loaded_model = mlflow . pyfunc . load_model ( logged_model ) Then, to generate predictions, call the predict() method\nand pass in a dictionary of input variables. For example: article_text = \"\"\" An MLflow Project is a format for packaging data science code in a reusable and reproducible way. The MLflow Projects component includes an API and command-line tools for running projects, which also integrate with the Tracking component to automatically record the parameters and git commit of your source code for reproducibility. This article describes the format of an MLflow Project and how to run an MLflow project remotely using the MLflow CLI, which makes it easy to vertically scale your data science code. \"\"\" question = \"What is an MLflow project?\" loaded_model . predict ({ \"article\" : article_text , \"question\" : question }) For more information about deployment for real-time serving with MLflow,\nsee the instructions below . ",
        "id": "5787a693d022b01b9e8295d75b43bad0"
    },
    {
        "text": " Step 13: Perform metric-based evaluation of your model\u00e2\u0080\u0099s outputs If you\u00e2\u0080\u0099d like to assess your model\u00e2\u0080\u0099s performance on specific metrics, MLflow provides the mlflow.evaluate() API. Let\u00e2\u0080\u0099s evaluate our model on some pre-defined metrics for text summarization: import mlflow import pandas as pd logged_model = \"runs:/840a5c43f3fb46f2a2059b761557c1d0/model\" article_text = \"\"\" An MLflow Project is a format for packaging data science code in a reusable and reproducible way. The MLflow Projects component includes an API and command-line tools for running projects, which also integrate with the Tracking component to automatically record the parameters and git commit of your source code for reproducibility. This article describes the format of an MLflow Project and how to run an MLflow project remotely using the MLflow CLI, which makes it easy to vertically scale your data science code. \"\"\" question = \"What is an MLflow project?\" data = pd . DataFrame ( { \"article\" : [ article_text ], \"question\" : [ question ], \"ground_truth\" : [ article_text ], # used for certain evaluation metrics, such as ROUGE score } ) with mlflow . start_run (): results = mlflow . evaluate ( model = logged_model , data = data , targets = \"ground_truth\" , model_type = \"text-summarization\" , ) eval_table = results . tables [ \"eval_results_table\" ] print ( f \"See evaluation table below: \\n { eval_table } \" ) The evaluation results can also be viewed in the MLflow Evaluation UI: The mlflow.evaluate() API also supports custom metrics , static dataset evaluation , and much more. For a\nmore in-depth guide, see MLflow LLM Evaluation .   Deployment for real-time serving Once you have found a configuration of LLM, prompt template, and parameters that performs well, you\ncan deploy the corresponding MLflow Model for real-time serving as follows: Register your model with the MLflow Model Registry. The following example registers\nan MLflow Model created from the Quickstart as Version 1 of the\nRegistered Model named \"mlflow_docs_qa_model\" . mlflow . register_model ( model_uri = \"runs:/8451075c46964f82b85fe16c3d2b7ea0/model\" , name = \"mlflow_docs_qa_model\" , ) Define the following environment variables in the environment where you will run your\nMLflow Model Server, such as a shell on your local machine: MLFLOW_DEPLOYMENTS_TARGET : The URL of the MLflow AI Gateway Use the mlflow models serve command to start the MLflow Model Server. For example,\nrunning the following command from a shell on your local machine will serve the model\non port 8000: mlflow models serve --model-uri models:/mlflow_docs_qa_model/1 --port 8000 Once the server has been started, it can be queried via REST API call. For example: input = ' { \"dataframe_records\": [ { \"article\": \"An MLflow Project is a format for packaging data science code...\", \"question\": \"What is an MLflow Project?\" } ] }' echo $input | curl \\ -s \\ -X POST \\ https://localhost:8000/invocations -H 'Content-Type: application/json' \\ -d @- where article and question are replaced with the input variable(s) from your\nprompt template. Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "e3f3f825a56586487f6a5cdbc127b55a"
    },
    {
        "text": "MLflow Transformers Flavor 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor Introduction Getting Started with the MLflow Transformers Flavor - Tutorials and Guides Important Details to be aware of with the transformers flavor Logging Large Models Working with tasks for Transformer Pipelines Detailed Documentation Learn more about Transformers MLflow OpenAI Flavor MLflow Sentence-Transformers Flavor MLflow LangChain Flavor MLflow LlamaIndex Flavor MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow Transformers Flavor  MLflow Transformers Flavor Attention The transformers flavor is in active development and is marked as Experimental. Public APIs may change and new features are\nsubject to be added as additional functionality is brought to the flavor.  Introduction Transformers by \u00f0\u009f\u00a4\u0097 Hugging Face represents a cornerstone in the realm of\nmachine learning, offering state-of-the-art capabilities for a multitude of frameworks including PyTorch , TensorFlow , and JAX .\nThis library has become the de facto standard for natural language processing (NLP) and audio transcription processing.\nIt also provides a compelling and advanced set of options for computer vision and multimodal AI tasks.\nTransformers achieves all of this by providing pre-trained models and accessible high-level APIs that are not only powerful\nbut also versatile and easy to implement. For instance, one of the cornerstones of the simplicity of the transformers library is the pipeline API ,\nan encapsulation of the most common NLP tasks into a single API call. This API allows users to perform a variety of tasks based on the specified task without\nhaving to worry about the underlying model or the preprocessing steps. ",
        "id": "6d9e24e6de972bfc19103167fb9a5893"
    },
    {
        "text": " Transformers Pipeline Architecture for the Whisper Model The integration of the Transformers library with MLflow enhances the management of machine learning workflows, from experiment\ntracking to model deployment. This combination offers a robust and efficient pathway for incorporating advanced NLP and AI capabilities\ninto your applications. Key Features of the Transformers Library : Access to Pre-trained Models : A vast collection of pre-trained models for various tasks, minimizing training time and resources. Task Versatility : Support for multiple modalities including text, image, and speech processing tasks. Framework Interoperability : Compatibility with PyTorch, TensorFlow, JAX, ONNX, and TorchScript. Community Support : An active community for collaboration and support, accessible via forums and the Hugging Face Hub. MLflow\u00e2\u0080\u0099s Transformers Flavor : MLflow supports the use of the Transformers package by providing: Simplified Experiment Tracking : Efficient logging of parameters, metrics, and models during the fine-tuning process . Effortless Model Deployment : Streamlined deployment to various production environments. Library Integration : Integration with HuggingFace libraries like Accelerate , PEFT for model optimization. Prompt Management : Save prompt templates with transformers pipelines to optimize inference with less boilerplate. Example Use Case: For an illustration of fine-tuning a model and logging the results with MLflow, refer to the fine-tuning tutorials . These tutorial demonstrate the process of fine-tuning a pretrained foundational model into the application-specific model such as a spam classifier, SQL generator. MLflow plays a pivotal role in tracking the fine-tuning process, including datasets, hyperparameters, performance metrics, and the final model artifacts. The image below shows the result of the tutorial within the MLflow UI.  Fine-tuning a Transformers Model with MLflow  Deployment Made Easy Once a model is trained, it needs to be deployed for inference .\nMLflow\u00e2\u0080\u0099s integration with Transformers simplifies this by providing functions such as mlflow.transformers.load_model() and mlflow.pyfunc.load_model() , which allow for easy model serving.\nAs part of the feature support for enhanced inference with transformers, MLflow provides mechanisms to enable the use of inference\narguments that can reduce the computational overhead and lower the memory requirements\nfor deployment.  Getting Started with the MLflow Transformers Flavor - Tutorials and Guides Below, you will find a number of guides that focus on different use cases using transformers that leverage MLflow\u00e2\u0080\u0099s\nAPIs for tracking and inference capabilities.  Introductory Quickstart to using Transformers with MLflow If this is your first exposure to transformers or use transformers extensively but are new to MLflow, this is a great place to start. Quickstart: Text Generation with Transformers Learn how to leverage the transformers integration with MLflow in this introductory quickstart .   Transformers Fine-Tuning Tutorials with MLflow Fine-tuning a model is a common task in machine learning workflows. These tutorials are designed to showcase how to fine-tune a model using the transformers library with harnessing MLflow\u00e2\u0080\u0099s APIs for tracking experiment configurations and results. Fine tuning a transformers Foundation Model Learn how to fine-tune a transformers model using MLflow to keep track of the training process and to log a use-case-specific tuned pipeline. Fine tuning LLMs efficiently using PEFT and MLflow Learn how to fine-tune a large foundational models with significantly reduced memory usage using PEFT (QLoRA) and MLflow. ",
        "id": "dd86d5a2716916a7d1b795ac3ee2fb56"
    },
    {
        "text": " Use Case Tutorials for Transformers with MLflow Interested in learning about how to leverage transformers for tasks other than basic text generation? Want to learn more about the breadth of problems that you can solve with transformers and MLflow? These more advanced tutorials are designed to showcase different applications of the transformers model architecture and how to leverage MLflow to track and deploy these models. Audio Transcription with Transformers Learn how to leverage the Whisper Model with MLflow to generate accurate audio transcriptions. Translation with Transformers Learn about the options for saving and loading transformers models in MLflow for customization of your workflows with a fun translation example! Chat with Transformers Learn the basics of stateful chat Conversational Pipelines with Transformers and MLflow. Building and Serving an OpenAI-Compatible Chatbot Learn how to build an OpenAI-compatible chatbot using a local Transformers\n                    model and MLflow, and serve it with minimal configuration. Prompt templating with Transformers Pipelines Learn how to set prompt templates on Transformers Pipelines to optimize your LLM's outputs, and simplify the end-user experience. Custom PyFunc for Transformers Learn how to define a custom PyFunc using transformers for advanced, state-of-the-art new models.  Important Details to be aware of with the transformers flavor When working with the transformers flavor in MLflow, there are several important considerations to keep in mind: Experimental Status : The Transformers flavor in MLflow is marked as experimental, which means that APIs are subject to change, and new features may be added over time with potentially breaking changes. PyFunc Limitations : Not all output from a Transformers pipeline may be captured when using the python_function flavor. For example, if additional references or scores are required from the output, the native implementation should be used instead. Also not all the pipeline types are supported for pyfunc. Please refer to Loading a Transformers Model as a Python Function for the supported pipeline types and their input and output format. Supported Pipeline Types : Not all Transformers pipeline types are currently supported for use with the python_function flavor. In particular, new model architectures may not be supported until the transformers library has a designated pipeline type in its supported pipeline implementations. Input and Output Types : The input and output types for the python_function implementation may differ from those expected from the native pipeline. Users need to ensure compatibility with their data processing workflows. Model Configuration : When saving or logging models, the model_config can be used to set certain parameters. However, if both model_config and a ModelSignature with parameters are saved, the default parameters in ModelSignature will override those in model_config . Audio and Vision Models : Audio and text-based large language models are supported for use with pyfunc, while other types like computer vision and multi-modal models are only supported for native type loading. Prompt Templates : Prompt templating is currently supported for a few pipeline types. For a full list of supported pipelines, and more information about the feature, see this link .  Logging Large Models By default, MLflow consumes certain memory footprint and storage space for logging models. This can be a concern when working with large foundational models with billions of parameters. To address this, MLflow provides a few optimization techniques to reduce resource consumption during logging and speed up the logging process. Please refer to the Working with Large Models in MLflow Transformers flavor guide to learn more about these tips. ",
        "id": "2594ec564408cef47c96b3769e10e900"
    },
    {
        "text": " Working with tasks for Transformer Pipelines In MLflow Transformers flavor, task plays a crucial role in determining the input and output format of the model. Please refer to the Tasks in MLflow Transformers guide on how to use the native Transformers task types, and leverage the advanced tasks such as llm/v1/chat and llm/v1/completions for OpenAI-compatible inference.  Detailed Documentation To learn more about the nuances of the transformers flavor in MLflow, delve into the comprehensive guide , which covers: Pipelines vs. Component Logging : Explore the different approaches for saving model components or complete pipelines and understand the nuances of loading these models for various use cases. Transformers Model as a Python Function : Familiarize yourself with the various transformers pipeline types compatible with the pyfunc model flavor. Understand the standardization of input and output formats in the pyfunc model implementation for the flavor, ensuring seamless integration with JSON and Pandas DataFrames. Prompt Template : Learn how to save a prompt template with transformers pipelines to optimize inference with less boilerplate. Model Config and Model Signature Params for Inference : Learn how to leverage model_config and ModelSignature for flexible and customized model loading and inference. Automatic Metadata and ModelCard Logging : Discover the automatic logging features for model cards and other metadata, enhancing model documentation and transparency. Model Signature Inference : Learn about MLflow\u00e2\u0080\u0099s capability within the transformers flavor to automatically infer and attach model signatures, facilitating easier model deployment. Overriding Pytorch dtype : Gain insights into optimizing transformers models for inference, focusing on memory optimization and data type configurations. Input Data Types for Audio Pipelines : Understand the specific requirements for handling audio data in transformers pipelines, including the handling of different input types like str, bytes, and np.ndarray. PEFT Models in MLflow Transformers flavor : PEFT (Parameter-Efficient Fine-Tuning) is natively supported in MLflow, enabling various optimization techniques like LoRA, QLoRA, and more for reducing fine-tuning cost significantly. Check out the guide and tutorials to learn more about how to leverage PEFT with MLflow.  Learn more about Transformers Interested in learning more about how to leverage transformers for your machine learning workflows? \u00f0\u009f\u00a4\u0097 Hugging Face has a fantastic NLP course. Check it out and see how to leverage Transformers, Datasets, Tokenizers, and Accelerate . Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "1a0f43aab071c98fd80d840835700acd"
    },
    {
        "text": "MLflow OpenAI Flavor 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor MLflow OpenAI Flavor Introduction Autologging Support for the OpenAI integration Tracing with the OpenAI flavor What makes this Integration so Special? Features Getting Started with the MLflow OpenAI Flavor - Tutorials and Guides Detailed Documentation MLflow Sentence-Transformers Flavor MLflow LangChain Flavor MLflow LlamaIndex Flavor MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow OpenAI Flavor  MLflow OpenAI Flavor Attention The openai flavor is under active development and is marked as Experimental. Public APIs are\nsubject to change, and new features may be added as the flavor evolves.  Introduction OpenAI\u00e2\u0080\u0099s GPT Models represent a significant leap in natural language processing (NLP) capabilities.\nThe Generative Pre-trained Transformer (GPT) models are renowned for\ntheir ability to generate human-like text, comprehend complex queries, summarize extensive documents,\nand much more. OpenAI has been at the forefront of NLP technology, offering models that are\nversatile and widely applicable in various domains. Leveraging MLflow\u00e2\u0080\u0099s robust experiment tracking and model management framework, the integration with\nOpenAI\u00e2\u0080\u0099s GPT-based models enables practitioners to efficiently utilize these advanced NLP tools in their\nprojects. From simple text generation to complex conversational AI applications, the MLflow-OpenAI\nintegration brings a new level of ease and effectiveness to managing these powerful models. The integration includes: Text Analysis and Generation : Utilizing models like GPT-3.5 and GPT-4 for diverse text-related tasks. Conversational AI : Exploring the capabilities of the Chat Completions API for interactive, context-aware applications. Embeddings Generation : Corpus and text embeddings generation capabilities for advanced document retrieval use cases.  Autologging Support for the OpenAI integration If you\u00e2\u0080\u0099d like to learn more about autologging support for OpenAI within MLflow, please visit the OpenAI Autologging page.  Tracing with the OpenAI flavor MLflow\u00e2\u0080\u0099s OpenAI flavor includes an integrated automated tracing feature with the use of the mlflow.openai.autolog() API. To learn more about\nhow to log your development usage of the OpenAI SDK, please visit the guide to autologging tracing for this flavor. ",
        "id": "ce86b8bb9cf5d33fb6833c708ba8a1c3"
    },
    {
        "text": " What makes this Integration so Special? The combination of MLflow\u00e2\u0080\u0099s experiment tracking and model management with OpenAI\u00e2\u0080\u0099s cutting-edge NLP models unlocks new potential for AI applications.\nThis MLflow flavor for OpenAI simplifies the process of: Developing an application that leverages the power of OpenAI\u00e2\u0080\u0099s models. By simplifying the process of keeping track of the highly iterative and creative process of prompt engineering, MLflow prompt engineering makes sure that you never lose track of a great idea. Auditing and Reviewing your most promising experiments. The MLflow tracking service means that you can easily share the results of your work and get peer review of your work. Customizing the interface to your application. Whether you want to allow creative control with exposing parameters such as temperature or to relax cost controls by exposing max_tokens , MLflow allows you to configure default values and restrict the ability to modify the parameters used for inference. Tagging and annotating particular runs with tags during the iterative prompt engineering phase to flag particularly promising ideas that you and others can revisit later for inspiration, further testing, or deployment.  The Elephant in the Room: Prompt Engineering In other fields of applied ML, the process of iterating over hypotheses is time-consuming, tedious, and lends itself to developing habits of meticulously\nrecording every step of the feature refinement and training process. With the advent of generative AI and the latent power of state-of-the-art LLMs such as\nthose offered by OpenAI, the process of refining the performance of a solution is much shorter. In the span of an hour, you could easily craft and test\na dozen prompts. While this speed and ease of use is remarkably empowering, it generally leads to the dreaded realization after a few hours of experimentation that you can\u00e2\u0080\u0099t\nremember which of the dozens of prompts that you created hours ago was the one that created the best results that you remember seeing. This is where MLflow comes in. With MLflow, you can easily track the prompts that you use, the results that you get, and the artifacts that you generate. The figure below shows a fun take on this problem that MLflow helps to solve.  Prompt Engineering for space flight with MLflow By logging each of the prompts that are used throughout testing, not only can you easily reproduce the results that you get, but you can also share those\nresults with others so that they can evaluate the subjective quality of the results. Without tracking in place, you\u00e2\u0080\u0099re forced to come up with a solution for\nrecording the various parameters, prompts, test inputs, and results. You could save all of that time and effort by using MLflow with OpenAI, giving you more time to come up with fun prompts.  Features With the MLflow OpenAI flavor, users can: Save and log applications using OpenAI models within MLflow using mlflow.openai.save_model() and mlflow.openai.log_model() . Seamlessly track detailed experiments, including parameters , prompts , and artifacts associated with model runs. Deploy OpenAI models for various NLP applications with ease. Utilize mlflow.pyfunc.PythonModel for flexible Python function inference, enabling custom and innovative ML solutions. ",
        "id": "0dfda78a81607597aa09155fa1e88187"
    },
    {
        "text": " What can you do with OpenAI and MLflow? The integration of OpenAI\u00e2\u0080\u0099s advanced NLP models with MLflow\u00e2\u0080\u0099s robust model management capabilities opens up a vast array of potential real-world applications. Here are some powerful and impactful use cases: Automated Customer Support : Develop sophisticated chatbots that understand and respond to customer inquiries in a human-like manner, significantly improving customer service efficiency and satisfaction. Content Generation and Curation : Automatically generate high-quality, contextually relevant content for articles, blogs, or social media posts. Curate content by summarizing and categorizing large volumes of text data, enhancing content management strategies. Language Translation Services : Create advanced translation tools that not only convert text from one language to another but also capture nuances, idioms, and cultural context, bridging communication gaps more effectively. Sentiment Analysis for Market Research : Analyze customer feedback, social media posts, or product reviews to gauge public sentiment about brands, products, or services, providing valuable insights for marketing and product development teams. Personalized Education and Training Tools : Develop AI-driven educational platforms that can adapt content and teaching styles to individual learning preferences, making education more engaging and effective. Legal and Compliance Document Analysis : Automate the review and analysis of legal documents, contracts, and compliance materials, increasing accuracy and reducing the time and resources required for legal workflows. Healthcare Assistance and Research : Assist in medical research by summarizing and analyzing medical literature, patient records, or clinical trial data, contributing to faster and more informed decision-making in healthcare. Financial Analysis and Forecasting : Leverage NLP models to analyze financial reports, market trends, and news articles, providing deeper insights and predictions for investment strategies and economic forecasting. With MLflow\u00e2\u0080\u0099s integration, these applications not only benefit from the linguistic prowess of OpenAI\u00e2\u0080\u0099s models but also gain from streamlined tracking , version control , and deployment processes. This synergy empowers developers and businesses to build sophisticated, AI-driven solutions that address complex challenges and create new opportunities in various industries.  Deployment Made Easy Deploying OpenAI models becomes a breeze with MLflow. Functions like mlflow.openai.load_model() and mlflow.pyfunc.load_model() facilitate easy model serving.\nDiscover more about deploying models with MLflow , explore the deployments API ,\nand learn about starting a local model serving endpoint to fully leverage the deployment capabilities of MLflow.  Getting Started with the MLflow OpenAI Flavor - Tutorials and Guides Below, you will find a number of guides that focus on different ways that you can leverage the power of the openai library, leveraging MLflow\u00e2\u0080\u0099s\nAPIs for tracking and inference capabilities. The diagram below shows the basic scope of the level of complexity that the tutorials cover.  The range of content within the tutorials for the OpenAI flavor  Introductory Tutorial OpenAI Quickstart Learn the very basics of using the OpenAI package with MLflow with some simple prompt engineering and a fun use case to get\n                    started with this powerful integration. ",
        "id": "d4b548ef9ffb659f1c603aa3aa697ee0"
    },
    {
        "text": " Advanced Tutorials In these tutorials, the topics cover applied interactions with OpenAI models, leveraging custom Python Models to enhance the functionality beyond what is\npossible with the basic prompt-based interaction from the introductory tutorial.\nIf you\u00e2\u0080\u0099re new to this flavor, please start with the Introductory Tutorial above, as it has information about environment configurations that you\u00e2\u0080\u0099ll need\nto understand in order to get the notebooks in this section to work. OpenAI ChatCompletions Learn how to leverage the ChatCompletions endpoint in the OpenAI flavor to create a useful text messaging screening tool within MLflow. OpenAI Custom Python Model - Code Helper Learn how to leverage Custom Python Models with a useful Code Helper application that leverages OpenAI Models and MLflow. OpenAI Embeddings - Document Comparison Explore the application of embeddings with document comparison using an OpenAI model with MLflow.  Detailed Documentation To learn more about the details of the MLflow flavor for OpenAI, delve into the comprehensive guide below. View the Comprehensive Guide Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "0a269811b4083e6a9033cf059b008524"
    },
    {
        "text": "MLflow Sentence-Transformers Flavor 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor MLflow OpenAI Flavor MLflow Sentence-Transformers Flavor Introduction What makes this Library so Special? Features Getting Started with the MLflow Sentence Transformers Flavor - Tutorials and Guides Detailed Documentation Learning More About Sentence Transformers MLflow LangChain Flavor MLflow LlamaIndex Flavor MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow Sentence-Transformers Flavor  MLflow Sentence-Transformers Flavor Attention The sentence-transformers flavor is under active development and is marked as Experimental. Public APIs are subject to change,\nand new features may be added as the flavor evolves.  Introduction Sentence-Transformers is a groundbreaking Python library that specializes in producing high-quality, semantically rich embeddings\nfor sentences and paragraphs. Developed as an extension of the well-known Transformers library\nby \u00f0\u009f\u00a4\u0097 Hugging Face, Sentence-Transformers is tailored for tasks requiring a deep understanding of sentence-level context. This library is\nessential for NLP applications such as semantic search, text clustering, and similarity assessment. Leveraging pre-trained models like BERT, RoBERTa, and DistilBERT, which are fine-tuned for sentence embeddings, Sentence-Transformers simplifies the process\nof generating meaningful vector representations of text. The library stands out for its simplicity, efficiency, and the quality of embeddings it produces. The library features a number of powerful high-level utility functions for performing common follow-on tasks with sentence embeddings.\nThese include: Semantic Textual Similarity : Assessing the semantic similarity between two sentences. Semantic Search : Searching for the most semantically similar sentences in a corpus for a given query. Clustering : Grouping similar sentences together. Information Retrieval : Finding the most relevant sentences for a given query via document retrieval and ranking . Paraphrase Mining : Finding text entries that have similar (or identical) meaning in a large corpus of text.  What makes this Library so Special? Let\u00e2\u0080\u0099s take a look at a very basic representation of how the Sentence-Transformers library works and what you can do with it! ",
        "id": "7d2a3858afa8bc1c23c98f099c290140"
    },
    {
        "text": " Sentence-Transformers Model Architecture Overview Integrating Sentence-Transformers with MLflow, a platform dedicated to streamlining the entire machine learning lifecycle, enhances the experiment tracking and deployment\ncapabilities for these specialized NLP models. MLflow\u00e2\u0080\u0099s support for Sentence-Transformers enables practitioners to effectively manage experiments, track different model versions,\nand deploy models for various NLP tasks with ease. Sentence-Transformers offers: High-Quality Sentence Embeddings : Efficient generation of sentence embeddings that capture the contextual and semantic nuances of language. Pre-Trained Model Availability : Access to a diverse range of pre-trained models fine-tuned for sentence embedding tasks, streamlining the process of embedding generation. Ease of Use : Simplified API, making it accessible for both NLP experts and newcomers. Custom Training and Fine-Tuning : Flexibility to fine-tune models on specific datasets or train new models from scratch for tailored NLP solutions. With MLflow\u00e2\u0080\u0099s Sentence-Transformers flavor, users benefit from: Streamlined Experiment Tracking : Easily log parameters, metrics, and sentence embedding models during the training and fine-tuning process. Hassle-Free Deployment : Deploy sentence embedding models for various applications with straightforward API calls. Broad Model Compatibility : Support for a range of sentence embedding models from the Sentence-Transformers library, ensuring access to the latest in embedding technology. Whether you\u00e2\u0080\u0099re working on semantic text similarity, clustering, or information retrieval, MLflow\u00e2\u0080\u0099s integration with Sentence-Transformers provides a robust and efficient\npathway for incorporating advanced sentence-level understanding into your applications.  Features With MLflow\u00e2\u0080\u0099s Sentence-Transformers flavor, users can: Save and log Sentence-Transformer models within MLflow with the respective APIs: mlflow.sentence_transformers.save_model() and mlflow.sentence_transformers.log_model() . Track detailed experiments, including parameters , metrics , and artifacts associated with fine tuning runs. Deploy sentence embedding models for practical applications. Utilize the mlflow.pyfunc.PythonModel flavor for generic Python function inference, enabling complex and powerful custom ML solutions.  What can you do with Sentence Transformers and MLflow? One of the more powerful applications that can be built with these tools is a semantic search engine. By using readily available open source\ntooling, you can build a semantic search engine that can find the most semantically similar sentences in a corpus for a given query. This is\na significant improvement over traditional keyword-based search engines, which are limited in their ability to understand the context of a query. An example high-level architecture for such an application stack is shown below:  A basic architecture for a semantic search engine built with Sentence Transformers and MLflow  Deployment Made Easy Once a model is trained, it needs to be deployed for inference. MLflow\u00e2\u0080\u0099s integration with Sentence Transformers simplifies this by providing\nfunctions such as mlflow.sentence_transformers.load_model() and mlflow.pyfunc.load_model() , which allow for easy model serving.\nYou can read more about deploying models with MLflow , find further information on using the deployments API , and starting a local model serving endpoint to get a\ndeeper understanding of the deployment options that MLflow has available.  Getting Started with the MLflow Sentence Transformers Flavor - Tutorials and Guides Below, you will find a number of guides that focus on different ways that you can leverage the power of the sentence-transformers library, leveraging MLflow\u00e2\u0080\u0099s\nAPIs for tracking and inference capabilities.  Introductory Tutorial Sentence Transformers Quickstart Learn the very basics of using the Sentence Transformers package with MLflow to generate sentence embeddings from a logged model in\n                    both native and generic Python function formats. ",
        "id": "6e823f13c0317eed317005e5f2124b5e"
    },
    {
        "text": " Advanced Tutorials Semantic Similarity Tutorial Learn how to leverage sentence embeddings to determine similarity scores between two sentences. Semantic Search Tutorial Learn how to use sentence embeddings to find the most similar embedding within a corpus of text. Paraphrase Mining Tutorial Explore the power of paraphrase mining to identify semantically similar sentences in a corpus of text.  Detailed Documentation To learn more about the details of the MLflow flavor for sentence transformers, delve into the comprehensive guide below. View the Comprehensive Guide  Learning More About Sentence Transformers Sentence Transformers is a versatile framework for computing dense vector representations of sentences, paragraphs, and images. Based on transformer networks like BERT, RoBERTa, and XLM-RoBERTa, it offers state-of-the-art performance across various tasks. The framework is designed for easy use and customization, making it suitable for a wide range of applications in natural language processing and beyond. For those interested in delving deeper into Sentence Transformers, the following resources are invaluable:  Official Documentation and Source code Official Documentation : For a comprehensive guide to getting started, advanced usage, and API references, visit the Sentence Transformers Documentation . GitHub Repository : The Sentence Transformers GitHub repository is the primary source for the latest code, examples, and updates. Here, you can also report issues, contribute to the project, or explore how the community is using and extending the framework.  Official Guides and Tutorials for Sentence Transformers Training Custom Models : The framework supports fine-tuning of custom embedding models to achieve the best performance on specific tasks. Publications and Research : To understand the scientific foundations of Sentence Transformers, the publications section offers a collection of research papers that have been integrated into the framework. Application Examples : Explore a variety of application examples demonstrating the practical use of Sentence Transformers in different scenarios.  Library Resources PyPI Package : The PyPI page for Sentence Transformers provides information on installation, version history, and package dependencies. Conda Forge Package : For users preferring Conda as their package manager, the Conda Forge page for Sentence Transformers is the go-to resource for installation and package details. Pretrained Models : Sentence Transformers offers an extensive range of pretrained models optimized for various languages and tasks. These models can be easily integrated into your projects. Sentence Transformers is continually evolving, with regular updates and additions to its capabilities. Whether you\u00e2\u0080\u0099re a researcher, developer, or enthusiast in the field of natural language processing, these resources will help you make the most of this powerful tool. Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "525c0c5729cf33f42cff4a5b9d14857d"
    },
    {
        "text": "MLflow LangChain Flavor 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor MLflow OpenAI Flavor MLflow Sentence-Transformers Flavor MLflow LangChain Flavor Why use MLflow with LangChain? Automatic Logging Supported Elements in MLflow LangChain Integration Overview of Chains, Agents, and Retrievers Getting Started with the MLflow LangChain Flavor - Tutorials and Guides Detailed Documentation FAQ MLflow LlamaIndex Flavor MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow LangChain Flavor  MLflow LangChain Flavor Attention The langchain flavor is under active development and is marked as Experimental. Public APIs are\nsubject to change, and new features may be added as the flavor evolves. Welcome to the developer guide for the integration of LangChain with MLflow. This guide serves as a comprehensive\nresource for understanding and leveraging the combined capabilities of LangChain and MLflow in developing advanced language model applications. LangChain is a versatile framework designed for building applications powered by language models. It excels in creating context-aware applications\nthat utilize language models for reasoning and generating responses, enabling the development of sophisticated NLP applications. LangGraph is a complementary agent-based framework from the creators of Langchain, supporting the creation of\nstateful agent and multi-agent GenAI applications. LangGraph utilizes LangChain in order to interface with GenAI agent components.  Why use MLflow with LangChain? Aside from the benefits of using MLflow for managing and deploying machine learning models, the integration of LangChain with MLflow provides a number of\nbenefits that are associated with using LangChain within the broader MLflow ecosystem.  Experiment Tracking LangChain\u00e2\u0080\u0099s flexibility in experimenting with various agents, tools, and retrievers becomes even more powerful when paired with MLflow Tracking . This combination allows for rapid experimentation and iteration. You can effortlessly compare runs, making it easier to refine models and accelerate the journey from development to production deployment.  Dependency Management Deploy your LangChain application with confidence, leveraging MLflow\u00e2\u0080\u0099s ability to manage and record code and environment dependencies automatically.\nYou can also explicitly declare external resource dependencies, like the LLM serving endpoint or vector search index queried by your LangChain application.\nThese dependencies are tracked by MLflow as model metadata, so that downstream serving systems can ensure authentication from your\ndeployed LangChain application to these dependent resources just works. These features ensure consistency between development and production environments, reducing deployment risks with less manual intervention.  MLflow Evaluate MLflow Evaluate provides native capabilities within MLflow to evaluate language models. With this feature you can easily utilize automated evaluation algorithms on the results of your LangChain application\u00e2\u0080\u0099s inference results. This capability facilitates the efficient assessment of inference results from your LangChain application, ensuring robust performance analytics. ",
        "id": "a3f1cdc0b3868189982221d3b84a62fd"
    },
    {
        "text": " Observability MLflow Tracing is a new feature of MLflow that allows you to trace how data flows through your LangChain chain/agents/etc. This feature provides a visual representation of the data flow, making it easier to understand the behavior of your LangChain application and identify potential bottlenecks or issues. With its powerful Automatic Tracing capability, you can instrument your LangChain application without any code change but just running mlflow.langchain.autolog() command once.  Automatic Logging Autologging is a powerful one stop solution to achieve all the above benefits with just one line of code mlflow.langchain.autolog() . By enabling autologging, you can automatically log all the components of your LangChain application, including chains, agents, and retrievers, with minimal effort. This feature simplifies the process of tracking and managing your LangChain application, allowing you to focus on developing and improving your models. For more information on how to use this feature, refer to the MLflow LangChain Autologging Documentation .  Supported Elements in MLflow LangChain Integration Agents Retrievers Runnables LangGraph Complied Graph (only supported via Model-from-Code ) LLMChain (deprecated, only support for langchain<0.3.0 ) RetrievalQA (deprecated, only support for langchain<0.3.0 ) Warning There is a known deserialization issue when logging chains or agents dependent upon LangChain components from the partner packages such as langchain-openai . If you log such models using the legacy serialization based logging, some components may be loaded from the respective langchain-community package instead of the partner package library, which can lead to unexpected behavior or import errors when executing your code.\nTo avoid this issue, we strongly recommend using the Model-from-Code method for logging such models. This method allows you to bypass the model serialization and robustly save the model definition. Attention Logging chains/agents that include ChatOpenAI and AzureChatOpenAI requires MLflow>=2.12.0 and LangChain>=0.0.307 .  Overview of Chains, Agents, and Retrievers  Chain  Agents  Retrievers  Sequences of actions or steps hardcoded in code. Chains in LangChain combine various components like prompts, models, and output parsers to create a flow of processing steps. The figure below shows an example of interfacing directly with a SaaS LLM via API calls with no context to the history of the conversation in the top portion. The\nbottom portion shows the same queries being submitted to a LangChain chain that incorporates a conversation history state such that the entire conversation\u00e2\u0080\u0099s history\nis included with each subsequent input. Preserving conversational context in this manner is key to creating a \u00e2\u0080\u009cchat bot\u00e2\u0080\u009d.  Dynamic constructs that use language models to choose a sequence of actions. Unlike chains, agents decide the order of actions based on inputs, tools available, and intermediate outcomes.  Components in RetrievalQA chains responsible for sourcing relevant documents or data. Retrievers are key in applications where LLMs need to reference specific external information for accurate responses.  Getting Started with the MLflow LangChain Flavor - Tutorials and Guides  Introductory Tutorial In this introductory tutorial, you will learn the most fundamental components of LangChain and how to leverage the integration with MLflow to store, retrieve, and\nuse a chain. LangChain Quickstart Get started with MLflow and LangChain by exploring the simplest possible chain configuration of a prompt and model chained to create\n                    a single-purpose utility application.  Advanced Tutorials In these tutorials, you can learn about more complex usages of LangChain with MLflow. It is highly advised to read through the introductory tutorial prior to\nexploring these more advanced use cases. RAG tutorial with LangChain Learn how to build a LangChain RAG with MLflow integration to answer highly specific questions about the legality of business ventures. ",
        "id": "5a2c2b4ad5afaf2055645ddbd74490b7"
    },
    {
        "text": " Logging models from Code Since MLflow 2.12.2, MLflow introduced the ability to log LangChain models directly from a code definition. The feature provides several benefits to manage LangChain models: Avoid Serialization Complication : File handles, sockets, external connections, dynamic references, lambda functions and system resources are unpicklable. Some LangChain components do not support native serialization, e.g. RunnableLambda . No Pickling : Loading a pickle or cloudpickle file in a Python version that was different than the one used to serialize the object does not guarantee compatibility. Readability : The serialized objects are often hardly readable by humans. Model-from-code allows you to review your model definition via code. Refer to the Models From Code feature documentation for more information about this feature. In order to use this feature, you will utilize the mlflow.models.set_model() API to define the chain that you would like to log as an MLflow model.\nAfter having this set within your code that defines your chain, when logging your model, you will specify the path to the file that defines your chain. The following example demonstrates how to log a simple chain with this method: Define the chain in a separate Python file.** Tip If you are using Jupyter Notebook, you can use the %%writefile magic command to write the code cell directly to a file, without leaving the notebook to create it manually. %% writefile chain . py import os from operator import itemgetter from langchain_core.output_parsers import StrOutputParser from langchain_core.prompts import PromptTemplate from langchain_core.runnables import RunnableLambda from langchain_openai import OpenAI import mlflow mlflow . set_experiment ( \"Homework Helper\" ) mlflow . langchain . autolog () prompt = PromptTemplate ( template = \"You are a helpful tutor that evaluates my homework assignments and provides suggestions on areas for me to study further.\" \" Here is the question: {question} and my answer which I got wrong: {answer} \" , input_variables = [ \"question\" , \"answer\" ], ) def get_question ( input ): default = \"What is your name?\" if isinstance ( input_data [ 0 ], dict ): return input_data [ 0 ] . get ( \"content\" ) . get ( \"question\" , default ) return default def get_answer ( input ): default = \"My name is Bobo\" if isinstance ( input_data [ 0 ], dict ): return input_data [ 0 ] . get ( \"content\" ) . get ( \"answer\" , default ) return default model = OpenAI ( temperature = 0.95 ) chain = ( { \"question\" : itemgetter ( \"messages\" ) | RunnableLambda ( get_question ), \"answer\" : itemgetter ( \"messages\" ) | RunnableLambda ( get_answer ), } | prompt | model | StrOutputParser () ) mlflow . models . set_model ( chain ) Then from the main notebook, log the model via supplying the path to the file that defines the chain: from pprint import pprint import mlflow chain_path = \"chain.py\" with mlflow . start_run (): info = mlflow . langchain . log_model ( lc_model = chain_path , artifact_path = \"chain\" ) The model defined in chain.py is now logged to MLflow. You can load the model back and run inference: # Load the model and run inference homework_chain = mlflow . langchain . load_model ( model_uri = info . model_uri ) exam_question = { \"messages\" : [ { \"role\" : \"user\" , \"content\" : { \"question\" : \"What is the primary function of control rods in a nuclear reactor?\" , \"answer\" : \"To stir the primary coolant so that the neutrons are mixed well.\" , }, }, ] } response = homework_chain . invoke ( exam_question ) pprint ( response ) You can see the model is logged as a code on MLflow UI: Warning When logging models from code, make sure that your code does not contain any sensitive information, such as API keys, passwords, or other confidential data. The code will be stored in plain text in the MLflow model artifact, and anyone with access to the artifact will be able to view the code.  Detailed Documentation To learn more about the details of the MLflow LangChain flavor, read the detailed guide below. View the Comprehensive Guide  FAQ ",
        "id": "50466160063cb70ea3cdbdbb37c67529"
    },
    {
        "text": " I can\u00e2\u0080\u0099t load my chain! Allowing for Dangerous Deserialization : Pickle opt-in logic in LangChain will prevent components from being loaded via MLflow. You might see an error like this: ValueError: This code relies on the pickle module. You will need to set allow_dangerous_deserialization=True if you want to opt-in to\nallow deserialization of data using pickle. Data can be compromised by a malicious actor if not handled properly to include a malicious\npayload that when deserialized with pickle can execute arbitrary code on your machine. A change within LangChain that forces users to opt-in to pickle deserialization can create\nsome issues with loading chains, vector stores, retrievers, and agents that have been logged using MLflow. Because the option is not exposed per component\nto set this argument on the loader function, you will need to ensure that you are setting this option directly within the defined loader function when\nlogging the model. LangChain components that do not set this value will be saved without issue, but a ValueError will be raised when loading if unset. To fix this, simply re-log your model, specifying the option allow_dangerous_deserialization=True in your defined loader function. See the tutorial for LangChain retrievers for an example of specifying this\noption when logging a FAISS vector store instance within a loader_fn declaration.  I can\u00e2\u0080\u0099t save my chain, agent, or retriever with MLflow. Tip If you\u00e2\u0080\u0099re encountering issues with logging or saving LangChain components with MLflow, see the models from code feature documentation to determine if logging your model from a script file provides a simpler and more robust logging solution! Serialization Challenges with Cloudpickle : Serialization with cloudpickle can encounter limitations depending on the complexity of the objects. Some objects, especially those with intricate internal states or dependencies on external system resources, are not inherently pickleable. This limitation\narises because serialization essentially requires converting an object to a byte stream, which can be complex for objects tightly coupled with system states\nor those having external I/O operations. Try upgrading PyDantic to 2.x version to resolve this issue. Verifying Native Serialization Support : Ensure that the langchain object (chain, agent, or retriever) is serializable natively using langchain APIs if saving or logging with MLflow doesn\u00e2\u0080\u0099t work. Due to their complex structures, not all langchain components are readily serializable. If native serialization\nis not supported and MLflow doesn\u00e2\u0080\u0099t support saving the model, you can file an issue in the LangChain repository or\nask for guidance in the LangChain Discussions board . Keeping Up with New Features in MLflow : MLflow might not immediately support the latest LangChain features immediately. If a new feature is not supported in MLflow, consider filing a feature request on the MLflow GitHub issues page .\nWith the rapid pace of changes in libraries that are in heavy active development (such as LangChain\u00e2\u0080\u0099s release velocity ),\nbreaking changes, API refactoring, and fundamental functionality support for even existing features can cause integration issues. If there is a chain, agent,\nretriever, or any future structure within LangChain that you\u00e2\u0080\u0099d like to see supported, please let us know! ",
        "id": "527a6f87cac016def4ca00fc5aa2cb0b"
    },
    {
        "text": " I\u00e2\u0080\u0099m getting an AttributeError when saving my model Handling Dependency Installation in LangChain and MLflow : LangChain and MLflow do not automatically install all dependencies. Other packages that might be required for specific agents, retrievers, or tools may need to be explicitly defined when saving or logging your model.\nIf your model relies on these external component libraries (particularly for tools) that not included in the standard LangChain package, these dependencies\nwill not be automatically logged as part of the model at all times (see below for guidance on how to include them). Declaring Extra Dependencies : Use the extra_pip_requirements parameter when saving and logging. When saving or logging your model that contains external dependencies that are not part of the core langchain installation, you will need these additional\ndependencies. The model flavor contains two options for declaring these dependencies: extra_pip_requirements and pip_requirements . While specifying pip_requirements is entirely valid, we recommend using extra_pip_requirements as it does not rely on defining all of the core dependent packages that\nare required to use the langchain model for inference (the other core dependencies will be inferred automatically).  How can I use a streaming API with LangChain? Streaming with LangChain Models : Ensure that the LangChain model supports a streaming response and use an MLflow version >= 2.12.2. As of the MLflow 2.12.2 release, LangChain models that support streaming responses that have been saved using MLflow 2.12.2 (or higher) can be loaded and used for\nstreamable inference using the predict_stream API. Ensure that you are consuming the return type correctly, as the return from these models is a Generator object.\nTo learn more, refer to the predict_stream guide .  How can I log an agent built with LangGraph to MLflow? The LangGraph integration with MLflow is designed to utilize the Models From Code feature in MLflow to broaden and simplify the support of agent serialization. To log a LangGraph agent, you can define your agent code within a script, as shown below, saved to a file langgraph.py : from typing import Literal from langchain_core.tools import tool from langchain_openai import ChatOpenAI from langgraph.prebuilt import create_react_agent import mlflow @tool def get_weather ( city : Literal [ \"seattle\" , \"sf\" ]): \"\"\"Use this to get weather information.\"\"\" if city == \"seattle\" : return \"It's probably raining. Again.\" elif city == \"sf\" : return \"It's always sunny in sf\" llm = ChatOpenAI () tools = [ get_weather ] graph = create_react_agent ( llm , tools ) # specify the Agent as the model interface to be loaded when executing the script mlflow . models . set_model ( graph ) When you\u00e2\u0080\u0099re ready to log this agent script definition to MLflow, you can refer to\nthis saved script directly when defining the model: import mlflow input_example = { \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"what is the weather in seattle today?\" }] } with mlflow . start_run (): model_info = mlflow . langchain . log_model ( lc_model = \"./langgraph.py\" , # specify the path to the LangGraph agent script definition artifact_path = \"langgraph\" , input_example = input_example , ) When the agent is loaded from MLflow, the script will be executed and the defined agent will be\nmade available for use for invocation. The agent can be loaded and used for inference as follows: agent = mlflow . langchain . load_model ( model_info . model_uri ) query = { \"messages\" : [ { \"role\" : \"user\" , \"content\" : \"Should I bring an umbrella today when I go to work in San Francisco?\" , } ] } agent . invoke ( query ) ",
        "id": "05f4f60144480fcf8a902016a548c7d8"
    },
    {
        "text": " How can I evaluate a LangGraph Agent? The mlflow.evaluate function provides\na robust way to evaluate model performance. LangGraph agents, especially those with chat functionality, can return multiple messages in one\ninference call. Given mlflow.evaluate performs naive comparisons between raw predictions and a specified\nground truth value, it is the user\u00e2\u0080\u0099s responsibility to reconcile potential differences prediction output\nand ground truth. Often, the best approach is to use a custom function to process the response. Below we provide an example of a custom function that extracts the last chat\nmessage from a LangGraph model. This function is then used in mlflow.evaluate to return a single\nstring response, which can be compared to the \u00e2\u0080\u009cground_truth\u00e2\u0080\u009d column. import mlflow import pandas as pd from typing import List # Note that we assume the `model_uri` variable is present # Also note that registering and loading the model is optional and you # can simply leverage your langgraph object in the custom function. loaded_model = mlflow . langchain . load_model ( model_uri ) eval_data = pd . DataFrame ( { \"inputs\" : [ \"What is MLflow?\" , \"What is Spark?\" , ], \"ground_truth\" : [ \"MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.\" , \"Apache Spark is an open-source, distributed computing system designed for big data processing and analytics. It was developed in response to limitations of the Hadoop MapReduce computing model, offering improvements in speed and ease of use. Spark provides libraries for various tasks such as data ingestion, processing, and analysis through its components like Spark SQL for structured data, Spark Streaming for real-time data processing, and MLlib for machine learning tasks\" , ], } ) def custom_langgraph_wrapper ( inputs : pd . DataFrame ) -> List [ str ]: \"\"\"Extract the predictions from a chat message sequence.\"\"\" answers = [] for content in inputs [ \"inputs\" ]: prediction = loaded_model . invoke ( { \"messages\" : [{ \"role\" : \"user\" , \"content\" : content }]} ) last_message_content = prediction [ \"messages\" ][ - 1 ] . content answers . append ( last_message_content ) return answers with mlflow . start_run () as run : results = mlflow . evaluate ( custom_langgraph_wrapper , # Pass our function defined above data = eval_data , targets = \"ground_truth\" , model_type = \"question-answering\" , extra_metrics = [ mlflow . metrics . latency (), mlflow . metrics . genai . answer_correctness ( \"openai:/gpt-4o\" ), ], ) print ( results . metrics )  Output { \"latency/mean\" : 1.8976624011993408 , \"latency/variance\" : 0.10328687906900313 , \"latency/p90\" : 2.1547686100006103 , \"flesch_kincaid_grade_level/v1/mean\" : 12.1 , \"flesch_kincaid_grade_level/v1/variance\" : 0.25 , \"flesch_kincaid_grade_level/v1/p90\" : 12.5 , \"ari_grade_level/v1/mean\" : 15.850000000000001 , \"ari_grade_level/v1/variance\" : 0.06250000000000044 , \"ari_grade_level/v1/p90\" : 16.05 , \"exact_match/v1\" : 0.0 , \"answer_correctness/v1/mean\" : 5.0 , \"answer_correctness/v1/variance\" : 0.0 , \"answer_correctness/v1/p90\" : 5.0 , } For a complete example of a LangGraph model that works with this evaluation example, see the MLflow LangGraph blog . ",
        "id": "1eae3f7bb74ca0e49fd9e4a512904cfb"
    },
    {
        "text": " How to control whether my input is converted to List[langchain.schema.BaseMessage] in PyFunc predict? By default, MLflow converts chat request format input {\"messages\": [{\"role\": \"user\", \"content\": \"some_question\"}]} to\nList[langchain.schema.BaseMessage] like [HumanMessage(content=\"some_question\")] for certain model types.\nTo force the conversion, set the environment variable MLFLOW_CONVERT_MESSAGES_DICT_FOR_LANGCHAIN to True .\nTo disable this behavior, set the environment variable MLFLOW_CONVERT_MESSAGES_DICT_FOR_LANGCHAIN to False as demonstrated below: import json import mlflow import os from operator import itemgetter from langchain.schema.runnable import RunnablePassthrough model = RunnablePassthrough . assign ( problem = lambda x : x [ \"messages\" ][ - 1 ][ \"content\" ] ) | itemgetter ( \"problem\" ) input_example = { \"messages\" : [ { \"role\" : \"user\" , \"content\" : \"Hello\" , } ] } # this model accepts the input_example assert model . invoke ( input_example ) == \"Hello\" # set this environment variable to avoid input conversion os . environ [ \"MLFLOW_CONVERT_MESSAGES_DICT_FOR_LANGCHAIN\" ] = \"false\" with mlflow . start_run (): model_info = mlflow . langchain . log_model ( model , \"model\" , input_example = input_example ) pyfunc_model = mlflow . pyfunc . load_model ( model_info . model_uri ) assert pyfunc_model . predict ( input_example ) == [ \"Hello\" ] Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "c2019649835739300485775a54a6e371"
    },
    {
        "text": "MLflow LlamaIndex Flavor 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor MLflow OpenAI Flavor MLflow Sentence-Transformers Flavor MLflow LangChain Flavor MLflow LlamaIndex Flavor Introduction Why use LlamaIndex with MLflow? Getting Started Concepts Usage FAQ MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow LlamaIndex Flavor  MLflow LlamaIndex Flavor Attention The llama_index flavor is under active development and is marked as Experimental. Public APIs are\nsubject to change and new features may be added as the flavor evolves.  Introduction LlamaIndex \u00f0\u009f\u00a6\u0099 is a powerful data-centric framework designed to seamlessly connect custom data sources to large language models (LLMs).\nIt offers a comprehensive suite of data structures and tools that simplify the process of ingesting, structuring, and\naccessing private or domain-specific data for use with LLMs. LlamaIndex excels in enabling context-aware AI applications\nby providing efficient indexing and retrieval mechanisms, making it easier to build advanced QA systems, chatbots,\nand other AI-driven applications that require integration of external knowledge.  Why use LlamaIndex with MLflow? The integration of the LlamaIndex library with MLflow provides a seamless experience for managing and deploying LlamaIndex engines. The following are some of the key benefits of using LlamaIndex with MLflow: MLflow Tracking allows you to track your indices within MLflow and manage the many moving parts that comprise your LlamaIndex project, such as prompts, LLMs, workflows, tools, global configurations, and more. MLflow Model packages your LlamaIndex index/engine/workflows with all its dependency versions, input and output interfaces, and other essential metadata. This allows you to deploy your LlamaIndex models for inference with ease, knowing that the environment is consistent across different stages of the ML lifecycle. MLflow Evaluate provides native capabilities within MLflow to evaluate GenAI applications. This capability facilitates the efficient assessment of inference results from your LlamaIndex models, ensuring robust performance analytics and facilitating quick iterations. MLflow Tracing is a powerful observability tool for monitoring and debugging what happens inside the LlamaIndex models, helping you identify potential bottlenecks or issues quickly. With its powerful automatic logging capability, you can instrument your LlamaIndex application without needing to add any code apart from running a single command.  Getting Started In these introductory tutorials, you will learn the most fundamental components of LlamaIndex and how to leverage the integration with MLflow to bring better maintainability and observability to your LlamaIndex applications. LlamaIndex Workflows with MLflow Get started with MLflow and LLamaIndex by building a simple agentic Workflow. Learn how to log and load the Workflow for inference, as well as enable tracing for observability. Building Index with MLflow Get started with MLflow and LlamaIndex by exploring the simplest possible index configuration of a VectorStoreIndex.  Concepts Note Workflow integration is only available in LlamaIndex >= 0.11.0 and MLflow >= 2.17.0. ",
        "id": "154c975994d403aef631cf593fd1c919"
    },
    {
        "text": " Workflow \u00f0\u009f\u0086\u0095 The Workflow is LlamaIndex\u00e2\u0080\u0099s event-driven orchestration framework. It is designed\nas a flexible and interpretable framework for building arbitrary LLM applications such as an agent, a RAG flow, a data extraction pipeline, etc.\nMLflow supports tracking, evaluating, and tracing the Workflow objects, which makes them more observable and maintainable.  Index The Index object is a collection of documents that are indexed for fast information retrieval, providing capabilities for applications such as Retrieval-Augmented Generation (RAG) and Agents. The Index object can be logged directly to an MLflow run and loaded back for use as an inference engine.  Engine The Engine is a generic interface built on top of the Index object, which provides a set of APIs to interact with the index. LlamaIndex provides two types of engines: QueryEngine and ChatEngine . The QueryEngine simply takes a single\nquery and returns a response based on the index. The ChatEngine is designed for conversational agents, which keeps track of the conversation history as well.  Settings The Settings object is a global service context that bundles commonly used resources throughout the\nLlamaIndex application. It includes settings such as the LLM model, embedding model, callbacks, and more. When logging a LlamaIndex index/engine/workflow, MLflow tracks\nthe state of the Settings object so that you can easily reproduce the same result when loading the model back for inference (note that some objects like API keys, non-serializable objects, etc., are not tracked).  Usage  Saving and Loading Index in MLflow Experiment  Creating an Index The index object is the centerpiece of the LlamaIndex and MLflow integration. With LlamaIndex, you can create an index from a collection of documents or external vector stores. The following code creates a sample index from Paul Graham\u00e2\u0080\u0099s essay data available within the LlamaIndex repository. mkdir -p data\ncurl -L https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt -o ./data/paul_graham_essay.txt from llama_index.core import VectorStoreIndex , SimpleDirectoryReader documents = SimpleDirectoryReader ( \"data\" ) . load_data () index = VectorStoreIndex . from_documents ( documents )  Logging the Index to MLflow You can log the index object to the MLflow experiment using the mlflow.llama_index.log_model() function. One key step here is to specify the engine_type parameter. The choice of engine type does not affect the index itself,\nbut dictates the interface of how you query the index when you load it back for inference. QueryEngine ( engine_type=\"query\" ) is designed for a simple query-response system that takes a single query string and returns a response. ChatEngine ( engine_type=\"chat\" ) is designed for a conversational agent that keeps track of the conversation history and responds to a user message. Retriever ( engine_type=\"retriever\" ) is a lower-level component that returns the top-k relevant documents matching the query. The following code is an example of logging an index to MLflow with the chat engine type. import mlflow mlflow . set_experiment ( \"llama-index-demo\" ) with mlflow . start_run (): model_info = mlflow . llama_index . log_model ( index , artifact_path = \"index\" , engine_type = \"chat\" , input_example = \"What did the author do growing up?\" , ) Note The above code snippet passes the index object directly to the log_model function.\nThis method only works with the default SimpleVectorStore vector store, which\nsimply keeps the embedded documents in memory. If your index uses external vector stores such as QdrantVectorStore or DatabricksVectorSearch , you can use the Model-from-Code\nlogging method. See the How to log an index with external vector stores for more details. Tip Under the hood, MLflow calls as_query_engine() / as_chat_engine() / as_retriever() method on the index object to convert it to the respective engine instance. ",
        "id": "84638c0c7dc6c4bd7ea81336dccb5143"
    },
    {
        "text": " Loading the Index Back for inference The saved index can be loaded back for inference using the mlflow.pyfunc.load_model() function. This function\ngives an MLflow Python Model backed by the LlamaIndex engine, with the engine type specified during logging. import mlflow model = mlflow . pyfunc . load_model ( model_info . model_uri ) response = model . predict ( \"What was the first program the author wrote?\" ) print ( response ) # >> The first program the author wrote was on the IBM 1401 ... # The chat engine keeps track of the conversation history response = model . predict ( \"How did the author feel about it?\" ) print ( response ) # >> The author felt puzzled by the first program ... Tip To load the index itself back instead of the engine, use the mlflow.llama_index.load_model() function. index = mlflow . llama_index . load_model ( \"runs:/<run_id>/index\" )  Enable Tracing You can enable tracing for your LlamaIndex code by calling the mlflow.llama_index.autolog() function. MLflow automatically logs the input and output of the LlamaIndex execution to the active MLflow experiment, providing you with a detailed view of the model\u00e2\u0080\u0099s behavior. import mlflow mlflow . llama_index . autolog () chat_engine = index . as_chat_engine () response = chat_engine . chat ( \"What was the first program the author wrote?\" ) Then you can navigate to the MLflow UI, select the experiment, and open the \u00e2\u0080\u009cTraces\u00e2\u0080\u009d tab to find the logged trace for the prediction made by the engine. It is impressive to see how the chat engine coordinates and executes a number of tasks to answer your question! You can disable tracing by running the same function with the disable parameter set to True : mlflow . llama_index . autolog ( disable = True ) Note The tracing supports async prediction and streaming response, however, it does not\nsupport the combination of async and streaming, such as the astream_chat method.  FAQ ",
        "id": "ba099b9f8e710d44de0394b0ef5d30e0"
    },
    {
        "text": " How to log and load an index with external vector stores? If your index uses the default SimpleVectorStore , you can log the index directly to MLflow using the mlflow.llama_index.log_model() function. MLflow persists the in-memory index data (embedded documents) to MLflow artifact store, which allows loading the index back with the same data without re-indexing the documents. However, when the index uses external vector stores like DatabricksVectorSearch and QdrantVectorStore , the index data is stored remotely and they do not support local serialization. Thereby, you cannot log the index with these stores directly. For such cases, you can use the Model-from-Code logging that provides more control over the index saving process and allow you to use the external vector store. To use model-from-code logging, you first need to create a separate Python file that defines the index. If you are on Jupyter notebook, you can use the %%writefile magic command to save the cell code to a Python file. %% writefile index . py # Create Qdrant client with your own settings. client = qdrant_client . QdrantClient ( host = \"localhost\" , port = 6333 , ) # Here we simply load vector store from the existing collection to avoid # re-indexing documents, because this Python file is executed every time # when the model is loaded. If you don't have an existing collection, create # a new one by following the official tutorial: # https://docs.llamaindex.ai/en/stable/examples/vector_stores/QdrantIndexDemo/ vector_store = QdrantVectorStore ( client = client , collection_name = \"my_collection\" ) index = VectorStoreIndex . from_vector_store ( vector_store = vector_store ) # IMPORTANT: call set_model() method to tell MLflow to log this index mlflow . models . set_model ( index ) Then you can log the index by passing the Python file path to the mlflow.llama_index.log_model() function. The global Settings object is saved normally as part of the model. import mlflow with mlflow . start_run (): model_info = mlflow . llama_index . log_model ( \"index.py\" , artifact_path = \"index\" , engine_type = \"query\" , ) The logged index can be loaded back using the mlflow.llama_index.load_model() or mlflow.pyfunc.load_model() function, in the same way as with the local index. index = mlflow . llama_index . load_model ( model_info . model_uri ) index . as_query_engine () . query ( \"What is MLflow?\" ) Note The object that is passed to the set_model() method must be a LlamaIndex index that is compatible with the engine type specified during logging. More\nobjects support will be added in the future releases.  How to log and load a LlamaIndex Workflow? Mlflow supports logging and loading a LlamaIndex Workflow via the Model-from-Code feature. For a detailed example of logging and loading a LlamaIndex Workflow, see the LlamaIndex Workflows with MLflow notebook. import mlflow with mlflow . start_run (): model_info = mlflow . llama_index . log_model ( \"/path/to/workflow.py\" , artifact_path = \"model\" , input_example = { \"input\" : \"What is MLflow?\" }, ) The logged workflow can be loaded back using the mlflow.llama_index.load_model() or mlflow.pyfunc.load_model() function. # Use mlflow.llama_index.load_model to load the workflow object as is workflow = mlflow . llama_index . load_model ( model_info . model_uri ) await workflow . run ( input = \"What is MLflow?\" ) # Use mlflow.pyfunc.load_model to load the workflow as a MLflow Pyfunc Model # with standard inference APIs for deployment and evaluation. pyfunc = mlflow . pyfunc . load_model ( model_info . model_uri ) pyfunc . predict ({ \"input\" : \"What is MLflow?\" }) Warning The MLflow PyFunc Model does not support async inference. When you load the workflow with mlflow.pyfunc.load_model() , the predict method becomes synchronous and will block until the workflow execution is completed. This also applies when deploying the logged LlamaIndex workflow to a production endpoint using MLflow Deployment or Databricks Model Serving . ",
        "id": "ac14160db58462d451b07bff1e664b72"
    },
    {
        "text": " I have an index logged with query engine type. Can I load it back a chat engine? While it is not possible to update the engine type of the logged model in-place,\nyou can always load the index back and re-log it with the desired engine type. This process\ndoes not require re-creating the index , so it is an efficient way to switch between\ndifferent engine types. import mlflow # Log the index with the query engine type first with mlflow . start_run (): model_info = mlflow . llama_index . log_model ( index , artifact_path = \"index-query\" , engine_type = \"query\" , ) # Load the index back and re-log it with the chat engine type index = mlflow . llama_index . load_model ( model_info . model_uri ) with mlflow . start_run (): model_info = mlflow . llama_index . log_model ( index , artifact_path = \"index-chat\" , # Specify the chat engine type this time engine_type = \"chat\" , ) Alternatively, you can leverage their standard inference APIs on the loaded LlamaIndex native index object, specifically: index.as_chat_engine().chat(\"hi\") index.as_query_engine().query(\"hi\") index.as_retriever().retrieve(\"hi\")  How to use different LLMs for inference with the loaded engine? When saving the index to MLflow, it persists the global Settings object as a part of the model. This object contains settings such as LLM and embedding\nmodels to be used by engines. import mlflow from llama_index.core import Settings from llama_index.llms.openai import OpenAI Settings . llm = OpenAI ( \"gpt-4o-mini\" ) # MLflow saves GPT-4o-Mini as the LLM to use for inference with mlflow . start_run (): model_info = mlflow . llama_index . log_model ( index , artifact_path = \"index\" , engine_type = \"chat\" ) Then later when you load the index back, the persisted settings are also applied globally. This means that the loaded engine will use the same LLM as when it was logged. However, sometimes you may want to use a different LLM for inference. In such cases, you can update the global Settings object directly after loading the index. import mlflow # Load the index back loaded_index = mlflow . llama_index . load_model ( model_info . model_uri ) assert Settings . llm . model == \"gpt-4o-mini\" # Update the settings to use GPT-4 instead Settings . llm = OpenAI ( \"gpt-4\" ) query_engine = loaded_index . as_query_engine () response = query_engine . query ( \"What is the capital of France?\" ) Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "86582904814ac82cde48b5d37ba92b0e"
    },
    {
        "text": "MLflow DSPy Flavor 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor MLflow OpenAI Flavor MLflow Sentence-Transformers Flavor MLflow LangChain Flavor MLflow LlamaIndex Flavor MLflow DSPy Flavor Introduction Why use DSPy with MLflow? Getting Started Concepts Usage FAQ Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow DSPy Flavor  MLflow DSPy Flavor Attention The dspy flavor is under active development and is marked as Experimental. Public APIs are\nsubject to change and new features may be added as the flavor evolves.  Introduction DSPy is a framework for algorithmically optimizing LM prompts and weights. It\u00e2\u0080\u0099s designed to\nimprove the process of prompt engineering by replacing hand-crafted prompt strings with modular\ncomponents. These modules are concise, well-defined, and maintain high quality and expressive power,\nmaking prompt creation more efficient and scalable. By parameterizing these modules and treating\nprompting as an optimization problem, DSPy can adapt better to different language models,\npotentially outperforming prompts crafted by experts. This modularity also enables easier\nexploration of complex pipelines, allowing for fine-tuning performance based on specific tasks or\nnuanced metrics.  Why use DSPy with MLflow? The native integration of the DSPy library with MLflow helps users manage the development lifecycle with DSPy. The following are some of the key benefits of using DSPy with MLflow: MLflow Tracking allows you to track your DSPy program\u00e2\u0080\u0099s training and execution. With the MLflow APIs, you can log a variety of artifacts and organize training runs, thereby increasing visibility into your model performance. MLflow Model packages your compiled DSPy program along with its dependency versions, input and output interfaces and other essential metadata. This allows you to deploy your compiled DSPy program with ease, knowing that the environment is consistent across different stages of the ML lifecycle. MLflow Evaluate provides native capabilities within MLflow to evaluate GenAI applications. This capability facilitates the efficient assessment of inference results from your DSPy compiled program, ensuring robust performance analytics and facilitating quick iterations. MLflow Tracing is a powerful observability tool for monitoring and debugging what happens inside the DSPy models, helping you identify potential bottlenecks or issues quickly. With its powerful automatic logging capability, you can instrument your DSPy application without needing to add any code apart from running a single command.  Getting Started In this introductory tutorial, you will learn the most fundamental components of DSPy and how to leverage the integration with MLflow to store, retrieve, and\nuse a DSPy program. DSPy Quickstart Get started with MLflow and DSPy by exploring the simplest possible configuration of a DSPy program.  Concepts  Module Modules are components that handle specific text transformations, like answering questions or summarizing. They replace traditional hand-written prompts and can learn from examples, making them more adaptable.  Signature A signature is a natural language description of a module\u00e2\u0080\u0099s input and output behavior. For example, \u00e2\u0080\u009cquestion -> answer\u00e2\u0080\u009d specifies that the module should take a question as input and return an answer. ",
        "id": "214deb577e77caef16bfd508418b3f80"
    },
    {
        "text": " Optimizer A optimizer improves LM pipelines by adjusting modules to meet a performance metric, either by generating better prompts or fine-tuning models.  Program A program is a a set of modules connected into a pipeline to perform complex tasks. DSPy programs are flexible, allowing you to optimize and adapt them using the compiler.  Usage  Saving and Loading DSPy Program in MLflow Experiment  Creating a DSPy Program The Module object is the centerpiece of\nthe DSPy and MLflow integration. With DSPy, you can create complex agentic logic via a module or\nset of modules. pip install mlflow dspy -U import dspy # Define our language model lm = dspy . LM ( model = \"openai/gpt-4o-mini\" , max_tokens = 250 ) dspy . settings . configure ( lm = lm ) # Define a Chain of Thought module class CoT ( dspy . Module ): def __init__ ( self ): super () . __init__ () self . prog = dspy . ChainOfThought ( \"question -> answer\" ) def forward ( self , question ): return self . prog ( question = question ) dspy_model = CoT () Tip Typically you\u00e2\u0080\u0099d want to leverage a compiled DSPy module. MLflow will natively supports logging both compiled and uncompiled DSPy modules. Above\nwe show an uncompiled version for simplicity, but in production you\u00e2\u0080\u0099d want to leverage an optimizer and log the\noutputted object instead.  Logging the Program to MLflow You can log the dspy.Module object to an MLflow run using the mlflow.dspy.log_model() function. We will also specify a model signature .\nAn MLflow model signature defines the expected schema for model inputs and outputs, ensuring\nconsistency and correctness during model inference. import mlflow # Start an MLflow run with mlflow . start_run (): # Log the model model_info = mlflow . dspy . log_model ( dspy_model , artifact_path = \"model\" , input_example = \"what is 2 + 2?\" , )  Loading the Module for inference The saved module can be loaded back for inference using the mlflow.pyfunc.load_model() function. This function\ngives an MLflow Python Model backed by the DSPy module. import mlflow # Load the model as an MLflow PythonModel model = mlflow . pyfunc . load_model ( model_info . model_uri ) # Predict with the object response = model . predict ( \"What kind of bear is best?\" ) print ( response )  Output { 'reasoning' : '''The question \"What kind of bear is best?\" is often associated with a humorous reference from the television show \"The Office,\" where the character Jim Halpert jokingly states, \"Bears, beets, Battlestar Galactica.\" However, if we consider the question seriously, it depends on the context. Different species of bears have different characteristics and adaptations that make them \"best\" in various ways. For example, the American black bear is known for its adaptability, while the polar bear is the largest land carnivore and is well adapted to its Arctic environment. Ultimately, the answer can vary based on personal preference or specific criteria such as strength, intelligence, or adaptability.''' , 'answer' : '''There isn \\' t a definitive answer, as it depends on the context. However, many people humorously refer to the American black bear or the polar bear when discussing \"the best\" kind of bear.''' } To load the DSPy program itself back instead of the PyFunc-wrapped model, use the mlflow.dspy.load_model() function. model = mlflow . dspy . load_model ( model_uri )  Enabling and Disabling Auto Tracing for DSPy Programs Auto tracing is a powerful feature that allows you to monitor and debug your DSPy programs. With MLflow, you can enable auto tracing just by calling the mlflow.dspy.autolog() function in your code. import mlflow mlflow . dspy . autolog () Once enabled, MLflow will generate traces whenever your DSPy program is executed and record them in your MLflow Experiment. You can disable auto-tracing for DSPy by calling mlflow.dspy.autolog(disabled=True) .  FAQ ",
        "id": "e866193eef8036a4788d33b01e0bcdb6"
    },
    {
        "text": " How can I save a compiled vs. uncompiled model? DSPy compiles models by updating various LLM parameters, such as prompts, hyperparameters, and\nmodel weights, to optimize training. While MLflow allows logging both compiled and uncompiled\nmodels, it\u00e2\u0080\u0099s generally preferable to use a compiled model, as it is expected to perform better in\npractice.  What can be serialized by MLflow? When using mlflow.dspy.log_model() or mlflow.dspy.save_model() in MLflow, the\nDSPy program is serialized and saved to the tracking server as a .pkl file. This enables easy\ndeployment. Under the hood, MLflow uses cloudpickle to serialize the DSPy object, but some\nDSPy artifacts are note serializable. Relevant examples are listed below. API tokens. These should be managed separately and passed securely via environment variables. The DSPy trace object, which is primarily used during training, not inference.  How do I manage secrets? When serializing using the MLflow DSPy flavor, tokens are dropped from the settings objects. It is\nthe user\u00e2\u0080\u0099s responsibility to securely pass the required secrets to the deployment environment.  How is the DSPy settings object saved? To ensure program reproducibility, the service context is converted to a Python dictionary and\npickled with the model artifact. Service context is a concept that has been popularized in GenAI\nframeworks. Put simply, it stores a configuration that is global to your project. For DSPy\nspecifically, we can set information such as the language model, reranker, adapter, etc. DSPy stores this service context in a Settings singleton class. Sensitive API access keys that\nare set within the Settings object are not persisted when logging your model. When deploying\nyour DSPy model, you must ensure that the deployment environment has these keys set so that your\nDSPy model can make remote calls to services that require access keys. Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "3a487930e7a3876639fc30d147c1d7c9"
    },
    {
        "text": "MLflow\u00e2\u0080\u0099s LLM Tracking Capabilities 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs LLM Tracking in MLflow MLflow\u00e2\u0080\u0099s LLM Tracking Capabilities Introduction to LLM Tracking Detailed Logging of LLM Interactions Structured Storage of LLM Tracking Data Benefits of the MLflow LLM Tracking System MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow\u00e2\u0080\u0099s LLM Tracking Capabilities   MLflow\u00e2\u0080\u0099s LLM Tracking Capabilities MLflow\u00e2\u0080\u0099s LLM Tracking system is an enhancement to the existing MLflow Tracking system, offerring additional capabilities for monitoring,\nmanaging, and interpreting interactions with Large Language Models (LLMs). At its core, MLflow\u00e2\u0080\u0099s LLM suite builds upon the standard logging capabilities familiar to professionals working with traditional\nMachine Learning (ML) and Deep Learning (DL). However, it introduces distinct features tailored for the unique intricacies of LLMs. One such standout feature is the introduction of \u00e2\u0080\u009cprompts\u00e2\u0080\u009d \u00e2\u0080\u0093 the queries or inputs directed towards an LLM \u00e2\u0080\u0093 and the subsequent data\nthe model generates in response. While MLflow\u00e2\u0080\u0099s offerings for other model types typically exclude built-in mechanisms for preserving\ninference results, LLMs necessitate this due to their dynamic and generative nature. Recognizing this, MLflow introduces the term\n\u00e2\u0080\u0098predictions\u00e2\u0080\u0099 alongside the existing tracking components of artifacts , parameters , tags , and metrics , ensuring comprehensive\nlineage and quality tracking for text-generating models. ",
        "id": "5ef7162695074b3d554b8044850cb46f"
    },
    {
        "text": "  Introduction to LLM Tracking The world of Large Language Models is vast, and as these models become more intricate and sophisticated, the need for a robust\ntracking system becomes paramount. MLflow\u00e2\u0080\u0099s LLM Tracking is centered around the concept of runs . In essence, a run is a\ndistinct execution or interaction with the LLM \u00e2\u0080\u0094 whether it\u00e2\u0080\u0099s a single query, a batch of prompts, or an entire fine-tuning session. Each run meticulously records: Parameters : Key-value pairs that detail the input parameters for the LLM. These could range from model-specific parameters like top_k and temperature to more generic ones. They provide context and configuration for each run. Parameters can be logged using both mlflow.log_param() for individual entries and mlflow.log_params() for bulk logging. Metrics : These are quantitative measures, often numeric, that give insights into the performance, accuracy, or any other measurable aspect of the LLM interaction. Metrics are dynamic and can be updated as the run progresses, offering a real-time or post-process insight into the model\u00e2\u0080\u0099s behavior. Logging of metrics is facilitated through mlflow.log_metric() and mlflow.log_metrics() . Predictions : To understand and evaluate LLM outputs, MLflow allows for the logging of predictions. This encompasses the prompts or inputs sent to the LLM and the outputs or responses received. For structured storage and easy retrieval, these predictions are stored as artifacts in CSV format, ensuring that each interaction is preserved in its entirety. This logging is achieved using the dedicated mlflow.log_table() . Artifacts : Beyond predictions, MLflow\u00e2\u0080\u0099s LLM Tracking can store a myriad of output files, ranging from visualization images (e.g., PNGs), serialized models (e.g., an openai model), to structured data files (e.g., a Parquet file). The mlflow.log_artifact() function is at the heart of this, allowing users to log and organize their artifacts with ease. Furthermore, to provide structured organization and comparative analysis capabilities, runs can be grouped into experiments .\nThese experiments act as containers, grouping related runs, and providing a higher level of organization. This organization ensures\nthat related runs can be compared, analyzed, and managed as a cohesive unit.   Detailed Logging of LLM Interactions MLflow\u00e2\u0080\u0099s LLM Tracking doesn\u00e2\u0080\u0099t just record data \u00e2\u0080\u0094 it offers structured logging mechanisms tailored to the needs of LLM interactions: Parameters : Logging parameters is straightforward. Whether you\u00e2\u0080\u0099re logging a single parameter using mlflow.log_param() or multiple parameters simultaneously with mlflow.log_params() , MLflow ensures that every detail is captured. Metrics : Quantitative insights are crucial. Whether it\u00e2\u0080\u0099s tracking the accuracy of a fine-tuned LLM or understanding its response time, metrics provide this insight. They can be logged individually via mlflow.log_metric() or in bulk using mlflow.log_metrics() . Predictions : Every interaction with an LLM yields a result \u00e2\u0080\u0094 a prediction. Capturing this prediction, along with the inputs that led to it, is crucial. The mlflow.log_table() function is specifically designed for this, ensuring that both inputs and outputs are logged cohesively. Artifacts : Artifacts act as the tangible outputs of an LLM run. They can be images, models, or any other form of data. Logging them is seamless with mlflow.log_artifact() , which ensures that every piece of data, regardless of its format, is stored and linked to its respective run.   Structured Storage of LLM Tracking Data Every piece of data, every parameter, metric, prediction, and artifact is not just logged \u00e2\u0080\u0094 it\u00e2\u0080\u0099s structured and stored as part of an\nMLflow Experiment run. This organization ensures data integrity, easy retrieval, and a structured approach to analyzing and understanding\nLLM interactions in the grand scheme of machine learning workflows. Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "85ac83c0095a952f9f8ebc2b762f7659"
    },
    {
        "text": "RAG Tutorials 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow Retrieval Augmented Generation (RAG) Benefits of RAG Understanding the Power of RAG Explore RAG Tutorials Deploying Advanced LLMs with Custom PyFuncs in MLflow LLM Evaluation Examples Tutorial: Getting Started with ChatModel Tutorial: Custom GenAI Models using ChatModel Build a tool-calling model with mlflow.pyfunc.ChatModel MLflow Trace UI in Jupyter Notebook Demo MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs Retrieval Augmented Generation (RAG) RAG Tutorials  RAG Tutorials You can find a list of tutorials for RAG below. These tutorials are designed to help you\nget started with RAG evaluation and walk you through a concrete example of how to evaluate\na RAG application that answers questions about MLflow documentation.  End-to-End LLM RAG Evaluation Tutorial This notebook, intended for use with the Databricks platform, showcases a full end-to-end example of how to configure, create, and interface with\na full RAG system. The example used in this tutorial uses the documentation of MLflow as the corpus of embedded documents that the RAG application will\nuse to answer questions. Using ChromaDB to store the document embeddings and LangChain to orchestrate the RAG application, we\u00e2\u0080\u0099ll use MLflow\u00e2\u0080\u0099s evaluate functionality to evaluate the retrieved documents from our corpus based on a series of questions. You can click \u00e2\u0080\u009cDownload this Notebook\u00e2\u0080\u009d button to\ndownload the .ipynb file locally and import it directly in the Databricks Workspace. End-to-End RAG Evaluation with MLflow Comprehensive tutorial on evaluating Retrieval-Augmented Generation (RAG) systems using MLflow  Question Generation for RAG Tutorial This notebook is a step-by-step tutorial on how to generate a question dataset with\nLLMs for retrieval evaluation within RAG. It will guide you through getting a document dataset,\ngenerating relevant questions through prompt engineering on LLMs, and analyzing the\nquestion dataset. The question dataset can then be used for the subsequent task of evaluating the\nretriever model, which is a part of RAG that collects and ranks relevant document chunks based on\nthe user\u00e2\u0080\u0099s question. Question Generation for RAG Evaluation Step-by-step demonstration for how to automatically generate a question-answering dataset for RAG evaluation  Retriever Evaluation Tutorial This tutorial walks you through a concrete example of how to build and evaluate\na RAG application that answers questions about MLflow documentation. In this tutorial you will learn: How to prepare an evaluation dataset for your RAG application. How to call your retriever in the MLflow evaluate API. How to evaluate a retriever\u00e2\u0080\u0099s capacity for retrieving relevant documents based on a series of queries using MLflow evaluate. If you would like a copy of this notebook to execute in your environment, download the notebook here: Retriever Evaluation with MLflow Learn how to leverage MLflow to evaluate the performance of a retriever in a RAG application,\n                    leveraging built-in retriever metrics precision_at_k and recall_at_k . Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "77e7a75dd83de5b650dbe890d4344a7f"
    },
    {
        "text": "Serving LLMs with MLflow: Leveraging Custom PyFunc 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow Retrieval Augmented Generation (RAG) Deploying Advanced LLMs with Custom PyFuncs in MLflow Explore the Tutorial LLM Evaluation Examples Tutorial: Getting Started with ChatModel Tutorial: Custom GenAI Models using ChatModel Build a tool-calling model with mlflow.pyfunc.ChatModel MLflow Trace UI in Jupyter Notebook Demo MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs Deploying Advanced LLMs with Custom PyFuncs in MLflow Custom PyFuncs for Advanced LLMs with MLflow - Notebooks Serving LLMs with MLflow: Leveraging Custom PyFunc  Serving LLMs with MLflow: Leveraging Custom PyFunc Download this Notebook  Introduction This tutorial guides you through saving and deploying Large Language Models (LLMs) using a custom pyfunc with MLflow, ideal for models not directly supported by MLflow\u00e2\u0080\u0099s default transformers flavor.  Learning Objectives Understand the need for custom pyfunc definitions in specific model scenarios. Learn to create a custom pyfunc to manage model dependencies and interface data. Gain insights into simplifying user interfaces in deployed environments with custom pyfunc .  The Challenge with Default Implementations While MLflow\u00e2\u0080\u0099s transformers flavor generally handles models from the HuggingFace Transformers library, some models or configurations might not align with this standard approach. In such cases, like ours, where the model cannot utilize the default pipeline type, we face a unique challenge of deploying these models using MLflow.  The Power of Custom PyFunc To address this, MLflow\u00e2\u0080\u0099s custom pyfunc comes to the rescue. It allows us to: Handle model loading and its dependencies efficiently. Customize the inference process to suit specific model requirements. Adapt interface data to create a user-friendly environment in deployed applications. Our focus will be on the practical application of a custom pyfunc to deploy LLMs effectively within MLflow\u00e2\u0080\u0099s ecosystem. By the end of this tutorial, you\u00e2\u0080\u0099ll be equipped with the knowledge to tackle similar challenges in your machine learning projects, leveraging the full potential of MLflow for custom model deployments.  Important Considerations Before Proceeding  Hardware Recommendations This guide demonstrates the usage of a particularly large and intricate Large Language Model (LLM). Given its complexity: GPU Requirement : It\u00e2\u0080\u0099s strongly advised to run this example on a system with a CUDA-capable GPU that possesses at least 64GB of VRAM. CPU Caution : While technically feasible, executing the model on a CPU can result in extremely prolonged inference times, potentially taking tens of minutes for a single prediction, even on top-tier CPUs. The final cell of this notebook is deliberately not executed due to the limitations with performance when running this model on a CPU-only system. However, with an appropriately powerful GPU, the total runtime of this notebook is ~8 minutes end to end. ",
        "id": "07b5f3b75e449f96860bd4c521250ab1"
    },
    {
        "text": " Execution Recommendations If you\u00e2\u0080\u0099re considering running the code in this notebook: Performance : For a smoother experience and to truly harness the model\u00e2\u0080\u0099s capabilities, use hardware aligned with the model\u00e2\u0080\u0099s design. Dependencies : Ensure you\u00e2\u0080\u0099ve installed the recommended dependencies for optimal model performance. These are crucial for efficient model loading, initialization, attention computations, and inference processing: pip install xformers == 0 .0.20 einops == 0 .6.1 flash-attn == v1.0.3.post0 triton-pre-mlir@git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory = python [1]: # Load necessary libraries import accelerate import torch import transformers from huggingface_hub import snapshot_download import mlflow /Users/benjamin.wilson/miniconda3/envs/mlflow-dev-env/lib/python3.8/site-packages/pydantic/_internal/_fields.py:128: UserWarning: Field \"model_server_url\" has conflict with protected namespace \"model_\".\n\nYou may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n  warnings.warn(\n/Users/benjamin.wilson/miniconda3/envs/mlflow-dev-env/lib/python3.8/site-packages/pydantic/_internal/_config.py:317: UserWarning: Valid config keys have changed in V2:\n* 'schema_extra' has been renamed to 'json_schema_extra'\n  warnings.warn(message, UserWarning)  Downloading the Model and Tokenizer First, we need to download our model and tokenizer. Here\u00e2\u0080\u0099s how we do it: [2]: # Download the MPT-7B instruct model and tokenizer to a local directory cache snapshot_location = snapshot_download ( repo_id = \"mosaicml/mpt-7b-instruct\" , local_dir = \"mpt-7b\" ) ",
        "id": "e19deb39d3cd50c34244c456c74b4cd0"
    },
    {
        "text": " Defining the Custom PyFunc Now, let\u00e2\u0080\u0099s define our custom pyfunc . This will dictate how our model loads its dependencies and how it performs predictions. Notice how we\u00e2\u0080\u0099ve wrapped the intricacies of the model within this class. [3]: class MPT ( mlflow . pyfunc . PythonModel ): def load_context ( self , context ): \"\"\" This method initializes the tokenizer and language model using the specified model snapshot directory. \"\"\" # Initialize tokenizer and language model self . tokenizer = transformers . AutoTokenizer . from_pretrained ( context . artifacts [ \"snapshot\" ], padding_side = \"left\" ) config = transformers . AutoConfig . from_pretrained ( context . artifacts [ \"snapshot\" ], trust_remote_code = True ) # If you are running this in a system that has a sufficiently powerful GPU with available VRAM, # uncomment the configuration setting below to leverage triton. # Note that triton dramatically improves the inference speed performance # config.attn_config[\"attn_impl\"] = \"triton\" self . model = transformers . AutoModelForCausalLM . from_pretrained ( context . artifacts [ \"snapshot\" ], config = config , torch_dtype = torch . bfloat16 , trust_remote_code = True , ) # NB: If you do not have a CUDA-capable device or have torch installed with CUDA support # this setting will not function correctly. Setting device to 'cpu' is valid, but # the performance will be very slow. self . model . to ( device = \"cpu\" ) # If running on a GPU-compatible environment, uncomment the following line: # self.model.to(device=\"cuda\") self . model . eval () def _build_prompt ( self , instruction ): \"\"\" This method generates the prompt for the model. \"\"\" INSTRUCTION_KEY = \"### Instruction:\" RESPONSE_KEY = \"### Response:\" INTRO_BLURB = ( \"Below is an instruction that describes a task. \" \"Write a response that appropriately completes the request.\" ) return f \"\"\" { INTRO_BLURB } { INSTRUCTION_KEY } { instruction } { RESPONSE_KEY } \"\"\" def predict ( self , context , model_input , params = None ): \"\"\" This method generates prediction for the given input. \"\"\" prompt = model_input [ \"prompt\" ][ 0 ] # Retrieve or use default values for temperature and max_tokens temperature = params . get ( \"temperature\" , 0.1 ) if params else 0.1 max_tokens = params . get ( \"max_tokens\" , 1000 ) if params else 1000 # Build the prompt prompt = self . _build_prompt ( prompt ) # Encode the input and generate prediction # NB: Sending the tokenized inputs to the GPU here explicitly will not work if your system does not have CUDA support. # If attempting to run this with GPU support, change 'cpu' to 'cuda' for maximum performance encoded_input = self . tokenizer . encode ( prompt , return_tensors = \"pt\" ) . to ( \"cpu\" ) output = self . model . generate ( encoded_input , do_sample = True , temperature = temperature , max_new_tokens = max_tokens , ) # Removing the prompt from the generated text prompt_length = len ( self . tokenizer . encode ( prompt , return_tensors = \"pt\" )[ 0 ]) generated_response = self . tokenizer . decode ( output [ 0 ][ prompt_length :], skip_special_tokens = True ) return { \"candidates\" : [ generated_response ]} ",
        "id": "1e3d05832d6878f2621cf1d21fe77fcb"
    },
    {
        "text": " Building the Prompt One key aspect of our custom pyfunc is the construction of a model prompt. Instead of the end-user having to understand and construct this prompt, our custom pyfunc takes care of it. This ensures that regardless of the intricacies of the model\u00e2\u0080\u0099s requirements, the end-user interface remains simple and consistent. Review the method _build_prompt() inside our class above to see how custom input processing logic can be added to a custom pyfunc to support required translations of user-input data into a format that is compatible with the wrapped model instance. [4]: import numpy as np import pandas as pd import mlflow from mlflow.models.signature import ModelSignature from mlflow.types import ColSpec , DataType , ParamSchema , ParamSpec , Schema # Define input and output schema input_schema = Schema ( [ ColSpec ( DataType . string , \"prompt\" ), ] ) output_schema = Schema ([ ColSpec ( DataType . string , \"candidates\" )]) parameters = ParamSchema ( [ ParamSpec ( \"temperature\" , DataType . float , np . float32 ( 0.1 ), None ), ParamSpec ( \"max_tokens\" , DataType . integer , np . int32 ( 1000 ), None ), ] ) signature = ModelSignature ( inputs = input_schema , outputs = output_schema , params = parameters ) # Define input example input_example = pd . DataFrame ({ \"prompt\" : [ \"What is machine learning?\" ]})  Set the experiment that we\u00e2\u0080\u0099re going to be logging our custom model to If the experiment doesn\u00e2\u0080\u0099t already exist, MLflow will create a new experiment with this name and will alert you that it has created a new experiment. [5]: # If you are running this tutorial in local mode, leave the next line commented out. # Otherwise, uncomment the following line and set your tracking uri to your local or remote tracking server. # mlflow.set_tracking_uri(\"http://127.0.0.1:8080\") mlflow . set_experiment ( experiment_name = \"mpt-7b-instruct-evaluation\" ) 2023/11/29 17:33:23 INFO mlflow.tracking.fluent: Experiment with name 'mpt-7b-instruct-evaluation' does not exist. Creating a new experiment. [5]: <Experiment: artifact_location='file:///Users/benjamin.wilson/repos/mlflow-fork/mlflow/docs/source/llms/custom-pyfunc-for-llms/notebooks/mlruns/265930820950682761', creation_time=1701297203895, experiment_id='265930820950682761', last_update_time=1701297203895, lifecycle_stage='active', name='mpt-7b-instruct-evaluation', tags={}> [6]: # Get the current base version of torch that is installed, without specific version modifiers torch_version = torch . __version__ . split ( \"+\" )[ 0 ] # Start an MLflow run context and log the MPT-7B model wrapper along with the param-included signature to # allow for overriding parameters at inference time with mlflow . start_run (): model_info = mlflow . pyfunc . log_model ( \"mpt-7b-instruct\" , python_model = MPT (), # NOTE: the artifacts dictionary mapping is critical! This dict is used by the load_context() method in our MPT() class. artifacts = { \"snapshot\" : snapshot_location }, pip_requirements = [ f \"torch== { torch_version } \" , f \"transformers== { transformers . __version__ } \" , f \"accelerate== { accelerate . __version__ } \" , \"einops\" , \"sentencepiece\" , ], input_example = input_example , signature = signature , ) 2023/11/29 17:33:24 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\n/Users/benjamin.wilson/miniconda3/envs/mlflow-dev-env/lib/python3.8/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")  Load the saved model [7]: loaded_model = mlflow . pyfunc . load_model ( model_info . model_uri ) /Users/benjamin.wilson/.cache/huggingface/modules/transformers_modules/mpt-7b/configuration_mpt.py:97: UserWarning: alibi is turned on, setting `learned_pos_emb` to `False.`\n  warnings.warn(f'alibi is turned on, setting `learned_pos_emb` to `False.`') ",
        "id": "f1ac85f7e06f44796ed0ee4e69e0eb4a"
    },
    {
        "text": " Test the model for inference [ ]: # The execution of this is commented out for the purposes of runtime on CPU. # If you are running this on a system with a sufficiently powerful GPU, you may uncomment and interface with the model! # loaded_model.predict(pd.DataFrame( #     {\"prompt\": [\"What is machine learning?\"]}), params={\"temperature\": 0.6} # )  Conclusion Through this tutorial, we\u00e2\u0080\u0099ve seen the power and flexibility of MLflow\u00e2\u0080\u0099s custom pyfunc . By understanding the specific needs of our model and defining a custom pyfunc to cater to those needs, we can ensure a seamless deployment process and a user-friendly interface. Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "95882b8c885448df642184204f855016"
    },
    {
        "text": "Retriever Evaluation with MLflow 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow Retrieval Augmented Generation (RAG) Benefits of RAG Understanding the Power of RAG Explore RAG Tutorials Deploying Advanced LLMs with Custom PyFuncs in MLflow LLM Evaluation Examples Tutorial: Getting Started with ChatModel Tutorial: Custom GenAI Models using ChatModel Build a tool-calling model with mlflow.pyfunc.ChatModel MLflow Trace UI in Jupyter Notebook Demo MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs Retrieval Augmented Generation (RAG) RAG Tutorials Retriever Evaluation with MLflow  Retriever Evaluation with MLflow Download this Notebook In MLflow 2.8.0, we introduced a new model type \u00e2\u0080\u009cretriever\u00e2\u0080\u009d to the mlflow.evaluate() API. It helps you to evaluate the retriever in a RAG application. It contains two built-in metrics precision_at_k and recall_at_k . In MLflow 2.9.0, ndcg_at_k is available. This notebook illustrates how to use mlflow.evaluate() to evaluate the retriever in a RAG application. It has the following steps: Step 1: Install and Load Packages Step 2: Evaluation Dataset Preparation Step 3: Calling mlflow.evaluate() Step 4: Result Analysis and Visualization  Step 1: Install and Load Packages [ ]: % pip install mlflow==2.9.0 langchain==0.0.339 openai faiss-cpu gensim nltk pyLDAvis tiktoken [1]: import ast import os import pprint import pandas as pd from langchain.docstore.document import Document from langchain.embeddings.openai import OpenAIEmbeddings from langchain.text_splitter import CharacterTextSplitter from langchain.vectorstores import FAISS import mlflow os . environ [ \"OPENAI_API_KEY\" ] = \"<redacted>\" CHUNK_SIZE = 1000 # Assume running from https://github.com/mlflow/mlflow/blob/master/examples/llms/rag OUTPUT_DF_PATH = \"question_answer_source.csv\" SCRAPPED_DOCS_PATH = \"mlflow_docs_scraped.csv\" EVALUATION_DATASET_PATH = \"static_evaluation_dataset.csv\" DB_PERSIST_DIR = \"faiss_index\"  Step 2: Evaluation Dataset Preparation The evaluation dataset should contain three columns: questions, ground truth doc IDs, retrieved relevant doc IDs. A \u00e2\u0080\u009cdoc ID\u00e2\u0080\u009d is a unique string identifier of the documents in you RAG application. For example, it could be the URL of a documentation web page, or the file path of a PDF document. If you have a list of questions that you would like to evaluate, please see 1.1 Manual Preparation. If you do not have a question list yet, please see 1.2 Generate the Evaluation Dataset.  Manual Preparation When evaluating a retriever, it\u00e2\u0080\u0099s recommended to save the retrieved document IDs into a static dataset represented by a Pandas Dataframe or an MLflow Pandas Dataset containing the input queries, retrieved relevant document IDs, and the ground-truth document IDs for the evaluation.  Concepts A \u00e2\u0080\u009cdocument ID\u00e2\u0080\u009d is a string that identifies a document. A list of \u00e2\u0080\u009cretrieved relevant document IDs\u00e2\u0080\u009d are the output of the retriever for a specific input query and a k value. A list of \u00e2\u0080\u009cground-truth document IDs\u00e2\u0080\u009d are the labeled relevant documents for a specific input query. ",
        "id": "43bb65687dd3f6b335d2d1de85009a4d"
    },
    {
        "text": " Expected Data Format For each row, the retrieved relevant document IDs and the ground-truth relevant document IDs should be provided as a tuple of document ID strings. The column name of the retrieved relevant document IDs should be specified by the predictions parameter, and the column name of the ground-truth relevant document IDs should be specified by the targets parameter. Here is a simple example dataset that illustrates the expected data format. The doc IDs are the paths of the documentation pages. [ ]: data = pd . DataFrame ( { \"questions\" : [ \"What is MLflow?\" , \"What is Databricks?\" , \"How to serve a model on Databricks?\" , \"How to enable MLflow Autologging for my workspace by default?\" , ], \"retrieved_context\" : [ [ \"mlflow/index.html\" , \"mlflow/quick-start.html\" , ], [ \"introduction/index.html\" , \"getting-started/overview.html\" , ], [ \"machine-learning/model-serving/index.html\" , \"machine-learning/model-serving/model-serving-intro.html\" , ], [], ], \"ground_truth_context\" : [ [ \"mlflow/index.html\" ], [ \"introduction/index.html\" ], [ \"machine-learning/model-serving/index.html\" , \"machine-learning/model-serving/llm-optimized-model-serving.html\" , ], [ \"mlflow/databricks-autologging.html\" ], ], } )  Generate the Evaluation Dataset There are two steps to generate the evaluation dataset: generate questions with ground truth doc IDs and retrieve relevant doc IDs.  Generate Questions with Ground Truth Doc IDs If you don\u00e2\u0080\u0099t have a list of questions to evaluate, you can generate them using LLMs. The Question Generation Notebook provides an example way to do it. Here is the result of running that notebook. [5]: generated_df = pd . read_csv ( OUTPUT_DF_PATH ) [7]: generated_df . head ( 3 ) [7]: question answer chunk chunk_id source 0 What is the purpose of the MLflow Model Registry? The purpose of the MLflow Model Registry is to... Documentation MLflow Model Registry MLflow Mod... 0 model-registry.html 1 What is the purpose of registering a model wit... The purpose of registering a model with the Mo... logged, this model can then be registered with... 1 model-registry.html 2 What can you do with registered models and mod... With registered models and model versions, you... associate with registered models and model ver... 2 model-registry.html [8]: # Prepare dataframe `data` with the required format data = pd . DataFrame ({}) data [ \"question\" ] = generated_df [ \"question\" ] . copy ( deep = True ) data [ \"source\" ] = generated_df [ \"source\" ] . apply ( lambda x : [ x ]) data . head ( 3 ) [8]: question source 0 What is the purpose of the MLflow Model Registry? [model-registry.html] 1 What is the purpose of registering a model wit... [model-registry.html] 2 What can you do with registered models and mod... [model-registry.html] ",
        "id": "cc3f6a4fc8b2bbedebfd2c1db324d667"
    },
    {
        "text": " Retrieve Relevant Doc IDs Once we have a list of questions with ground truth doc IDs from 1.1, we can collect the retrieved relevant doc IDs. In this tutorial, we use a LangChain retriever. You can plug in your own retriever as needed. First, we build a FAISS retriever from the docs saved at https://github.com/mlflow/mlflow/blob/master/examples/llms/question_generation/mlflow_docs_scraped.csv . See the Question Generation Notebook for how to create this csv file. [10]: embeddings = OpenAIEmbeddings () [ ]: scrapped_df = pd . read_csv ( SCRAPPED_DOCS_PATH ) list_of_documents = [ Document ( page_content = row [ \"text\" ], metadata = { \"source\" : row [ \"source\" ]}) for i , row in scrapped_df . iterrows () ] text_splitter = CharacterTextSplitter ( chunk_size = CHUNK_SIZE , chunk_overlap = 0 ) docs = text_splitter . split_documents ( list_of_documents ) db = FAISS . from_documents ( docs , embeddings ) # Save the db to local disk db . save_local ( DB_PERSIST_DIR ) [11]: # Load the db from local disk db = FAISS . load_local ( DB_PERSIST_DIR , embeddings ) retriever = db . as_retriever () [13]: # Test the retriever with a query retrieved_docs = retriever . get_relevant_documents ( \"What is the purpose of the MLflow Model Registry?\" ) len ( retrieved_docs ) [13]: 4 After building a retriever, we define a function that takes a question string as input and returns a list of relevant doc ID strings. [14]: # Define a function to return a list of retrieved doc ids def retrieve_doc_ids ( question : str ) -> list [ str ]: docs = retriever . get_relevant_documents ( question ) return [ doc . metadata [ \"source\" ] for doc in docs ] We can store the retrieved doc IDs in the dataframe as a column \u00e2\u0080\u009cretrieved_doc_ids\u00e2\u0080\u009d. [17]: data [ \"retrieved_doc_ids\" ] = data [ \"question\" ] . apply ( retrieve_doc_ids ) data . head ( 3 ) [17]: question source retrieved_doc_ids 0 What is the purpose of the MLflow Model Registry? [model-registry.html] [model-registry.html, introduction/index.html,... 1 What is the purpose of registering a model wit... [model-registry.html] [model-registry.html, models.html, introductio... 2 What can you do with registered models and mod... [model-registry.html] [model-registry.html, models.html, deployment/... [ ]: # Persist the static evaluation dataset to disk data . to_csv ( EVALUATION_DATASET_PATH , index = False ) [16]: # Load the static evaluation dataset from disk and deserialize the source and retrieved doc ids data = pd . read_csv ( EVALUATION_DATASET_PATH ) data [ \"source\" ] = data [ \"source\" ] . apply ( ast . literal_eval ) data [ \"retrieved_doc_ids\" ] = data [ \"retrieved_doc_ids\" ] . apply ( ast . literal_eval ) data . head ( 3 ) [16]: question source retrieved_doc_ids 0 What is the purpose of the MLflow Model Registry? [model-registry.html] [model-registry.html, introduction/index.html,... 1 What is the purpose of registering a model wit... [model-registry.html] [model-registry.html, models.html, introductio... 2 What can you do with registered models and mod... [model-registry.html] [model-registry.html, models.html, deployment/...  Step 3: Calling mlflow.evaluate()  Metrics Definition There are three built-in metrics provided for the retriever model type. Click the metric name below to see the metrics definitions. mlflow.metrics.precision_at_k(k) mlflow.metrics.recall_at_k(k) mlflow.metrics.ndcg_at_k(k) All metrics compute a score between 0 and 1 for each row representing the corresponding metric of the retriever model at the given k value. The k parameter should be a positive integer representing the number of retrieved documents to evaluate for each row. k defaults to 3. When the model type is \"retriever\" , these metrics will be calculated automatically with the default k value of 3. ",
        "id": "53c22d999d0d8a996d92ab48c8ad014b"
    },
    {
        "text": " Basic usage There are two supported ways to specify the retriever\u00e2\u0080\u0099s output: Case 1: Save the retriever\u00e2\u0080\u0099s output to a static evaluation dataset Case 2: Wrap the retriever in a function [ ]: # Case 1: Evaluating a static evaluation dataset with mlflow . start_run () as run : evaluate_results = mlflow . evaluate ( data = data , model_type = \"retriever\" , targets = \"source\" , predictions = \"retrieved_doc_ids\" , evaluators = \"default\" , ) 2023/11/22 14:39:59 WARNING mlflow.data.pandas_dataset: Failed to infer schema for Pandas dataset. Exception: Unable to map 'object' type to MLflow DataType. object can be mapped iff all values have identical data type which is one of (string, (bytes or byterray),  int, float).\n2023/11/22 14:39:59 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n2023/11/22 14:39:59 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n2023/11/22 14:39:59 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: precision_at_3\n2023/11/22 14:39:59 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: recall_at_3\n2023/11/22 14:39:59 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: ndcg_at_3 [18]: question_source_df = data [[ \"question\" , \"source\" ]] question_source_df . head ( 3 ) [18]: question source 0 What is the purpose of the MLflow Model Registry? [model-registry.html] 1 What is the purpose of registering a model wit... [model-registry.html] 2 What can you do with registered models and mod... [model-registry.html] [ ]: # Case 2: Evaluating a function def retriever_model_function ( question_df : pd . DataFrame ) -> pd . Series : return question_df [ \"question\" ] . apply ( retrieve_doc_ids ) with mlflow . start_run () as run : evaluate_results = mlflow . evaluate ( model = retriever_model_function , data = question_source_df , model_type = \"retriever\" , targets = \"source\" , evaluators = \"default\" , ) 2023/11/22 14:09:12 WARNING mlflow.data.pandas_dataset: Failed to infer schema for Pandas dataset. Exception: Unable to map 'object' type to MLflow DataType. object can be mapped iff all values have identical data type which is one of (string, (bytes or byterray),  int, float).\n2023/11/22 14:09:12 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n2023/11/22 14:09:12 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n2023/11/22 14:09:24 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n2023/11/22 14:09:24 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: precision_at_3\n2023/11/22 14:09:24 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: recall_at_3\n2023/11/22 14:09:24 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: ndcg_at_3 [ ]: pp = pprint . PrettyPrinter ( indent = 4 ) pp . pprint ( evaluate_results . metrics ) {   'ndcg_at_3/mean': 0.7530888125490431,\n    'ndcg_at_3/p90': 1.0,\n    'ndcg_at_3/variance': 0.1209151911325433,\n    'precision_at_3/mean': 0.26785714285714285,\n    'precision_at_3/p90': 0.3333333333333333,\n    'precision_at_3/variance': 0.017538265306122448,\n    'recall_at_3/mean': 0.8035714285714286,\n    'recall_at_3/p90': 1.0,\n    'recall_at_3/variance': 0.15784438775510204} ",
        "id": "a4ca37715c90a75dd29de84b32e63c12"
    },
    {
        "text": " Try different k values To use another k value, use the evaluator_config parameter in the mlflow.evaluate() API as follows: evaluator_config={\"retriever_k\": <k_value>} . # Case 1: Specifying the model type evaluate_results = mlflow . evaluate ( data = data , model_type = \"retriever\" , targets = \"ground_truth_context\" , predictions = \"retrieved_context\" , evaluators = \"default\" , evaluator_config = { \"retriever_k\" : 5 } ) Alternatively, you can directly specify the desired metrics in the extra_metrics parameter of the mlflow.evaluate() API without specifying a model type. In this case, the k value specified in the evaluator_config parameter will be ignored. # Case 2: Specifying the extra_metrics evaluate_results = mlflow . evaluate ( data = data , targets = \"ground_truth_context\" , predictions = \"retrieved_context\" , extra_metrics = [ mlflow . metrics . precision_at_k ( 4 ), mlflow . metrics . precision_at_k ( 5 ) ], ) [ ]: with mlflow . start_run () as run : evaluate_results = mlflow . evaluate ( data = data , targets = \"source\" , predictions = \"retrieved_doc_ids\" , evaluators = \"default\" , extra_metrics = [ mlflow . metrics . precision_at_k ( 1 ), mlflow . metrics . precision_at_k ( 2 ), mlflow . metrics . precision_at_k ( 3 ), mlflow . metrics . recall_at_k ( 1 ), mlflow . metrics . recall_at_k ( 2 ), mlflow . metrics . recall_at_k ( 3 ), mlflow . metrics . ndcg_at_k ( 1 ), mlflow . metrics . ndcg_at_k ( 2 ), mlflow . metrics . ndcg_at_k ( 3 ), ], ) 2023/11/22 14:40:22 WARNING mlflow.data.pandas_dataset: Failed to infer schema for Pandas dataset. Exception: Unable to map 'object' type to MLflow DataType. object can be mapped iff all values have identical data type which is one of (string, (bytes or byterray),  int, float).\n2023/11/22 14:40:22 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n2023/11/22 14:40:22 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n2023/11/22 14:40:22 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: precision_at_1\n2023/11/22 14:40:22 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: precision_at_2\n2023/11/22 14:40:22 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: precision_at_3\n2023/11/22 14:40:22 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: recall_at_1\n2023/11/22 14:40:22 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: recall_at_2\n2023/11/22 14:40:22 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: recall_at_3\n2023/11/22 14:40:22 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: ndcg_at_1\n2023/11/22 14:40:22 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: ndcg_at_2\n2023/11/22 14:40:22 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: ndcg_at_3 [ ]: import matplotlib.pyplot as plt # Plotting each metric for metric_name in [ \"precision\" , \"recall\" , \"ndcg\" ]: y = [ evaluate_results . metrics [ f \" { metric_name } _at_ { k } /mean\" ] for k in range ( 1 , 4 )] plt . plot ([ 1 , 2 , 3 ], y , label = f \" { metric_name } @k\" ) # Adding labels and title plt . xlabel ( \"k\" ) plt . ylabel ( \"Metric Value\" ) plt . title ( \"Metrics Comparison at Different Ks\" ) # Setting x-axis ticks plt . xticks ([ 1 , 2 , 3 ]) plt . legend () # Display the plot plt . show ()  Corner case handling There are a few corner cases handle specially for each built-in metric.  Empty retrieved document IDs When no relevant docs are retrieved: mlflow.metrics.precision_at_k(k) is defined as: 0 if the ground-truth doc IDs is non-empty 1 if the ground-truth doc IDs is also empty mlflow.metrics.ndcg_at_k(k) is defined as: 0 if the ground-truth doc IDs is non-empty 1 if the ground-truth doc IDs is also empty ",
        "id": "7138793e7e6d0d9b7fd1328230c642fc"
    },
    {
        "text": " Empty ground-truth document IDs When no ground-truth document IDs are provided: mlflow.metrics.recall_at_k(k) is defined as: 0 if the retrieved doc IDs is non-empty 1 if the retrieved doc IDs is also empty mlflow.metrics.ndcg_at_k(k) is defined as: 0 if the retrieved doc IDs is non-empty 1 if the retrieved doc IDs is also empty  Duplicate retreived document IDs It is a common case for the retriever in a RAG system to retrieve multiple chunks in the same document for a given query. In this case, mlflow.metrics.ndcg_at_k(k) is calculated as follows: If the duplicate doc IDs are in the ground truth, they will be treated as different docs. For example, if the ground truth doc IDs are [1, 2] and the retrieved doc IDs are [1, 1, 1, 3], the score will be equavalent to ground truth doc IDs [10, 11, 12, 2] and retrieved doc IDs [10, 11, 12, 3]. If the duplicate doc IDs are not in the ground truth, the ndcg score is calculated as normal. ",
        "id": "4ffcf49d7b58a96941a8b8dc210cabdc"
    },
    {
        "text": " Step 4: Result Analysis and Visualization You can view the per-row scores in the logged \u00e2\u0080\u009ceval_results_table.json\u00e2\u0080\u009d in artifacts by either loading it to a pandas dataframe (shown below) or visiting the MLflow run comparison UI. [ ]: eval_results_table = evaluate_results . tables [ \"eval_results_table\" ] eval_results_table . head ( 5 ) question source retrieved_doc_ids precision_at_1/score precision_at_2/score precision_at_3/score recall_at_1/score recall_at_2/score recall_at_3/score ndcg_at_1/score ndcg_at_2/score ndcg_at_3/score 0 What is the purpose of the MLflow Model Registry? [model-registry.html] [model-registry.html, introduction/index.html,... 1 0.5 0.333333 1 1 1 1 1.0 0.919721 1 What is the purpose of registering a model wit... [model-registry.html] [model-registry.html, models.html, introductio... 1 0.5 0.333333 1 1 1 1 1.0 1.000000 2 What can you do with registered models and mod... [model-registry.html] [model-registry.html, models.html, deployment/... 1 0.5 0.333333 1 1 1 1 1.0 1.000000 3 How can you add, modify, update, or delete a m... [model-registry.html] [model-registry.html, models.html, deployment/... 1 0.5 0.333333 1 1 1 1 1.0 1.000000 4 How can you deploy and organize models in the ... [model-registry.html] [model-registry.html, deployment/index.html, d... 1 0.5 0.333333 1 1 1 1 1.0 0.919721 With the evaluate results table, you can further visualize the well-answered questions and poorly-answered questions using topical analysis techniques. [ ]: import nltk import pyLDAvis.gensim_models as gensimvis from gensim import corpora , models from nltk.corpus import stopwords from nltk.tokenize import word_tokenize # Initialize NLTK resources nltk . download ( \"punkt\" ) nltk . download ( \"stopwords\" ) def topical_analysis ( questions : list [ str ]): stop_words = set ( stopwords . words ( \"english\" )) # Tokenize and remove stop words tokenized_data = [] for question in questions : tokens = word_tokenize ( question . lower ()) filtered_tokens = [ word for word in tokens if word not in stop_words and word . isalpha ()] tokenized_data . append ( filtered_tokens ) # Create a dictionary and corpus dictionary = corpora . Dictionary ( tokenized_data ) corpus = [ dictionary . doc2bow ( text ) for text in tokenized_data ] # Apply LDA model lda_model = models . LdaModel ( corpus , num_topics = 5 , id2word = dictionary , passes = 15 ) # Get topic distribution for each question topic_distribution = [] for i , ques in enumerate ( questions ): bow = dictionary . doc2bow ( tokenized_data [ i ]) topics = lda_model . get_document_topics ( bow ) topic_distribution . append ( topics ) print ( f \"Question: { ques } \\n Topic: { topics } \" ) # Print all topics print ( \" \\n Topics found are:\" ) for idx , topic in lda_model . print_topics ( - 1 ): print ( f \"Topic: { idx } \\n Words: { topic } \\n \" ) return lda_model , corpus , dictionary [nltk_data] Downloading package punkt to\n[nltk_data]     /Users/liang.zhang/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/liang.zhang/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date! [ ]: filtered_df = eval_results_table [ eval_results_table [ \"precision_at_1/score\" ] == 1 ] hit_questions = filtered_df [ \"question\" ] . tolist () filtered_df = eval_results_table [ eval_results_table [ \"precision_at_1/score\" ] == 0 ] miss_questions = filtered_df [ \"question\" ] . tolist () [ ]: lda_model , corpus , dictionary = topical_analysis ( hit_questions ) vis_data = gensimvis . prepare ( lda_model , corpus , dictionary ) Question: What is the purpose of the MLflow Model Registry?\nTopic: [(0, 0.0400703), (1, 0.040002838), (2, 0.040673085), (3, 0.04075462), (4, 0.8384991)]\nQuestion: What is the purpose of registering a model with the Model Registry?\nTopic: [(0, 0.0334267), (1, 0.033337697), (2, 0.033401005), (3, 0.033786207), (4, 0.8660484)]\nQuestion: What can you do with registered models and model versions?\nTopic: [(0, 0.04019648), (1, 0.04000775), (2, 0.040166058), (3, 0.8391777), (4, 0.040452003)]\nQuestion: How can you add, modify, update, or delete a model in the Model Registry?\nTopic: [(0, 0.025052568), (1, 0.025006149), (2, 0.025024023), (3, 0.025236268), (4, 0.899681)]\nQuestion: How can you deploy and organize models in the Model Registry?\nTopic: [(0, 0.033460867), (1, 0.033337582), (2, 0.0",
        "id": "45b06ab00fbcce439d05b9ee0714bfd2"
    },
    {
        "text": "6207), (4, 0.8660484)]\nQuestion: What can you do with registered models and model versions?\nTopic: [(0, 0.04019648), (1, 0.04000775), (2, 0.040166058), (3, 0.8391777), (4, 0.040452003)]\nQuestion: How can you add, modify, update, or delete a model in the Model Registry?\nTopic: [(0, 0.025052568), (1, 0.025006149), (2, 0.025024023), (3, 0.025236268), (4, 0.899681)]\nQuestion: How can you deploy and organize models in the Model Registry?\nTopic: [(0, 0.033460867), (1, 0.033337582), (2, 0.033362914), (3, 0.8659808), (4, 0.033857808)]\nQuestion: What method do you use to create a new registered model?\nTopic: [(0, 0.028867528), (1, 0.028582651), (2, 0.882546), (3, 0.030021703), (4, 0.029982116)]\nQuestion: How can you deploy and organize models in the Model Registry?\nTopic: [(0, 0.033460878), (1, 0.033337586), (2, 0.033362918), (3, 0.8659798), (4, 0.03385884)]\nQuestion: How can you fetch a list of registered models in the MLflow registry?\nTopic: [(0, 0.0286206), (1, 0.028577656), (2, 0.02894385), (3, 0.88495284), (4, 0.028905064)]\nQuestion: What is the default channel logged for models using MLflow v1.18 and above?\nTopic: [(0, 0.02862059), (1, 0.028577654), (2, 0.028883327), (3, 0.8851736), (4, 0.028744776)]\nQuestion: What information is stored in the conda.yaml file?\nTopic: [(0, 0.050020963), (1, 0.051287953), (2, 0.051250603), (3, 0.7968765), (4, 0.05056402)]\nQuestion: How can you save a model with a manually specified conda environment?\nTopic: [(0, 0.02862434), (1, 0.02858204), (2, 0.02886313), (3, 0.8851747), (4, 0.028755778)]\nQuestion: What are inference params and how are they used during model inference?\nTopic: [(0, 0.86457103), (1, 0.03353862), (2, 0.033417325), (3, 0.034004394), (4, 0.034468662)]\nQuestion: What is the purpose of model signatures in MLflow?\nTopic: [(0, 0.040070876), (1, 0.04000346), (2, 0.040688124), (3, 0.040469088), (4, 0.8387685)]\nQuestion: What is the API used to set signatures on models?\nTopic: [(0, 0.033873636), (1, 0.033508822), (2, 0.033337757), (3, 0.035357967), (4, 0.8639218)]\nQuestion: What components are used to generate the final time series?\nTopic: [(0, 0.028693806), (1, 0.8853218), (2, 0.028573763), (3, 0.02862714), (4, 0.0287835)]\nQuestion: What functionality does the configuration DataFrame submitted to the pyfunc flavor provide?\nTopic: [(0, 0.02519801), (1, 0.025009492), (2, 0.025004204), (3, 0.025004204), (4, 0.8997841)]\nQuestion: What is a common configuration for lowering the total memory pressure for pytorch models within transformers pipelines?\nTopic: [(0, 0.93316424), (1, 0.016669936), (2, 0.016668117), (3, 0.016788227), (4, 0.016709473)]\nQuestion: What does the save_model() function do?\nTopic: [(0, 0.10002145), (1, 0.59994656), (2, 0.10001026), (3, 0.10001026), (4, 0.10001151)]\nQuestion: What is an MLflow Project?\nTopic: [(0, 0.06667001), (1, 0.06667029), (2, 0.7321751), (3, 0.06711196), (4, 0.06737265)]\nQuestion: What are the entry points in a MLproject file and how can you specify parameters for them?\nTopic: [(0, 0.02857626), (1, 0.88541776), (2, 0.02868285), (3, 0.028626908), (4, 0.02869626)]\nQuestion: What are the project environments supported by MLflow?\nTopic: [(0, 0.040009078), (1, 0.040009864), (2, 0.839655), (3, 0.040126894), (4, 0.040199146)]\nQuestion: What is the purpose of specifying a Conda environment in an MLflow project?\nTopic: [(0, 0.028579442), (1, 0.028580135), (2, 0.8841217), (3, 0.028901232), (4, 0.029817443)]\nQuestion: What is the purpose of the MLproject file?\nTopic: [(0, 0.05001335), (1, 0.052611485), (2, 0.050071735), (3, 0.05043289), (4, 0.7968705)]\nQuestion: How can you pass runtime parameters to the entry point of an MLflow Project?\nTopic: [(0, 0.025007373), (1, 0.025498485), (2, 0.8993807), (3, 0.02504522), (4, 0.025068246)]\nQuestion: How does MLflow run a Project on Kubernetes?\nTopic: [(0, 0.04000677), (1, 0.040007353), (2, 0.83931196), (3, 0.04012452), (4, 0.04054937)]\nQuestion: What fields are replaced when MLflow creates a Kubernetes Job for an MLflow Project?\nTopic: [(0, 0.022228329), (1, 0.022228856), (2, 0.023192631), (3, 0.02235802), (4, 0.90999216)]\nQuestion: What is the syntax for searching runs using the MLflow UI and API?\nTopic: [(0, 0.025003674), (1, 0.02500399), (2, 0.02527212), (3, 0.89956146), (4, 0.025158761)]\nQuestion: What is the syntax for searching runs using the MLflow UI and API?\nTopic: [(0, 0.025003672), (1, 0.025003988), (",
        "id": "59f11c84833b2022e298197770061235"
    },
    {
        "text": ")]\nQuestion: What fields are replaced when MLflow creates a Kubernetes Job for an MLflow Project?\nTopic: [(0, 0.022228329), (1, 0.022228856), (2, 0.023192631), (3, 0.02235802), (4, 0.90999216)]\nQuestion: What is the syntax for searching runs using the MLflow UI and API?\nTopic: [(0, 0.025003674), (1, 0.02500399), (2, 0.02527212), (3, 0.89956146), (4, 0.025158761)]\nQuestion: What is the syntax for searching runs using the MLflow UI and API?\nTopic: [(0, 0.025003672), (1, 0.025003988), (2, 0.025272164), (3, 0.8995614), (4, 0.025158769)]\nQuestion: What are the key parts of a search expression in MLflow?\nTopic: [(0, 0.03334423), (1, 0.03334517), (2, 0.8662702), (3, 0.033611353), (4, 0.033429127)]\nQuestion: What are the key attributes for the model with the run_id 'a1b2c3d4' and run_name 'my-run'?\nTopic: [(0, 0.05017508), (1, 0.05001634), (2, 0.05058142), (3, 0.7985237), (4, 0.050703418)]\nQuestion: What information does each run record in MLflow Tracking?\nTopic: [(0, 0.03333968), (1, 0.033340227), (2, 0.86639804), (3, 0.03349555), (4, 0.033426523)]\nQuestion: What are the two components used by MLflow for storage?\nTopic: [(0, 0.0334928), (1, 0.033938777), (2, 0.033719826), (3, 0.03357158), (4, 0.86527705)]\nQuestion: What interfaces does the MLflow client use to record MLflow entities and artifacts when running MLflow on a local machine with a SQLAlchemy-compatible database?\nTopic: [(0, 0.014289577), (1, 0.014289909), (2, 0.94276434), (3, 0.014325481), (4, 0.014330726)]\nQuestion: What is the default backend store used by MLflow?\nTopic: [(0, 0.033753525), (1, 0.03379533), (2, 0.033777602), (3, 0.86454684), (4, 0.0341267)]\nQuestion: What information does autologging capture when launching short-lived MLflow runs?\nTopic: [(0, 0.028579954), (1, 0.02858069), (2, 0.8851724), (3, 0.029027484), (4, 0.028639426)]\nQuestion: What is the purpose of the --serve-artifacts flag?\nTopic: [(0, 0.06670548), (1, 0.066708855), (2, 0.067003354), (3, 0.3969311), (4, 0.40265122)]\n\nTopics found are:\nTopic: 0\nWords: 0.059*\"inference\" + 0.032*\"models\" + 0.032*\"used\" + 0.032*\"configuration\" + 0.032*\"common\" + 0.032*\"transformers\" + 0.032*\"total\" + 0.032*\"within\" + 0.032*\"pytorch\" + 0.032*\"pipelines\"\n\nTopic: 1\nWords: 0.036*\"file\" + 0.035*\"mlproject\" + 0.035*\"used\" + 0.035*\"components\" + 0.035*\"entry\" + 0.035*\"parameters\" + 0.035*\"specify\" + 0.035*\"final\" + 0.035*\"points\" + 0.035*\"time\"\n\nTopic: 2\nWords: 0.142*\"mlflow\" + 0.066*\"project\" + 0.028*\"information\" + 0.028*\"use\" + 0.028*\"record\" + 0.028*\"run\" + 0.015*\"key\" + 0.015*\"running\" + 0.015*\"artifacts\" + 0.015*\"client\"\n\nTopic: 3\nWords: 0.066*\"models\" + 0.066*\"model\" + 0.066*\"mlflow\" + 0.041*\"using\" + 0.041*\"registry\" + 0.028*\"api\" + 0.028*\"registered\" + 0.028*\"runs\" + 0.028*\"syntax\" + 0.028*\"searching\"\n\nTopic: 4\nWords: 0.089*\"model\" + 0.074*\"purpose\" + 0.074*\"mlflow\" + 0.046*\"registry\" + 0.031*\"used\" + 0.031*\"signatures\" + 0.017*\"kubernetes\" + 0.017*\"fields\" + 0.017*\"job\" + 0.017*\"replaced\" [3]: # Uncomment the following line to render the interactive widget # pyLDAvis.display(vis_data) [ ]: lda_model , corpus , dictionary = topical_analysis ( miss_questions ) vis_data = gensimvis . prepare ( lda_model , corpus , dictionary ) Question: What is the purpose of the mlflow.sklearn.log_model() method?\nTopic: [(0, 0.0669118), (1, 0.06701085), (2, 0.06667974), (3, 0.73235476), (4, 0.06704286)]\nQuestion: How can you fetch a specific model version?\nTopic: [(0, 0.83980393), (1, 0.040003464), (2, 0.04000601), (3, 0.040101767), (4, 0.040084846)]\nQuestion: How can you fetch the latest model version in a specific stage?\nTopic: [(0, 0.88561153), (1, 0.028575428), (2, 0.028578365), (3, 0.0286214), (4, 0.028613236)]\nQuestion: What can you do to promote MLflow Models across environments?\nTopic: [(0, 0.8661927), (1, 0.0333396), (2, 0.03362743), (3, 0.033428304), (4, 0.033411972)]\nQuestion: What is the name of the model and its version details?\nTopic: [(0, 0.83978903), (1, 0.04000637), (2, 0.04001106), (3, 0.040105395), (4, 0.040088095)]\nQuestion: What is the purpose of saving the model in pickled format?\nTopic: [(0, 0.033948876), (1, 0.03339717), (2, 0.033340737), (3, 0.86575514), (4, 0.033558063)]\nQuestion: What is an MLflow Model and what is its purpose?\nTopic: [(0, 0.7940762), (1, 0.05068333), (2, 0.050770763), (3, 0.053328265), (4, 0.05114142)]\nQuestion: What are the flavors defined in the MLmodel file for the mlflow",
        "id": "62bf32ad33755cc7c7f994f4e109e1ce"
    },
    {
        "text": "?\nTopic: [(0, 0.83978903), (1, 0.04000637), (2, 0.04001106), (3, 0.040105395), (4, 0.040088095)]\nQuestion: What is the purpose of saving the model in pickled format?\nTopic: [(0, 0.033948876), (1, 0.03339717), (2, 0.033340737), (3, 0.86575514), (4, 0.033558063)]\nQuestion: What is an MLflow Model and what is its purpose?\nTopic: [(0, 0.7940762), (1, 0.05068333), (2, 0.050770763), (3, 0.053328265), (4, 0.05114142)]\nQuestion: What are the flavors defined in the MLmodel file for the mlflow.sklearn library?\nTopic: [(0, 0.86628276), (1, 0.033341788), (2, 0.03334801), (3, 0.03368498), (4, 0.033342462)]\nQuestion: What command can be used to package and deploy models to AWS SageMaker?\nTopic: [(0, 0.89991224), (1, 0.025005225), (2, 0.025009066), (3, 0.025006713), (4, 0.025066752)]\nQuestion: What is the purpose of the --build-image flag when running mlflow run?\nTopic: [(0, 0.033957016), (1, 0.033506736), (2, 0.034095332), (3, 0.034164555), (4, 0.86427635)]\nQuestion: What is the relative path to the python_env YAML file within the MLflow project's directory?\nTopic: [(0, 0.02243), (1, 0.02222536), (2, 0.022470985), (3, 0.9105873), (4, 0.02228631)]\nQuestion: What are the additional local volume mounted and environment variables in the docker container?\nTopic: [(0, 0.022225259), (1, 0.9110914), (2, 0.02222932), (3, 0.022227468), (4, 0.022226628)]\nQuestion: What are some examples of entity names that contain special characters?\nTopic: [(0, 0.028575381), (1, 0.88568854), (2, 0.02858065), (3, 0.028578246), (4, 0.028577149)]\nQuestion: What type of constant does the RHS need to be if LHS is a metric?\nTopic: [(0, 0.028575381), (1, 0.8856886), (2, 0.028580645), (3, 0.028578239), (4, 0.028577147)]\nQuestion: How can you get all active runs from experiments IDs 3, 4, and 17 that used a CNN model with 10 layers and had a prediction accuracy of 94.5% or higher?\nTopic: [(0, 0.015563371), (1, 0.015387185), (2, 0.015389071), (3, 0.015427767), (4, 0.9382326)]\nQuestion: What is the purpose of the 'experimentIds' variable in the given paragraph?\nTopic: [(0, 0.040206533), (1, 0.8384999), (2, 0.040013183), (3, 0.040967643), (4, 0.040312726)]\nQuestion: What is the MLflow Tracking component used for?\nTopic: [(0, 0.8390845), (1, 0.04000697), (2, 0.040462855), (3, 0.04014182), (4, 0.040303845)]\nQuestion: How can you create an experiment in MLflow?\nTopic: [(0, 0.050333958), (1, 0.0500024), (2, 0.7993825), (3, 0.050153885), (4, 0.05012722)]\nQuestion: How can you create an experiment using MLflow?\nTopic: [(0, 0.04019285), (1, 0.04000254), (2, 0.8396381), (3, 0.040091105), (4, 0.04007539)]\nQuestion: What is the architecture depicted in this example scenario?\nTopic: [(0, 0.04000523), (1, 0.040007014), (2, 0.040012203), (3, 0.04000902), (4, 0.83996654)]\n\nTopics found are:\nTopic: 0\nWords: 0.078*\"model\" + 0.059*\"mlflow\" + 0.059*\"version\" + 0.041*\"models\" + 0.041*\"fetch\" + 0.041*\"specific\" + 0.041*\"used\" + 0.022*\"command\" + 0.022*\"deploy\" + 0.022*\"sagemaker\"\n\nTopic: 1\nWords: 0.030*\"local\" + 0.030*\"container\" + 0.030*\"variables\" + 0.030*\"docker\" + 0.030*\"mounted\" + 0.030*\"environment\" + 0.030*\"volume\" + 0.030*\"additional\" + 0.030*\"special\" + 0.030*\"names\"\n\nTopic: 2\nWords: 0.096*\"experiment\" + 0.096*\"create\" + 0.096*\"mlflow\" + 0.051*\"using\" + 0.009*\"purpose\" + 0.009*\"model\" + 0.009*\"method\" + 0.009*\"file\" + 0.009*\"version\" + 0.009*\"used\"\n\nTopic: 3\nWords: 0.071*\"purpose\" + 0.039*\"file\" + 0.039*\"mlflow\" + 0.039*\"yaml\" + 0.039*\"directory\" + 0.039*\"relative\" + 0.039*\"within\" + 0.039*\"path\" + 0.039*\"project\" + 0.039*\"format\"\n\nTopic: 4\nWords: 0.032*\"purpose\" + 0.032*\"used\" + 0.032*\"model\" + 0.032*\"prediction\" + 0.032*\"get\" + 0.032*\"accuracy\" + 0.032*\"active\" + 0.032*\"layers\" + 0.032*\"higher\" + 0.032*\"experiments\" [4]: # Uncomment the following line to render the interactive widget # pyLDAvis.display(vis_data) [ ]: Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "4286a32aa404e30e0a9cdf1c99f5eb97"
    },
    {
        "text": "Custom PyFuncs for Advanced LLMs with MLflow - Notebooks 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow Retrieval Augmented Generation (RAG) Deploying Advanced LLMs with Custom PyFuncs in MLflow Explore the Tutorial LLM Evaluation Examples Tutorial: Getting Started with ChatModel Tutorial: Custom GenAI Models using ChatModel Build a tool-calling model with mlflow.pyfunc.ChatModel MLflow Trace UI in Jupyter Notebook Demo MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs Deploying Advanced LLMs with Custom PyFuncs in MLflow Custom PyFuncs for Advanced LLMs with MLflow - Notebooks  Custom PyFuncs for Advanced LLMs with MLflow - Notebooks If you\u00e2\u0080\u0099d like to delve deeper into the notebooks in this guide, they can be viewed or downloaded directly below.  Deploying Advanced LLMs with Custom PyFuncs  Introduction In this tutorial, we\u00e2\u0080\u0099ll explore the nuances of deploying advanced Large Language Models (LLMs) with MLflow, particularly focusing on models\nthat can\u00e2\u0080\u0099t be readily managed with MLflow\u00e2\u0080\u0099s built-in functionality. We\u00e2\u0080\u0099ll highlight the necessity of custom pyfunc definitions when\ndealing with such complex models, emphasizing its role in managing intricate model behaviors and dependencies. By the end, you\u00e2\u0080\u0099ll understand\nthe intricacies of deploying an LLM model using the MPT-7B instruct transformer, wrapped efficiently using a custom pyfunc .  What you will learn LLM Deployment Challenges : Recognize the complexities and challenges associated with deploying advanced LLMs in MLflow. Custom PyFuncs for LLMs : Understand the need and process of creating a custom pyfunc to effectively manage LLMs, particularly when default flavors fall short. Prompt Management in Deployment : Delve into how custom pyfunc allows manipulation of interface data to generate prompts, simplifying end-user interactions in a RESTful environment. Leveraging Custom PyFunc for Enhanced Flexibility : Witness how custom pyfunc definitions provide the flexibility needed for advanced model behaviors and dependencies.  Why Custom pyfunc for LLM Deployment? Deploying advanced LLMs isn\u00e2\u0080\u0099t straightforward. Models like the MPT-7B instruct transformer have specific requirements and behaviors that don\u00e2\u0080\u0099t align with traditional MLflow flavors. This section highlights the challenges faced and the importance of custom pyfunc definitions in addressing these challenges.  Crafting the Custom pyfunc Venturing into the solution, we\u00e2\u0080\u0099ll craft a custom pyfunc to efficiently wrap and manage our LLM. This custom definition serves as a bridge, ensuring our LLM can be deployed seamlessly while retaining its original capabilities and adhering to MLflow\u00e2\u0080\u0099s standards.  Step-by-step Guide LLM Introduction : Understand the MPT-7B instruct transformer, its importance, and its intricacies. Challenges with Traditional Deployment : Recognize the difficulties when attempting to deploy such an LLM using MLflow\u00e2\u0080\u0099s default capabilities. Designing the Custom `pyfunc` : Create a custom pyfunc that addresses the LLM\u00e2\u0080\u0099s requirements and behaviors. Deploying the LLM : Integrate with MLflow to deploy the LLM using the crafted custom pyfunc . Interface Simplification : Examine how the custom pyfunc simplifies user interactions, particularly in RESTful deployments. ",
        "id": "77a4f7d20e7db0446520de854fd2a602"
    },
    {
        "text": " Wrap Up With the complexities of advanced LLM deployment unraveled, this tutorial showcases the indispensable role of custom pyfunc in MLflow. Through a detailed, hands-on approach, you\u00e2\u0080\u0099ll appreciate how custom pyfunc definitions can make seemingly insurmountable deployment challenges manageable and streamlined. Serving LLMs with MLflow: Leveraging Custom PyFunc Learn how to use the MLflow Custom Pyfunc Model to serve Large Language Models (LLMs) in a RESTful environment. Note To execute the notebooks, ensure you either have a local MLflow Tracking Server running or adjust the mlflow.set_tracking_uri() to point to an active MLflow Tracking Server instance.\nTo engage with the MLflow UI, ensure you\u00e2\u0080\u0099re either running the UI server locally or have a configured, accessible, deployed MLflow UI server. Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "2fc10cb6fa3138d948d1760c8fefdbeb"
    },
    {
        "text": "LLM Evaluation with MLflow Example Notebook 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow Retrieval Augmented Generation (RAG) Deploying Advanced LLMs with Custom PyFuncs in MLflow LLM Evaluation Examples LLM Evaluation with MLflow Example Notebook LLM RAG Evaluation with MLflow Example Notebook LLM RAG Evaluation with MLflow using llama2-as-judge Example Notebook Create a RAG system Evaluate the RAG system using mlflow.evaluate() Evaluate a Hugging Face LLM with mlflow.evaluate() QA Evaluation Tutorial RAG Evaluation Tutorials Tutorial: Getting Started with ChatModel Tutorial: Custom GenAI Models using ChatModel Build a tool-calling model with mlflow.pyfunc.ChatModel MLflow Trace UI in Jupyter Notebook Demo MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs LLM Evaluation Examples LLM Evaluation with MLflow Example Notebook  LLM Evaluation with MLflow Example Notebook In this notebook, we will demonstrate how to evaluate various LLMs and RAG systems with MLflow, leveraging simple metrics such as toxicity, as well as LLM-judged metrics such as relevance, and even custom LLM-judged metrics such as professionalism Download this Notebook We need to set our OpenAI API key, since we will be using GPT-4 for our LLM-judged metrics. In order to set your private key safely, please be sure to either export your key through a command-line terminal for your current instance, or, for a permanent addition to all user-based sessions, configure your favored environment management configuration file (i.e., .bashrc, .zshrc) to have the following entry: OPENAI_API_KEY=<your openai API key> [3]: import openai import pandas as pd import mlflow ",
        "id": "72ae33c3796571a32818dd8cda58b145"
    },
    {
        "text": " Basic Question-Answering Evaluation Create a test case of inputs that will be passed into the model and ground_truth which will be used to compare against the generated output from the model. [4]: eval_df = pd . DataFrame ( { \"inputs\" : [ \"How does useEffect() work?\" , \"What does the static keyword in a function mean?\" , \"What does the 'finally' block in Python do?\" , \"What is the difference between multiprocessing and multithreading?\" , ], \"ground_truth\" : [ \"The useEffect() hook tells React that your component needs to do something after render. React will remember the function you passed (we\u00e2\u0080\u0099ll refer to it as our \u00e2\u0080\u009ceffect\u00e2\u0080\u009d), and call it later after performing the DOM updates.\" , \"Static members belongs to the class, rather than a specific instance. This means that only one instance of a static member exists, even if you create multiple objects of the class, or if you don't create any. It will be shared by all objects.\" , \"'Finally' defines a block of code to run when the try... except...else block is final. The finally block will be executed no matter if the try block raises an error or not.\" , \"Multithreading refers to the ability of a processor to execute multiple threads concurrently, where each thread runs a process. Whereas multiprocessing refers to the ability of a system to run multiple processors in parallel, where each processor can run one or more threads.\" , ], } ) Create a simple OpenAI model that asks gpt-4o to answer the question in two sentences. Call mlflow.evaluate() with the model and evaluation dataframe. [5]: with mlflow . start_run () as run : system_prompt = \"Answer the following question in two sentences\" basic_qa_model = mlflow . openai . log_model ( model = \"gpt-4o-mini\" , task = openai . chat . completions , artifact_path = \"model\" , messages = [ { \"role\" : \"system\" , \"content\" : system_prompt }, { \"role\" : \"user\" , \"content\" : \" {question} \" }, ], ) results = mlflow . evaluate ( basic_qa_model . model_uri , eval_df , targets = \"ground_truth\" , # specify which column corresponds to the expected output model_type = \"question-answering\" , # model type indicates which metrics are relevant for this task evaluators = \"default\" , ) results . metrics 2023/10/27 00:56:56 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n2023/10/27 00:56:56 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\nUsing default facebook/roberta-hate-speech-dynabench-r4-target checkpoint\n2023/10/27 00:57:06 INFO mlflo",
        "id": "fe76b89f708eafa43e2bb6a10687ffdb"
    },
    {
        "text": "flow.models.evaluation.base: Evaluating the model with the default evaluator.\n2023/10/27 00:56:56 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\nUsing default facebook/roberta-hate-speech-dynabench-r4-target checkpoint\n2023/10/27 00:57:06 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: token_count\n2023/10/27 00:57:06 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: toxicity\n2023/10/27 00:57:06 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: flesch_kincaid_grade_level\n2023/10/27 00:57:06 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: ari_grade_level\n2023/10/27 00:57:06 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: exact_match [5]: {'toxicity/v1/mean': 0.00020573455913108774,\n 'toxicity/v1/variance': 3.4433758978645428e-09,\n 'toxicity/v1/p90': 0.00027067282790085303,\n 'toxicity/v1/ratio': 0.0,\n 'flesch_kincaid_grade_level/v1/mean': 15.149999999999999,\n 'flesch_kincaid_grade_level/v1/variance': 26.502499999999998,\n 'flesch_kincaid_grade_level/v1/p90': 20.85,\n 'ari_grade_level/v1/mean': 17.375,\n 'ari_grade_level/v1/variance': 42.92187499999999,\n 'ari_grade_level/v1/p90': 24.48,\n 'exact_match/v1': 0.0} Inspect the evaluation results table as a dataframe to see row-by-row metrics to further assess model performance [6]: results . tables [ \"eval_results_table\" ] [6]: inputs ground_truth outputs token_count toxicity/v1/score flesch_kincaid_grade_level/v1/score ari_grade_level/v1/score 0 How does useEffect() work? The useEffect() hook tells React that your com... useEffect() is a React hook that allows you to... 64 0.000243 14.2 15.8 1 What does the static keyword in a function mean? Static members belongs to the class, rather th... The static keyword in a function means that th... 32 0.000150 12.6 14.9 2 What does the 'finally' block in Python do? 'Finally' defines a block of code to run when ... The 'finally' block in Python is used to speci... 46 0.000283 10.1 10.6 3 What is the difference between multiprocessing... Multithreading refers to the ability of a proc... The main difference between multiprocessing an... 34 0.000148 23.7 28.2 ",
        "id": "0cf037f44c90a8de60edb59c9d5ba7c4"
    },
    {
        "text": " LLM-judged correctness with OpenAI GPT-4 Construct an answer similarity metric using the answer_similarity() metric factory function. [7]: from mlflow.metrics.genai import EvaluationExample , answer_similarity # Create an example to describe what answer_similarity means like for this problem. example = EvaluationExample ( input = \"What is MLflow?\" , output = \"MLflow is an open-source platform for managing machine \" \"learning workflows, including experiment tracking, model packaging, \" \"versioning, and deployment, simplifying the ML lifecycle.\" , score = 4 , justification = \"The definition effectively explains what MLflow is \" \"its purpose, and its developer. It could be more concise for a 5-score.\" , grading_context = { \"targets\" : \"MLflow is an open-source platform for managing \" \"the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, \" \"a company that specializes in big data and machine learning solutions. MLflow is \" \"designed to address the challenges that data scientists and machine learning \" \"engineers face when developing, training, and deploying machine learning models.\" }, ) # Construct the metric using OpenAI GPT-4 as the judge answer_similarity_metric = answer_similarity ( model = \"openai:/gpt-4\" , examples = [ example ]) print ( answer_similarity_metric ) EvaluationMetric(name=answer_similarity, greater_is_better=True, long_name=answer_similarity, version=v1, metric_details=\nTask:\nYou are an impartial judge. You will be given an input that was sent to a machine\nlearning model, and you will be given an output that the model produced. You\nmay also be given additional information that was used by the model to generate the output.\n\nYour task is to determine a numerical score called answer_similarity based on the input and output.\nA definition of answer_similarity and a grading rubric are provided below.\nYou must use the grading rubric to determine your score. You must also justify your score.\n\nExamples could be included below for reference. Make sure to use them as references and to\nunderstand them before completing the task.\n\nInput:\n{input}\n\nOutput:\n{output}\n\n{grading_context_columns}\n\nMetric definition:\nAnswer similarity is evaluated on the degree of semantic similarity of the provided output to the provided targets, which is the ground truth. Scores can be assigned based on the gradual similarity in meaning and description to the provided targets, where a higher score indicates greater alignment between the provided output and provided targets.\n\nGrading rubric:\nAnswer similarity: Below are the details for different scores:\n- Score 1: the output has little to no semantic similarity to the provided targets.\n- Score 2: the output displays partial semantic similarity to the provided targets on some aspects.\n- Score 3: the output has moderate semantic similarity to the provided targets.\n- Score 4: the output aligns with the provided targets in most aspects and has substantial semantic similarity.\n- Score 5: the output closely aligns with the provided targets in all significant aspects.\n\nExamples:\n\nInput:\nWhat is MLflow?\n\nOutput:\nMLflow is an open-source platform for managing machine learning workflows, including experiment tracking, model packaging, versioning, and deployment, simplifying the ML lifecycle.\n\nAdditional information used by the model:\nkey: ground_truth\nvalue:\nMLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.\n\nscore: 4\njustification: The definition effectively explains what MLflow is its purpose, and its developer. It could be more concise for a 5-score.\n\n\nYou must return the following fields in your response one below the other:\nscore: Your numerical score for the model's answer_similarity",
        "id": "4f6c8d09fefcca3ad5b23febcf881af6"
    },
    {
        "text": "s designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.\n\nscore: 4\njustification: The definition effectively explains what MLflow is its purpose, and its developer. It could be more concise for a 5-score.\n\n\nYou must return the following fields in your response one below the other:\nscore: Your numerical score for the model's answer_similarity based on the rubric\njustification: Your step-by-step reasoning about the model's answer_similarity score\n    ) Call mlflow.evaluate() again but with your new answer_similarity_metric [8]: with mlflow . start_run () as run : results = mlflow . evaluate ( basic_qa_model . model_uri , eval_df , targets = \"ground_truth\" , model_type = \"question-answering\" , evaluators = \"default\" , extra_metrics = [ answer_similarity_metric ], # use the answer similarity metric created above ) results . metrics 2023/10/27 00:57:07 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n2023/10/27 00:57:07 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n2023/10/27 00:57:13 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: token_count\n2023/10/27 00:57:13 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: toxicity\n2023/10/27 00:57:13 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: flesch_kincaid_grade_level\n2023/10/27 00:57:13 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: ari_grade_level\n2023/10/27 00:57:13 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: exact_match\n2023/10/27 00:57:13 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: answer_similarity [8]: {'toxicity/v1/mean': 0.00023413174494635314,\n 'toxicity/v1/variance': 4.211776498455113e-09,\n 'toxicity/v1/p90': 0.00029628578631673007,\n 'toxicity/v1/ratio': 0.0,\n 'flesch_kincaid_grade_level/v1/mean': 14.774999999999999,\n 'flesch_kincaid_grade_level/v1/variance': 21.546875000000004,\n 'flesch_kincaid_grade_level/v1/p90': 19.71,\n 'ari_grade_level/v1/mean': 17.0,\n 'ari_grade_level/v1/variance': 41.005,\n 'ari_grade_level/v1/p90': 23.92,\n 'exact_match/v1': 0.0,\n 'answer_similarity/v1/mean': 3.75,\n 'answer_similarity/v1/variance': 1.1875,\n 'answer_similarity/v1/p90': 4.7} See the row-by-row LLM-judged answer similarity score and justifications [9]: results . tables [ \"eval_results_table\" ] [9]: inputs ground_truth outputs token_count toxicity/v1/score flesch_kincaid_grade_level/v1/score ari_grade_level/v1/score answer_similarity/v1/score answer_similarity/v1/justification 0 How does useEffect() work? The useEffect() hook tells React that your com... useEffect() is a React hook that allows you to... 53 0.000299 12.1 12.1 4 The output provided by the model aligns well w... 1 What does the static keyword in a function mean? Static members belongs to the class, rather th... In C/C++, the static keyword in a function mea... 55 0.000141 12.5 14.4 2 The output provided by the model does correctl... 2 What does the 'finally' block in Python do? 'Finally' defines a block of code to run when ... The 'finally' block in Python is used to defin... 64 0.000290 11.7 13.5 5 The output provided by the model aligns very c... 3 What is the difference between multiprocessing... Multithreading refers to the ability of a proc... Multiprocessing involves the execution of mult... 49 0.000207 22.8 28.0 4 The output provided by the model aligns well w... ",
        "id": "9c3c8e3b0dd3a15fdb3d240d3ecd90d9"
    },
    {
        "text": " Custom LLM-judged metric for professionalism Create a custom metric that will be used to determine professionalism of the model outputs. Use make_genai_metric with a metric definition, grading prompt, grading example, and judge model configuration [10]: from mlflow.metrics.genai import EvaluationExample , make_genai_metric professionalism_metric = make_genai_metric ( name = \"professionalism\" , definition = ( \"Professionalism refers to the use of a formal, respectful, and appropriate style of communication that is tailored to the context and audience. It often involves avoiding overly casual language, slang, or colloquialisms, and instead using clear, concise, and respectful language\" ), grading_prompt = ( \"Professionalism: If the answer is written using a professional tone, below \" \"are the details for different scores: \" \"- Score 1: Language is extremely casual, informal, and may include slang or colloquialisms. Not suitable for professional contexts.\" \"- Score 2: Language is casual but generally respectful and avoids strong informality or slang. Acceptable in some informal professional settings.\" \"- Score 3: Language is balanced and avoids extreme informality or formality. Suitable for most professional contexts. \" \"- Score 4: Language is noticeably formal, respectful, and avoids casual elements. Appropriate for business or academic settings. \" \"- Score 5: Language is excessively formal, respectful, and avoids casual elements. Appropriate for the most formal settings such as textbooks. \" ), examples = [ EvaluationExample ( input = \"What is MLflow?\" , output = ( \"MLflow is like your friendly neighborhood toolkit for managing your machine learning projects. It helps you track experiments, package your code and models, and collaborate with your team, making the whole ML workflow smoother. It's like your Swiss Army knife for machine learning!\" ), score = 2 , justification = ( \"The response is written in a casual tone. It uses contractions, filler words such as 'like', and exclamation points, which make it sound less professional. \" ), ) ], version = \"v1\" , model = \"openai:/gpt-4\" , parameters = { \"temperature\" : 0.0 }, grading_context_columns = [], aggregations = [ \"mean\" , \"variance\" , \"p90\" ], greater_is_better = True , ) print ( professionalism_metric ) EvaluationMetric(name=professionalism, greater_is_better=True, long_name=professionalism, version=v1, metric_details=\nTask:\nYou are an impartial judge. You will be given an input that was sent to a machine\nlearning model, and you will be given an output that the model produced. You\nmay also be given additional information that was used by the model to generate the output.\n\nYour task is to determine a numerical score called professionalism based on the input and output.\nA definition of professionalism and a grading rubric are provided below.\nYou must use the grading rubric to determine your score. You must also justify your score.\n\nExamples could be included below for reference. Make sure to use them as references and to\nunderstand them before completing the task.\n\nInput:\n{input}\n\nOutput:\n{output}\n\n{grading_context_columns}\n\nMetric definition:\nProfessionalism refers to the use of a formal, respectful, and appropriate style of communication that is tailored to the context and audience. It often involves avoiding overly casual language, slang, or colloquialisms, and instead using clear, concise, and respectful language\n\nGrading rubric:\nProfessionalism: If the answer is written using a professional tone, below are the details for different scores: - Score 1: Language is extremely casual, informal, and may include slang or colloquialisms. Not suitable for professional contexts.- Score 2: Language is casual but generally respectful and avoids strong informality or slang. Acceptable in some informal professional settings.- Score 3: Language is balanced and avoids extreme informality or formality. Suitable for most professional contexts. - Score 4: Language is noticeably formal, respectful, and avoids casual elements. Appropriate for business or academic settings. - Score 5: Language is excessively formal, respectful, and avoids casual elements. Appropriate for the most formal settings such as textbooks.\n\nExamples:\n\nInput:\nWhat is MLflow?\n\nOutput:\nMLflow is like your friendly neighborhood toolkit for managing your machine learning projects. It helps you track experiments, package your code and models, and collaborate with your team, making the whole ML workf",
        "id": "7773994e576e447b85a19b46ee0bd447"
    },
    {
        "text": "y formal, respectful, and avoids casual elements. Appropriate for business or academic settings. - Score 5: Language is excessively formal, respectful, and avoids casual elements. Appropriate for the most formal settings such as textbooks.\n\nExamples:\n\nInput:\nWhat is MLflow?\n\nOutput:\nMLflow is like your friendly neighborhood toolkit for managing your machine learning projects. It helps you track experiments, package your code and models, and collaborate with your team, making the whole ML workflow smoother. It's like your Swiss Army knife for machine learning!\n\n\n\nscore: 2\njustification: The response is written in a casual tone. It uses contractions, filler words such as 'like', and exclamation points, which make it sound less professional.\n\n\nYou must return the following fields in your response one below the other:\nscore: Your numerical score for the model's professionalism based on the rubric\njustification: Your step-by-step reasoning about the model's professionalism score\n    ) Call mlflow.evaluate with your new professionalism metric. [11]: with mlflow . start_run () as run : results = mlflow . evaluate ( basic_qa_model . model_uri , eval_df , model_type = \"question-answering\" , evaluators = \"default\" , extra_metrics = [ professionalism_metric ], # use the professionalism metric we created above ) print ( results . metrics ) 2023/10/27 00:57:20 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n2023/10/27 00:57:20 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n2023/10/27 00:57:24 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: token_count\n2023/10/27 00:57:24 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: toxicity\n2023/10/27 00:57:25 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: flesch_kincaid_grade_level\n2023/10/27 00:57:25 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: ari_grade_level\n2023/10/27 00:57:25 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: exact_match\n2023/10/27 00:57:25 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: professionalism {'toxicity/v1/mean': 0.0002044261127593927, 'toxicity/v1/variance': 1.8580601275034412e-09, 'toxicity/v1/p90': 0.00025343164161313326, 'toxicity/v1/ratio': 0.0, 'flesch_kincaid_grade_level/v1/mean': 13.649999999999999, 'flesch_kincaid_grade_level/v1/variance': 33.927499999999995, 'flesch_kincaid_grade_level/v1/p90': 19.92, 'ari_grade_level/v1/mean': 16.25, 'ari_grade_level/v1/variance': 51.927499999999995, 'ari_grade_level/v1/p90': 23.900000000000002, 'professionalism/v1/mean': 4.0, 'professionalism/v1/variance': 0.0, 'professionalism/v1/p90': 4.0} [12]: results . tables [ \"eval_results_table\" ] [12]: inputs ground_truth outputs token_count toxicity/v1/score flesch_kincaid_grade_level/v1/score ari_grade_level/v1/score professionalism/v1/score professionalism/v1/justification 0 How does useEffect() work? The useEffect() hook tells React that your com... useEffect() is a hook in React that allows you... 46 0.000218 11.1 12.7 4 The language used in the output is formal and ... 1 What does the static keyword in a function mean? Static members belongs to the class, rather th... The static keyword in a function means that th... 48 0.000158 9.7 12.3 4 The language used in the output is formal and ... 2 What does the 'finally' block in Python do? 'Finally' defines a block of code to run when ... The 'finally' block in Python is used to defin... 45 0.000269 10.1 11.3 4 The language used in the output is formal and ... 3 What is the difference between multiprocessing... Multithreading refers to the ability of a proc... Multiprocessing involves running multiple proc... 33 0.000173 23.7 28.7 4 The language used in the output is formal and ... Lets see if we can improve basic_qa_model by creating a new model that could perform better by changing the system prompt. Call mlflow.evaluate() using the new model. Observe that the professionalism score has increased! [13]: with mlflow . start_run () as run : system_prompt = \"Answer the following question using extreme formality.\" professional_qa_model = mlflow . openai . log_model ( model = \"gpt-4o-mini\" , task = openai . chat . completions , artifact_path = \"model\" , messages = [ { \"role\" : \"system\" , \"content\" : system_prompt }, { \"role\" : \"user\" , \"content\" : \" {question} \" }, ], ) results = ",
        "id": "fd3b41d12709bef5ec134dc59577529e"
    },
    {
        "text": "system prompt. Call mlflow.evaluate() using the new model. Observe that the professionalism score has increased! [13]: with mlflow . start_run () as run : system_prompt = \"Answer the following question using extreme formality.\" professional_qa_model = mlflow . openai . log_model ( model = \"gpt-4o-mini\" , task = openai . chat . completions , artifact_path = \"model\" , messages = [ { \"role\" : \"system\" , \"content\" : system_prompt }, { \"role\" : \"user\" , \"content\" : \" {question} \" }, ], ) results = mlflow . evaluate ( professional_qa_model . model_uri , eval_df , model_type = \"question-answering\" , evaluators = \"default\" , extra_metrics = [ professionalism_metric ], ) print ( results . metrics ) /Users/sunish.sheth/.local/lib/python3.8/site-packages/_distutils_hack/__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n  warnings.warn(\n/Users/sunish.sheth/.local/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\n2023/10/27 00:57:30 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n2023/10/27 00:57:30 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n2023/10/27 00:57:37 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: token_count\n2023/10/27 00:57:37 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: toxicity\n2023/10/27 00:57:38 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: flesch_kincaid_grade_level\n2023/10/27 00:57:38 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: ari_grade_level\n2023/10/27 00:57:38 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: exact_match\n2023/10/27 00:57:38 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: professionalism {'toxicity/v1/mean': 0.00030383203556993976, 'toxicity/v1/variance': 9.482036560896618e-09, 'toxicity/v1/p90': 0.0003866828687023372, 'toxicity/v1/ratio': 0.0, 'flesch_kincaid_grade_level/v1/mean': 17.625, 'flesch_kincaid_grade_level/v1/variance': 2.9068750000000003, 'flesch_kincaid_grade_level/v1/p90': 19.54, 'ari_grade_level/v1/mean': 21.425, 'ari_grade_level/v1/variance': 3.6168750000000007, 'ari_grade_level/v1/p90': 23.6, 'professionalism/v1/mean': 4.5, 'professionalism/v1/variance': 0.25, 'professionalism/v1/p90': 5.0} [14]: results . tables [ \"eval_results_table\" ] [14]: inputs ground_truth outputs token_count toxicity/v1/score flesch_kincaid_grade_level/v1/score ari_grade_level/v1/score professionalism/v1/score professionalism/v1/justification 0 How does useEffect() work? The useEffect() hook tells React that your com... Certainly, I shall elucidate the mechanics of ... 386 0.000398 16.3 19.7 5 The response is written in an excessively form... 1 What does the static keyword in a function mean? Static members belongs to the class, rather th... The static keyword utilized in the context of ... 73 0.000143 16.4 20.0 4 The language used in the output is formal and ... 2 What does the 'finally' block in Python do? 'Finally' defines a block of code to run when ... The 'finally' block in Python serves as an int... 97 0.000313 20.5 24.5 4 The language used in the output is formal and ... 3 What is the difference between multiprocessing... Multithreading refers to the ability of a proc... Allow me to elucidate upon the distinction bet... 324 0.000361 17.3 21.5 5 The response is written in an excessively form... [ ]: Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "749110b24d0a480bc7eef45d7942edfd"
    },
    {
        "text": "LLM RAG Evaluation with MLflow Example Notebook 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow Retrieval Augmented Generation (RAG) Deploying Advanced LLMs with Custom PyFuncs in MLflow LLM Evaluation Examples LLM Evaluation with MLflow Example Notebook LLM RAG Evaluation with MLflow Example Notebook LLM RAG Evaluation with MLflow using llama2-as-judge Example Notebook Create a RAG system Evaluate the RAG system using mlflow.evaluate() Evaluate a Hugging Face LLM with mlflow.evaluate() QA Evaluation Tutorial RAG Evaluation Tutorials Tutorial: Getting Started with ChatModel Tutorial: Custom GenAI Models using ChatModel Build a tool-calling model with mlflow.pyfunc.ChatModel MLflow Trace UI in Jupyter Notebook Demo MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs LLM Evaluation Examples LLM RAG Evaluation with MLflow Example Notebook  LLM RAG Evaluation with MLflow Example Notebook In this notebook, we will demonstrate how to evaluate various a RAG system with MLflow. Download this Notebook We need to set our OpenAI API key. In order to set your private key safely, please be sure to either export your key through a command-line terminal for your current instance, or, for a permanent addition to all user-based sessions, configure your favored environment management configuration file (i.e., .bashrc, .zshrc) to have the following entry: OPENAI_API_KEY=<your openai API key> If using Azure OpenAI, you will instead need to set OPENAI_API_TYPE=\"azure\" OPENAI_API_VERSION=<YYYY-MM-DD> OPENAI_API_KEY=<https://<>.<>.<>.com> OPENAI_API_DEPLOYMENT_NAME=<deployment name>  Notebook compatibility With rapidly changing libraries such as langchain , examples can become outdated rather quickly and will no longer work. For the purposes of demonstration, here are the critical dependencies that are recommended to use to effectively run this notebook: Package Version langchain 0.1.16 lanchain-community 0.0.33 langchain-openai 0.0.8 openai 1.12.0 mlflow 2.12.1 chromadb 0.4.24 If you attempt to execute this notebook with different versions, it may function correctly, but it is recommended to use the precise versions above to ensure that your code executes properly.  Create a RAG system Use Langchain and Chroma to create a RAG system that answers questions based on the MLflow documentation. [4]: import pandas as pd from langchain.chains import RetrievalQA from langchain.document_loaders import WebBaseLoader from langchain.text_splitter import CharacterTextSplitter from langchain.vectorstores import Chroma from langchain_openai import OpenAI , OpenAIEmbeddings import mlflow [5]: loader = WebBaseLoader ( \"https://mlflow.org/docs/latest/index.html\" ) documents = loader . load () text_splitter = CharacterTextSplitter ( chunk_size = 1000 , chunk_overlap = 0 ) texts = text_splitter . split_documents ( documents ) embeddings = OpenAIEmbeddings () docsearch = Chroma . from_documents ( texts , embeddings ) qa = RetrievalQA . from_chain_type ( llm = OpenAI ( temperature = 0 ), chain_type = \"stuff\" , retriever = docsearch . as_retriever (), return_source_documents = True , ) ",
        "id": "95861ce3184939fcf246f29e87b4273e"
    },
    {
        "text": " Evaluate the RAG system using mlflow.evaluate() Create a simple function that runs each input through the RAG chain [6]: def model ( input_df ): answer = [] for index , row in input_df . iterrows (): answer . append ( qa ( row [ \"questions\" ])) return answer Create an eval dataset [8]: eval_df = pd . DataFrame ( { \"questions\" : [ \"What is MLflow?\" , \"How to run mlflow.evaluate()?\" , \"How to log_table()?\" , \"How to load_table()?\" , ], } ) Create a faithfulness metric [9]: from mlflow.metrics.genai import EvaluationExample , faithfulness # Create a good and bad example for faithfulness in the context of this problem faithfulness_examples = [ EvaluationExample ( input = \"How do I disable MLflow autologging?\" , output = \"mlflow.autolog(disable=True) will disable autologging for all functions. In Databricks, autologging is enabled by default. \" , score = 2 , justification = \"The output provides a working solution, using the mlflow.autolog() function that is provided in the context.\" , grading_context = { \"context\" : \"mlflow.autolog(log_input_examples: bool = False, log_model_signatures: bool = True, log_models: bool = True, log_datasets: bool = True, disable: bool = False, exclusive: bool = False, disable_for_unsupported_versions: bool = False, silent: bool = False, extra_tags: Optional[Dict[str, str]] = None) \u00e2\u0086\u0092 None[source] Enables (or disables) and configures autologging for all supported integrations. The parameters are passed to any autologging integrations that support them. See the tracking docs for a list of supported autologging integrations. Note that framework-specific configurations set at any point will take precedence over any configurations set by this function.\" }, ), EvaluationExample ( input = \"How do I disable MLflow autologging?\" , output = \"mlflow.autolog(disable=True) will disable autologging for all functions.\" , score = 5 , justification = \"The output provides a solution that is using the mlflow.autolog() function that is provided in the context.\" , grading_context = { \"context\" : \"mlflow.autolog(log_input_examples: bool = False, log_model_signatures: bool = True, log_models: bool = True, log_datasets: bool = True, disable: bool = False, exclusive: bool = False, disable_for_unsupported_versions: bool = False, silent: bool = False, extra_tags: Optional[Dict[str, str]] = None) \u00e2\u0086\u0092 None[source] Enables (or disables) and configures autologging for all supported integrations. The parameters are passed to any autologging integrations that support them. See the tracking docs for a list of supported autologging integrations. Note that framework-specific configurations set at any point will take precedence over any configurations set by this function.\" }, ), ] faithfulness_metric = faithfulness ( model = \"openai:/gpt-4\" , examples = faithfulness_examples ) print ( faithfulness_metric ) EvaluationMetric(name=faithfulness, greater_is_better=True, long_name=faithfulness, version=v1, metric_details=\nTask:\nYou must return the following fields in your response one below the other:\nscore: Your numerical score for the model's faithfulness based on the rubric\njustification: Your step-by-step reasoning about the model's faithfulness score\n\nYou are an impartial judge. You will be given an input that was sent to a machine\nlearning model, and you will be given an output that the model produced. You\nmay also be given additional information that was used by the model to generate the output.\n\nYour task is to determine a numerical score called faithfulness based on the input and output.\nA definition of faithfulness and a grading rubric are provided below.\nYou must use the grading rubric to determine your score. You must also justify your score.\n\nExamples could be included below for reference. Make sure to use them as references and to\nunderstand them before completing the task.\n\nInput:\n{input}\n\nOutput:\n{output}\n\n{grading_context_columns}\n\nMetric definition:\nFaithfulness is only evaluated with the provided output and provided context, please ignore the provided input entirely when scoring faithfulness. Faithfulness assesses how much of the provided output is factually consistent with the provided context. A higher score indicates that a higher proportion of claims present in the output can be derived from the provided context. Faithfulness does not consider how much extra information from the context is not prese",
        "id": "609257885ee49c81bb8e2d8e4ad891ed"
    },
    {
        "text": "ns}\n\nMetric definition:\nFaithfulness is only evaluated with the provided output and provided context, please ignore the provided input entirely when scoring faithfulness. Faithfulness assesses how much of the provided output is factually consistent with the provided context. A higher score indicates that a higher proportion of claims present in the output can be derived from the provided context. Faithfulness does not consider how much extra information from the context is not present in the output.\n\nGrading rubric:\nFaithfulness: Below are the details for different scores:\n- Score 1: None of the claims in the output can be inferred from the provided context.\n- Score 2: Some of the claims in the output can be inferred from the provided context, but the majority of the output is missing from, inconsistent with, or contradictory to the provided context.\n- Score 3: Half or more of the claims in the output can be inferred from the provided context.\n- Score 4: Most of the claims in the output can be inferred from the provided context, with very little information that is not directly supported by the provided context.\n- Score 5: All of the claims in the output are directly supported by the provided context, demonstrating high faithfulness to the provided context.\n\nExamples:\n\nExample Input:\nHow do I disable MLflow autologging?\n\nExample Output:\nmlflow.autolog(disable=True) will disable autologging for all functions. In Databricks, autologging is enabled by default.\n\nAdditional information used by the model:\nkey: context\nvalue:\nmlflow.autolog(log_input_examples: bool = False, log_model_signatures: bool = True, log_models: bool = True, log_datasets: bool = True, disable: bool = False, exclusive: bool = False, disable_for_unsupported_versions: bool = False, silent: bool = False, extra_tags: Optional[Dict[str, str]] = None) \u00e2\u0086\u0092 None[source] Enables (or disables) and configures autologging for all supported integrations. The parameters are passed to any autologging integrations that support them. See the tracking docs for a list of supported autologging integrations. Note that framework-specific configurations set at any point will take precedence over any configurations set by this function.\n\nExample score: 2\nExample justification: The output provides a working solution, using the mlflow.autolog() function that is provided in the context.\n\n\nExample Input:\nHow do I disable MLflow autologging?\n\nExample Output:\nmlflow.autolog(disable=True) will disable autologging for all functions.\n\nAdditional information used by the model:\nkey: context\nvalue:\nmlflow.autolog(log_input_examples: bool = False, log_model_signatures: bool = True, log_models: bool = True, log_datasets: bool = True, disable: bool = False, exclusive: bool = False, disable_for_unsupported_versions: bool = False, silent: bool = False, extra_tags: Optional[Dict[str, str]] = None) \u00e2\u0086\u0092 None[source] Enables (or disables) and configures autologging for all supported integrations. The parameters are passed to any autologging integrations that support them. See the tracking docs for a list of supported autologging integrations. Note that framework-specific configurations set at any point will take precedence over any configurations set by this function.\n\nExample score: 5\nExample justification: The output provides a solution that is using the mlflow.autolog() function that is provided in the context.\n\n\nYou must return the following fields in your response one below the other:\nscore: Your numerical score for the model's faithfulness based on the rubric\njustification: Your step-by-step reasoning about the model's faithfulness score\n    ) Create a relevance metric. You can see the full grading prompt by printing the metric or by accessing the metric_details attribute of the metric. [10]: from mlflow.metrics.genai import EvaluationExample , relevance relevance_metric = relevance ( model = \"openai:/gpt-4\" ) print ( relevance_metric ) EvaluationMetric(name=relevance, greater_is_better=True, long_name=relevance, version=v1, metric_details=\nTask:\nYou must return the following fields in your response one below the other:\nscore: Your numerical score for the model's relevance based on the rubric\njustification: Your step-by-step reasoning about the model's relevance score\n\nYou are an impartial judge. You will be given an input that was sent to a machine\nlearning model, and you will ",
        "id": "cf3b5494bca0c85042f4d612120b75ab"
    },
    {
        "text": "/gpt-4\" ) print ( relevance_metric ) EvaluationMetric(name=relevance, greater_is_better=True, long_name=relevance, version=v1, metric_details=\nTask:\nYou must return the following fields in your response one below the other:\nscore: Your numerical score for the model's relevance based on the rubric\njustification: Your step-by-step reasoning about the model's relevance score\n\nYou are an impartial judge. You will be given an input that was sent to a machine\nlearning model, and you will be given an output that the model produced. You\nmay also be given additional information that was used by the model to generate the output.\n\nYour task is to determine a numerical score called relevance based on the input and output.\nA definition of relevance and a grading rubric are provided below.\nYou must use the grading rubric to determine your score. You must also justify your score.\n\nExamples could be included below for reference. Make sure to use them as references and to\nunderstand them before completing the task.\n\nInput:\n{input}\n\nOutput:\n{output}\n\n{grading_context_columns}\n\nMetric definition:\nRelevance encompasses the appropriateness, significance, and applicability of the output with respect to both the input and context. Scores should reflect the extent to which the output directly addresses the question provided in the input, given the provided context.\n\nGrading rubric:\nRelevance: Below are the details for different scores:- Score 1: The output doesn't mention anything about the question or is completely irrelevant to the provided context.\n- Score 2: The output provides some relevance to the question and is somehow related to the provided context.\n- Score 3: The output mostly answers the question and is largely consistent with the provided context.\n- Score 4: The output answers the question and is consistent with the provided context.\n- Score 5: The output answers the question comprehensively using the provided context.\n\nExamples:\n\nExample Input:\nHow is MLflow related to Databricks?\n\nExample Output:\nDatabricks is a data engineering and analytics platform designed to help organizations process and analyze large amounts of data. Databricks is a company specializing in big data and machine learning solutions.\n\nAdditional information used by the model:\nkey: context\nvalue:\nMLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.\n\nExample score: 2\nExample justification: The output provides relevant information about Databricks, mentioning it as a company specializing in big data and machine learning solutions. However, it doesn't directly address how MLflow is related to Databricks, which is the specific question asked in the input. Therefore, the output is only somewhat related to the provided context.\n\n\nExample Input:\nHow is MLflow related to Databricks?\n\nExample Output:\nMLflow is a product created by Databricks to enhance the efficiency of machine learning processes.\n\nAdditional information used by the model:\nkey: context\nvalue:\nMLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.\n\nExample score: 4\nExample justification: The output provides a relevant and accurate statement about the relationship between MLflow and Databricks. While it doesn't provide extensive detail, it still offers a substantial and meaningful response. To achieve a score of 5, the response could be further improved by providing additional context or details about how MLflow specifically functions within the Databricks ecosystem.\n\n\nYou must return the following fields in your response one below the other:\nscore: Your numerical score for the model's relevance based on the rubric\njustification: Your step-by-step reasoning about the model's relevance score\n    ) [11]: results = mlflow . evaluate ( model , eval_df , model_type = \"question-answeri",
        "id": "14a4b6076ee716b6d87dc1bc6a958b8f"
    },
    {
        "text": "a score of 5, the response could be further improved by providing additional context or details about how MLflow specifically functions within the Databricks ecosystem.\n\n\nYou must return the following fields in your response one below the other:\nscore: Your numerical score for the model's relevance based on the rubric\njustification: Your step-by-step reasoning about the model's relevance score\n    ) [11]: results = mlflow . evaluate ( model , eval_df , model_type = \"question-answering\" , evaluators = \"default\" , predictions = \"result\" , extra_metrics = [ faithfulness_metric , relevance_metric , mlflow . metrics . latency ()], evaluator_config = { \"col_mapping\" : { \"inputs\" : \"questions\" , \"context\" : \"source_documents\" , } }, ) print ( results . metrics ) 2023/11/16 09:05:21 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n2023/11/16 09:05:21 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n2023/11/16 09:05:28 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\nUsing default facebook/roberta-hate-speech-dynabench-r4-target checkpoint 2023/11/16 09:05:58 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: token_count\n2023/11/16 09:05:58 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: toxicity\n2023/11/16 09:05:58 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: flesch_kincaid_grade_level\n2023/11/16 09:05:58 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: ari_grade_level\n2023/11/16 09:05:58 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: exact_match\n2023/11/16 09:05:58 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: faithfulness 2023/11/16 09:06:12 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: relevance {'toxicity/v1/mean': 0.00022622970209340565, 'toxicity/v1/variance': 3.84291113351624e-09, 'toxicity/v1/p90': 0.0002859298692783341, 'toxicity/v1/ratio': 0.0, 'flesch_kincaid_grade_level/v1/mean': 8.1, 'flesch_kincaid_grade_level/v1/variance': 8.815, 'flesch_kincaid_grade_level/v1/p90': 11.48, 'ari_grade_level/v1/mean': 11.649999999999999, 'ari_grade_level/v1/variance': 19.527499999999993, 'ari_grade_level/v1/p90': 16.66, 'faithfulness/v1/mean': 4.0, 'faithfulness/v1/variance': 3.0, 'faithfulness/v1/p90': 5.0, 'relevance/v1/mean': 4.5, 'relevance/v1/variance': 0.25, 'relevance/v1/p90': 5.0} [13]: results . tables [ \"eval_results_table\" ] [13]: questions outputs source_documents latency token_count toxicity/v1/score flesch_kincaid_grade_level/v1/score ari_grade_level/v1/score faithfulness/v1/score faithfulness/v1/justification relevance/v1/score relevance/v1/justification 0 What is MLflow? MLflow is an open-source platform, purpose-bu... [{'lc_attributes': {}, 'lc_namespace': ['langc... 1.989822 53 0.000137 12.5 18.4 5 The output provided by the model is a direct e... 5 The output provides a comprehensive answer to ... 1 How to run mlflow.evaluate()? The mlflow.evaluate() API allows you to valid... [{'lc_attributes': {}, 'lc_namespace': ['langc... 1.945368 55 0.000200 9.1 12.6 5 The output provided by the model is completely... 4 The output provides a relevant and accurate ex... 2 How to log_table()? You can log a table with MLflow using the log... [{'lc_attributes': {}, 'lc_namespace': ['langc... 1.521511 32 0.000289 5.0 6.8 1 The output claims that you can log a table wit... 5 The output provides a comprehensive answer to ... 3 How to load_table()? You can't load_table() with MLflow. MLflow is... [{'lc_attributes': {}, 'lc_namespace': ['langc... 1.105279 27 0.000279 5.8 8.8 5 The output claim that \"You can't load_table() ... 4 The output provides a relevant and accurate re... [ ]: Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "01d90b71879a6f70a8a3a42a7a4ffe32"
    },
    {
        "text": "LLM RAG Evaluation with MLflow using llama2-as-judge Example Notebook 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow Retrieval Augmented Generation (RAG) Deploying Advanced LLMs with Custom PyFuncs in MLflow LLM Evaluation Examples LLM Evaluation with MLflow Example Notebook LLM RAG Evaluation with MLflow Example Notebook LLM RAG Evaluation with MLflow using llama2-as-judge Example Notebook Create a RAG system Evaluate the RAG system using mlflow.evaluate() Evaluate a Hugging Face LLM with mlflow.evaluate() QA Evaluation Tutorial RAG Evaluation Tutorials Tutorial: Getting Started with ChatModel Tutorial: Custom GenAI Models using ChatModel Build a tool-calling model with mlflow.pyfunc.ChatModel MLflow Trace UI in Jupyter Notebook Demo MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs LLM Evaluation Examples LLM RAG Evaluation with MLflow using llama2-as-judge Example Notebook  LLM RAG Evaluation with MLflow using llama2-as-judge Example Notebook In this notebook, we will demonstrate how to evaluate various a RAG system with MLflow. We will use llama2-70b as the judge model, via a Databricks serving endpoint. Download this Notebook  Notebook compatibility With rapidly changing libraries such as langchain , examples can become outdated rather quickly and will no longer work. For the purposes of demonstration, here are the critical dependencies that are recommended to use to effectively run this notebook: Package Version langchain 0.1.16 lanchain-community 0.0.33 langchain-openai 0.0.8 openai 1.12.0 mlflow 2.12.1 If you attempt to execute this notebook with different versions, it may function correctly, but it is recommended to use the precise versions above to ensure that your code executes properly.  Installing Requirements Before proceeding with this tutorial, ensure that your versions of the installed packages meet the requirements listed above. pip install langchain == 0 .1.16 langchain-community == 0 .0.33 langchain-openai == 0 .0.8 openai == 1 .12.0  Configuration We need to set our OpenAI API key. In order to set your private key safely, please be sure to either export your key through a command-line terminal for your current instance, or, for a permanent addition to all user-based sessions, configure your favored environment management configuration file (i.e., .bashrc, .zshrc) to have the following entry: OPENAI_API_KEY=<your openai API key> In order to run this notebook, using a Databricks hosted Llama2 model, you will need to set your host and personal access token. Please ensure that these are set either using the Databricks SDK or setting the environment variables: DATABRICKS_HOST=<your Databricks workspace URI> DATABRICKS_TOKEN=<your personal access token> [1]: import pandas as pd from langchain.chains import RetrievalQA from langchain.document_loaders import WebBaseLoader from langchain.text_splitter import CharacterTextSplitter from langchain.vectorstores import Chroma from langchain_openai import OpenAI , OpenAIEmbeddings import mlflow from mlflow.deployments import set_deployments_target from mlflow.metrics.genai import EvaluationExample , faithfulness , relevance Set the deployment target to \u00e2\u0080\u009cdatabricks\u00e2\u0080\u009d for use with Databricks served models. [2]: set_deployments_target ( \"databricks\" ) ",
        "id": "6406797b2660f5edf2723520aa4c64f9"
    },
    {
        "text": " Create a RAG system Use Langchain and Chroma to create a RAG system that answers questions based on the MLflow documentation. [3]: loader = WebBaseLoader ( \"https://mlflow.org/docs/latest/index.html\" ) documents = loader . load () text_splitter = CharacterTextSplitter ( chunk_size = 1000 , chunk_overlap = 0 ) texts = text_splitter . split_documents ( documents ) embeddings = OpenAIEmbeddings () docsearch = Chroma . from_documents ( texts , embeddings ) qa = RetrievalQA . from_chain_type ( llm = OpenAI ( temperature = 0 ), chain_type = \"stuff\" , retriever = docsearch . as_retriever (), return_source_documents = True , ) ",
        "id": "36961633267c9d4e836109e89fd540b7"
    },
    {
        "text": " Evaluate the RAG system using mlflow.evaluate() Create a simple function that runs each input through the RAG chain [4]: def model ( input_df ): answer = [] for index , row in input_df . iterrows (): answer . append ( qa ( row [ \"questions\" ])) return answer Create an eval dataset [5]: eval_df = pd . DataFrame ( { \"questions\" : [ \"What is MLflow?\" , \"How to run mlflow.evaluate()?\" , \"How to log_table()?\" , \"How to load_table()?\" , ], } ) Create a faithfulness metric using databricks-llama2-70b-chat as the judge [6]: # Create a good and bad example for faithfulness in the context of this problem faithfulness_examples = [ EvaluationExample ( input = \"How do I disable MLflow autologging?\" , output = \"mlflow.autolog(disable=True) will disable autologging for all functions. In Databricks, autologging is enabled by default. \" , score = 2 , justification = \"The output provides a working solution, using the mlflow.autolog() function that is provided in the context.\" , grading_context = { \"context\" : \"mlflow.autolog(log_input_examples: bool = False, log_model_signatures: bool = True, log_models: bool = True, log_datasets: bool = True, disable: bool = False, exclusive: bool = False, disable_for_unsupported_versions: bool = False, silent: bool = False, extra_tags: Optional[Dict[str, str]] = None) \u00e2\u0086\u0092 None[source] Enables (or disables) and configures autologging for all supported integrations. The parameters are passed to any autologging integrations that support them. See the tracking docs for a list of supported autologging integrations. Note that framework-specific configurations set at any point will take precedence over any configurations set by this function.\" }, ), EvaluationExample ( input = \"How do I disable MLflow autologging?\" , output = \"mlflow.autolog(disable=True) will disable autologging for all functions.\" , score = 5 , justification = \"The output provides a solution that is using the mlflow.autolog() function that is provided in the context.\" , grading_context = { \"context\" : \"mlflow.autolog(log_input_examples: bool = False, log_model_signatures: bool = True, log_models: bool = True, log_datasets: bool = True, disable: bool = False, exclusive: bool = False, disable_for_unsupported_versions: bool = False, silent: bool = False, extra_tags: Optional[Dict[str, str]] = None) \u00e2\u0086\u0092 None[source] Enables (or disables) and configures autologging for all supported integrations. The parameters are passed to any autologging integrations that support them. See the tracking docs for a list of supported autologging integrations. Note that framework-specific configurations set at any point will take precedence over any configurations set by this function.\" }, ), ] faithfulness_metric = faithfulness ( model = \"endpoints:/databricks-llama-2-70b-chat\" , examples = faithfulness_examples ) print ( faithfulness_metric ) EvaluationMetric(name=faithfulness, greater_is_better=True, long_name=faithfulness, version=v1, metric_details=\nTask:\nYou must return the following fields in your response in two lines, one below the other:\nscore: Your numerical score for the model's faithfulness based on the rubric\njustification: Your reasoning about the model's faithfulness score\n\nYou are an impartial judge. You will be given an input that was sent to a machine\nlearning model, and you will be given an output that the model produced. You\nmay also be given additional information that was used by the model to generate the output.\n\nYour task is to determine a numerical score called faithfulness based on the input and output.\nA definition of faithfulness and a grading rubric are provided below.\nYou must use the grading rubric to determine your score. You must also justify your score.\n\nExamples could be included below for reference. Make sure to use them as references and to\nunderstand them before completing the task.\n\nInput:\n{input}\n\nOutput:\n{output}\n\n{grading_context_columns}\n\nMetric definition:\nFaithfulness is only evaluated with the provided output and provided context, please ignore the provided input entirely when scoring faithfulness. Faithfulness assesses how much of the provided output is factually consistent with the provided context. A higher score indicates that a higher proportion of claims present in the output can be derived from the provided context. Faithfulness does not consider how much extra information from the context is not present in the output.\n\nGrading rubric:\nFaithfulness: Below are the details for different scores:\n- Score 1: None of",
        "id": "490e194031f7c2b7128ec3afa9d7a4be"
    },
    {
        "text": "xt, please ignore the provided input entirely when scoring faithfulness. Faithfulness assesses how much of the provided output is factually consistent with the provided context. A higher score indicates that a higher proportion of claims present in the output can be derived from the provided context. Faithfulness does not consider how much extra information from the context is not present in the output.\n\nGrading rubric:\nFaithfulness: Below are the details for different scores:\n- Score 1: None of the claims in the output can be inferred from the provided context.\n- Score 2: Some of the claims in the output can be inferred from the provided context, but the majority of the output is missing from, inconsistent with, or contradictory to the provided context.\n- Score 3: Half or more of the claims in the output can be inferred from the provided context.\n- Score 4: Most of the claims in the output can be inferred from the provided context, with very little information that is not directly supported by the provided context.\n- Score 5: All of the claims in the output are directly supported by the provided context, demonstrating high faithfulness to the provided context.\n\nExamples:\n\nExample Output:\nmlflow.autolog(disable=True) will disable autologging for all functions. In Databricks, autologging is enabled by default.\n\nAdditional information used by the model:\nkey: context\nvalue:\nmlflow.autolog(log_input_examples: bool = False, log_model_signatures: bool = True, log_models: bool = True, log_datasets: bool = True, disable: bool = False, exclusive: bool = False, disable_for_unsupported_versions: bool = False, silent: bool = False, extra_tags: Optional[Dict[str, str]] = None) \u00e2\u0086\u0092 None[source] Enables (or disables) and configures autologging for all supported integrations. The parameters are passed to any autologging integrations that support them. See the tracking docs for a list of supported autologging integrations. Note that framework-specific configurations set at any point will take precedence over any configurations set by this function.\n\nExample score: 2\nExample justification: The output provides a working solution, using the mlflow.autolog() function that is provided in the context.\n\n\nExample Output:\nmlflow.autolog(disable=True) will disable autologging for all functions.\n\nAdditional information used by the model:\nkey: context\nvalue:\nmlflow.autolog(log_input_examples: bool = False, log_model_signatures: bool = True, log_models: bool = True, log_datasets: bool = True, disable: bool = False, exclusive: bool = False, disable_for_unsupported_versions: bool = False, silent: bool = False, extra_tags: Optional[Dict[str, str]] = None) \u00e2\u0086\u0092 None[source] Enables (or disables) and configures autologging for all supported integrations. The parameters are passed to any autologging integrations that support them. See the tracking docs for a list of supported autologging integrations. Note that framework-specific configurations set at any point will take precedence over any configurations set by this function.\n\nExample score: 5\nExample justification: The output provides a solution that is using the mlflow.autolog() function that is provided in the context.\n\n\nYou must return the following fields in your response in two lines, one below the other:\nscore: Your numerical score for the model's faithfulness based on the rubric\njustification: Your reasoning about the model's faithfulness score\n\nDo not add additional new lines. Do not add any other fields.\n    ) Create a relevance metric using databricks-llama2-70b-chat as the judge [7]: relevance_metric = relevance ( model = \"endpoints:/databricks-llama-2-70b-chat\" ) print ( relevance_metric ) EvaluationMetric(name=relevance, greater_is_better=True, long_name=relevance, version=v1, metric_details=\nTask:\nYou must return the following fields in your response in two lines, one below the other:\nscore: Your numerical score for the model's relevance based on the rubric\njustification: Your reasoning about the model's relevance score\n\nYou are an impartial judge. You will be given an input that was sent to a machine\nlearning model, and you will be given an output that the model produced. You\nmay also be given additional information that was used by the model to generate the output.\n\nYour task is to determine a numerical score called relevance based on the input and output.\nA definition of relevance and a grading rubric are provided below.\nYou must use the grading rubric to determine your score. You must als",
        "id": "c1def2b4819466bb9a3f4ee33b5ed366"
    },
    {
        "text": "'s relevance score\n\nYou are an impartial judge. You will be given an input that was sent to a machine\nlearning model, and you will be given an output that the model produced. You\nmay also be given additional information that was used by the model to generate the output.\n\nYour task is to determine a numerical score called relevance based on the input and output.\nA definition of relevance and a grading rubric are provided below.\nYou must use the grading rubric to determine your score. You must also justify your score.\n\nExamples could be included below for reference. Make sure to use them as references and to\nunderstand them before completing the task.\n\nInput:\n{input}\n\nOutput:\n{output}\n\n{grading_context_columns}\n\nMetric definition:\nRelevance encompasses the appropriateness, significance, and applicability of the output with respect to both the input and context. Scores should reflect the extent to which the output directly addresses the question provided in the input, given the provided context.\n\nGrading rubric:\nRelevance: Below are the details for different scores:- Score 1: The output doesn't mention anything about the question or is completely irrelevant to the provided context.\n- Score 2: The output provides some relevance to the question and is somehow related to the provided context.\n- Score 3: The output mostly answers the question and is largely consistent with the provided context.\n- Score 4: The output answers the question and is consistent with the provided context.\n- Score 5: The output answers the question comprehensively using the provided context.\n\nExamples:\n\nExample Input:\nHow is MLflow related to Databricks?\n\nExample Output:\nDatabricks is a data engineering and analytics platform designed to help organizations process and analyze large amounts of data. Databricks is a company specializing in big data and machine learning solutions.\n\nAdditional information used by the model:\nkey: context\nvalue:\nMLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.\n\nExample score: 2\nExample justification: The output provides relevant information about Databricks, mentioning it as a company specializing in big data and machine learning solutions. However, it doesn't directly address how MLflow is related to Databricks, which is the specific question asked in the input. Therefore, the output is only somewhat related to the provided context.\n\n\nExample Input:\nHow is MLflow related to Databricks?\n\nExample Output:\nMLflow is a product created by Databricks to enhance the efficiency of machine learning processes.\n\nAdditional information used by the model:\nkey: context\nvalue:\nMLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.\n\nExample score: 4\nExample justification: The output provides a relevant and accurate statement about the relationship between MLflow and Databricks. While it doesn't provide extensive detail, it still offers a substantial and meaningful response. To achieve a score of 5, the response could be further improved by providing additional context or details about how MLflow specifically functions within the Databricks ecosystem.\n\n\nYou must return the following fields in your response in two lines, one below the other:\nscore: Your numerical score for the model's relevance based on the rubric\njustification: Your reasoning about the model's relevance score\n\nDo not add additional new lines. Do not add any other fields.\n    ) [8]: results = mlflow . evaluate ( model , eval_df , model_type = \"question-answering\" , evaluators = \"default\" , predictions = \"result\" , extra_metrics = [ faithfulness_metric , relevance_metric , mlflow . metrics . latency ()], evaluator_config = { \"col_mapping\" : { \"inputs\" : \"questions\" , \"context\" : \"source_documents\" , } }, ) print ( results . metrics ) 2024/04/23 14:24:36 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n2024/04/23 14:24:36 INFO m",
        "id": "58ba73900c62ebe74e63a9b28608c741"
    },
    {
        "text": "\n    ) [8]: results = mlflow . evaluate ( model , eval_df , model_type = \"question-answering\" , evaluators = \"default\" , predictions = \"result\" , extra_metrics = [ faithfulness_metric , relevance_metric , mlflow . metrics . latency ()], evaluator_config = { \"col_mapping\" : { \"inputs\" : \"questions\" , \"context\" : \"source_documents\" , } }, ) print ( results . metrics ) 2024/04/23 14:24:36 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n2024/04/23 14:24:36 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n/Users/benjamin.wilson/miniconda3/envs/mlflow-dev-env/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n  warn_deprecated(\n2024/04/23 14:24:46 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n2024/04/23 14:24:50 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: RuntimeError(\"Failed to import transformers.pipelines because of the following error (look up to see its traceback):\\ncannot import name 'DEFAULT_CIPHERS' from 'urllib3.util.ssl_' (/Users/benjamin.wilson/miniconda3/envs/mlflow-dev-env/lib/python3.8/site-packages/urllib3/util/ssl_.py)\")), skipping metric logging.\n2024/04/23 14:24:50 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'toxicity' because it returned None.\n2024/04/23 14:24:50 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'exact_match' because it returned None. /Users/benjamin.wilson/miniconda3/envs/mlflow-dev-env/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n  return _methods._mean(a, axis=axis, dtype=dtype,\n/Users/benjamin.wilson/miniconda3/envs/mlflow-dev-env/lib/python3.8/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n  ret = ret.dtype.type(ret / rcount)\n/Users/benjamin.wilson/miniconda3/envs/mlflow-dev-env/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3747: RuntimeWarning: Degrees of freedom <= 0 for slice\n  return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n/Users/benjamin.wilson/miniconda3/envs/mlflow-dev-env/lib/python3.8/site-packages/numpy/core/_methods.py:226: RuntimeWarning: invalid value encountered in divide\n  arrmean = um.true_divide(arrmean, div, out=arrmean,\n/Users/benjamin.wilson/miniconda3/envs/mlflow-dev-env/lib/python3.8/site-packages/numpy/core/_methods.py:261: RuntimeWarning: invalid value encountered in scalar divide\n  ret = ret.dtype.type(ret / rcount) 2024/04/23 14:24:50 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: RuntimeError(\"Failed to import transformers.pipelines because of the following error (look up to see its traceback):\\ncannot import name 'DEFAULT_CIPHERS' from 'urllib3.util.ssl_' (/Users/benjamin.wilson/miniconda3/envs/mlflow-dev-env/lib/python3.8/site-packages/urllib3/util/ssl_.py)\")), skipping metric logging.\n2024/04/23 14:24:50 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'toxicity' because it returned None.\n2024/04/23 14:24:50 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'exact_match' because it returned None. {'latency/mean': 2.329627513885498, 'latency/variance': 6.333362589765358, 'latency/p90': 5.018124270439149, 'flesch_kincaid_grade_level/v1/mean': 3.7, 'flesch_kincaid_grade_level/v1/variance': 42.96, 'flesch_kincaid_grade_level/v1/p90': 10.9, 'ari_grade_level/v1/mean': 5.25, 'ari_grade_level/v1/variance': 71.20249999999999, 'ari_grade_level/v1/p90': 14.8, 'faithfulness/v1/mean': nan, 'faithfulness/v1/variance': nan, 'relevance/v1/mean': nan, 'relevance/v1/variance': nan} [ ]: results . tables [ \"eval_results_table\" ] Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "b012710a1562a6a1718ea20d540ff84e"
    },
    {
        "text": "Evaluate a Hugging Face LLM with mlflow.evaluate() 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow Retrieval Augmented Generation (RAG) Deploying Advanced LLMs with Custom PyFuncs in MLflow LLM Evaluation Examples LLM Evaluation with MLflow Example Notebook LLM RAG Evaluation with MLflow Example Notebook LLM RAG Evaluation with MLflow using llama2-as-judge Example Notebook Create a RAG system Evaluate the RAG system using mlflow.evaluate() Evaluate a Hugging Face LLM with mlflow.evaluate() QA Evaluation Tutorial RAG Evaluation Tutorials Tutorial: Getting Started with ChatModel Tutorial: Custom GenAI Models using ChatModel Build a tool-calling model with mlflow.pyfunc.ChatModel MLflow Trace UI in Jupyter Notebook Demo MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs LLM Evaluation Examples Evaluate a Hugging Face LLM with mlflow.evaluate()  Evaluate a Hugging Face LLM with mlflow.evaluate() This guide will show how to load a pre-trained Hugging Face pipeline, log it to MLflow, and use mlflow.evaluate() to evaluate builtin metrics as well as custom LLM-judged metrics for the model. For detailed information, please read the documentation on using MLflow evaluate . Download this Notebook  Start MLflow Server You can either: Start a local tracking server by running mlflow ui within the same directory that your notebook is in. Use a tracking server, as described in this overview .  Install necessary dependencies [ ]: % pip install -q mlflow transformers torch torchvision evaluate datasets openai tiktoken fastapi rouge_score textstat [2]: # Necessary imports import warnings import pandas as pd from datasets import load_dataset from transformers import pipeline import mlflow from mlflow.metrics.genai import EvaluationExample , answer_correctness , make_genai_metric [3]: # Disable FutureWarnings warnings . filterwarnings ( \"ignore\" , category = FutureWarning )  Load a pretrained Hugging Face pipeline Here we are loading a text generation pipeline, but you can also use either a text summarization or question answering pipeline. [4]: mpt_pipeline = pipeline ( \"text-generation\" , model = \"mosaicml/mpt-7b-chat\" )  Log the model using MLflow We log our pipeline as an MLflow Model, which follows a standard format that lets you save a model in different \u00e2\u0080\u009cflavors\u00e2\u0080\u009d that can be understood by different downstream tools. In this case, the model is of the transformers \u00e2\u0080\u009cflavor\u00e2\u0080\u009d. [5]: mlflow . set_experiment ( \"Evaluate Hugging Face Text Pipeline\" ) # Define the signature signature = mlflow . models . infer_signature ( model_input = \"What are the three primary colors?\" , model_output = \"The three primary colors are red, yellow, and blue.\" , ) # Log the model using mlflow with mlflow . start_run (): model_info = mlflow . transformers . log_model ( transformers_model = mpt_pipeline , artifact_path = \"mpt-7b\" , signature = signature , registered_model_name = \"mpt-7b-chat\" , ) Successfully registered model 'mpt-7b-chat'.\nCreated version '1' of model 'mpt-7b-chat'. ",
        "id": "935e9f480ca0134b15a4d95062532a01"
    },
    {
        "text": " Load Evaluation Data Load in a dataset from Hugging Face Hub to use for evaluation. The data fields in the dataset below represent: instruction : Describes the task that the model should perform. Each row within the dataset is a unique instruction (task) to be performed. input : Optional contextual information that relates to the task defined in the instruction field. For example, for the instruction \u00e2\u0080\u009cIdentify the odd one out\u00e2\u0080\u009d, the input contextual guidance is given as the list of items to select an outlier from, \u00e2\u0080\u009cTwitter, Instagram, Telegram\u00e2\u0080\u009d. output : The answer to the instruction (with the optional input context provided) as generated by the original evaluation model ( text-davinci-003 from OpenAI) text : The final total text as a result of applying the instruction , input , and output to the prompt template used, which is sent to the model for fine tuning purposes. [7]: dataset = load_dataset ( \"tatsu-lab/alpaca\" ) eval_df = pd . DataFrame ( dataset [ \"train\" ]) eval_df . head ( 10 ) [7]: instruction input output text 0 Give three tips for staying healthy. 1.Eat a balanced diet and make sure to include... Below is an instruction that describes a task.... 1 What are the three primary colors? The three primary colors are red, blue, and ye... Below is an instruction that describes a task.... 2 Describe the structure of an atom. An atom is made up of a nucleus, which contain... Below is an instruction that describes a task.... 3 How can we reduce air pollution? There are a number of ways to reduce air pollu... Below is an instruction that describes a task.... 4 Describe a time when you had to make a difficu... I had to make a difficult decision when I was ... Below is an instruction that describes a task.... 5 Identify the odd one out. Twitter, Instagram, Telegram Telegram Below is an instruction that describes a task,... 6 Explain why the following fraction is equivale... 4/16 The fraction 4/16 is equivalent to 1/4 because... Below is an instruction that describes a task,... 7 Write a short story in third person narration ... John was at a crossroads in his life. He had j... Below is an instruction that describes a task.... 8 Render a 3D model of a house <nooutput> This type of instruction cannot be ... Below is an instruction that describes a task.... 9 Evaluate this sentence for spelling and gramma... He finnished his meal and left the resturant He finished his meal and left the restaurant. Below is an instruction that describes a task,...  Define Metrics Since we are evaluating how well our model can provide an answer to a given instruction, we may want to choose some metrics to help measure this on top of any builtin metrics that mlflow.evaluate() gives us. Let\u00e2\u0080\u0099s measure how well our model is doing on the following two metrics: Is the answer correct? Let\u00e2\u0080\u0099s use the predefined metric answer_correctness here. Is the answer fluent, clear, and concise? We will define a custom metric answer_quality to measure this. We will need to pass both of these into the extra_metrics argument for mlflow.evaluate() in order to assess the quality of our model. ",
        "id": "f0f5ac087414e36327821cf4c5767791"
    },
    {
        "text": " What is an Evaluation Metric? An evaluation metric encapsulates any quantitative or qualitative measure you want to calculate for your model. For each model type, mlflow.evaluate() will automatically calculate some set of builtin metrics. Refer here for which builtin metrics will be calculated for each model type. You can also pass in any other metrics you want to calculate as extra metrics. MLflow provides a set of predefined metrics that you\ncan find here , or you can define your own custom metrics. In the example here, we will use the combination of predefined metrics mlflow.metrics.genai.answer_correctness and a custom metric for the quality evaluation. Let\u00e2\u0080\u0099s load our predefined metrics - in this case we are using answer_correctness with GPT-4. [9]: answer_correctness_metric = answer_correctness ( model = \"openai:/gpt-4\" ) Now we want to create a custom LLM-judged metric named answer_quality using make_genai_metric() . We need to define a metric definition and grading rubric, as well as some examples for the LLM judge to use. [8]: # The definition explains what \"answer quality\" entails answer_quality_definition = \"\"\"Please evaluate answer quality for the provided output on the following criteria: fluency, clarity, and conciseness. Each of the criteria is defined as follows: - Fluency measures how naturally and smooth the output reads. - Clarity measures how understandable the output is. - Conciseness measures the brevity and efficiency of the output without compromising meaning. The more fluent, clear, and concise a text, the higher the score it deserves. \"\"\" # The grading prompt explains what each possible score means answer_quality_grading_prompt = \"\"\"Answer quality: Below are the details for different scores: - Score 1: The output is entirely incomprehensible and cannot be read. - Score 2: The output conveys some meaning, but needs lots of improvement in to improve fluency, clarity, and conciseness. - Score 3: The output is understandable but still needs improvement. - Score 4: The output performs well on two of fluency, clarity, and conciseness, but could be improved on one of these criteria. - Score 5: The output reads smoothly, is easy to understand, and clear. There is no clear way to improve the output on these criteria. \"\"\" # We provide an example of a \"bad\" output example1 = EvaluationExample ( input = \"What is MLflow?\" , output = \"MLflow is an open-source platform. For managing machine learning workflows, it \" \"including experiment tracking model packaging versioning and deployment as well as a platform \" \"simplifying for on the ML lifecycle.\" , score = 2 , justification = \"The output is difficult to understand and demonstrates extremely low clarity. \" \"However, it still conveys some meaning so this output deserves a score of 2.\" , ) # We also provide an example of a \"good\" output example2 = EvaluationExample ( input = \"What is MLflow?\" , output = \"MLflow is an open-source platform for managing machine learning workflows, including \" \"experiment tracking, model packaging, versioning, and deployment.\" , score = 5 , justification = \"The output is easily understandable, clear, and concise. It deserves a score of 5.\" , ) answer_quality_metric = make_genai_metric ( name = \"answer_quality\" , definition = answer_quality_definition , grading_prompt = answer_quality_grading_prompt , version = \"v1\" , examples = [ example1 , example2 ], model = \"openai:/gpt-4\" , greater_is_better = True , ) ",
        "id": "4c1d56f83c25af26521bbba94496fa61"
    },
    {
        "text": " Evaluate We need to set our OpenAI API key, since we are using GPT-4 for our LLM-judged metrics. In order to set your private key safely, please be sure to either export your key through a command-line terminal for your current instance, or, for a permanent addition to all user-based sessions, configure your favored environment management configuration file (i.e., .bashrc, .zshrc) to have the following entry: OPENAI_API_KEY=<your openai API key> Now, we can call mlflow.evaluate() . Just to test it out, let\u00e2\u0080\u0099s use the first 10 rows of the data. Using the \"text\" model type, toxicity and readability metrics are calculated as builtin metrics. We also pass in the two metrics we defined above into the extra_metrics parameter to be evaluated. [14]: with mlflow . start_run (): results = mlflow . evaluate ( model_info . model_uri , eval_df . head ( 10 ), evaluators = \"default\" , model_type = \"text\" , targets = \"output\" , extra_metrics = [ answer_correctness_metric , answer_quality_metric ], evaluator_config = { \"col_mapping\" : { \"inputs\" : \"instruction\" }}, ) 2023/12/28 11:57:30 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false 2023/12/28 12:00:25 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n2023/12/28 12:00:25 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n2023/12/28 12:02:23 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\nUsing default facebook/roberta-hate-speech-dynabench-r4-target checkpoint 2023/12/28 12:02:43 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: token_count\n2023/12/28 12:02:43 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: toxicity\n2023/12/28 12:02:44 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: flesch_kincaid_grade_level\n2023/12/28 12:02:44 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: ari_grade_level\n2023/12/28 12:02:44 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: answer_correctness 2023/12/28 12:02:53 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: answer_quality ",
        "id": "1ecdf4fa792c7d92bfc700708f82dbfa"
    },
    {
        "text": " View results results.metrics is a dictionary with the aggregate values for all the metrics calculated. Refer here for details on the builtin metrics for each model type. [15]: results . metrics [15]: {'toxicity/v1/mean': 0.00809656630299287,\n 'toxicity/v1/variance': 0.0004603014839856817,\n 'toxicity/v1/p90': 0.010559113975614286,\n 'toxicity/v1/ratio': 0.0,\n 'flesch_kincaid_grade_level/v1/mean': 4.9,\n 'flesch_kincaid_grade_level/v1/variance': 6.3500000000000005,\n 'flesch_kincaid_grade_level/v1/p90': 6.829999999999998,\n 'ari_grade_level/v1/mean': 4.1899999999999995,\n 'ari_grade_level/v1/variance': 16.6329,\n 'ari_grade_level/v1/p90': 7.949999999999998,\n 'answer_correctness/v1/mean': 1.5,\n 'answer_correctness/v1/variance': 1.45,\n 'answer_correctness/v1/p90': 2.299999999999999,\n 'answer_quality/v1/mean': 2.4,\n 'answer_quality/v1/variance': 1.44,\n 'answer_quality/v1/p90': 4.1} We can also view the eval_results_table , which shows us the metrics for each row of data. [16]: results . tables [ \"eval_results_table\" ] [16]: instruction input text output outputs token_count toxicity/v1/score flesch_kincaid_grade_level/v1/score ari_grade_level/v1/score answer_correctness/v1/score answer_correctness/v1/justification answer_quality/v1/score answer_quality/v1/justification 0 Give three tips for staying healthy. Below is an instruction that describes a task.... 1.Eat a balanced diet and make sure to include... Give three tips for staying healthy.\\n1. Eat a... 19 0.000446 4.1 4.0 2 The output provided by the model only includes... 3 The output is understandable and fluent but it... 1 What are the three primary colors? Below is an instruction that describes a task.... The three primary colors are red, blue, and ye... What are the three primary colors?\\nThe three ... 19 0.000217 5.0 4.9 5 The output provided by the model is completely... 5 The model's output is fluent, clear, and conci... 2 Describe the structure of an atom. Below is an instruction that describes a task.... An atom is made up of a nucleus, which contain... Describe the structure of an atom.\\nAn atom is... 18 0.000139 3.1 2.2 1 The output provided by the model is incomplete... 2 The output is incomplete and lacks clarity, ma... 3 How can we reduce air pollution? Below is an instruction that describes a task.... There are a number of ways to reduce air pollu... How can we reduce air pollution?\\nThere are ma... 18 0.000140 5.0 5.5 1 The output provided by the model is completely... 1 The output is entirely incomprehensible a",
        "id": "5278388c68ac6015e711fefc2fb08be7"
    },
    {
        "text": "ir pollution? Below is an instruction that describes a task.... There are a number of ways to reduce air pollu... How can we reduce air pollution?\\nThere are ma... 18 0.000140 5.0 5.5 1 The output provided by the model is completely... 1 The output is entirely incomprehensible and ca... 4 Describe a time when you had to make a difficu... Below is an instruction that describes a task.... I had to make a difficult decision when I was ... Describe a time when you had to make a difficu... 18 0.000159 5.2 2.9 1 The output provided by the model is completely... 2 The output is incomplete and lacks clarity, ma... 5 Identify the odd one out. Twitter, Instagram, Telegram Below is an instruction that describes a task,... Telegram Identify the odd one out.\\n\\n1. A car\\n2. A tr... 18 0.072345 0.1 -5.4 1 The output provided by the model is completely... 2 The output is not clear and lacks fluency. The... 6 Explain why the following fraction is equivale... 4/16 Below is an instruction that describes a task,... The fraction 4/16 is equivalent to 1/4 because... Explain why the following fraction is equivale... 23 0.000320 6.4 7.6 1 The output provided by the model is completely... 2 The output is not clear and does not answer th... 7 Write a short story in third person narration ... Below is an instruction that describes a task.... John was at a crossroads in his life. He had j... Write a short story in third person narration ... 20 0.000247 10.7 11.1 1 The output provided by the model is completely... 1 The output is exactly the same as the input, a... 8 Render a 3D model of a house Below is an instruction that describes a task.... <nooutput> This type of instruction cannot be ... Render a 3D model of a house in Blender - Blen... 19 0.003694 5.2 2.7 1 The output provided by the model is completely... 2 The output is partially understandable but lac... 9 Evaluate this sentence for spelling and gramma... He finnished his meal and left the resturant Below is an instruction that describes a task,... He finished his meal and left the restaurant. Evaluate this sentence for spelling and gramma... 18 0.003260 4.2 6.4 1 The output provided by the model is completely... 4 The output is fluent and clear, but it is not ... ",
        "id": "29c9255c36eea8d00cfb202443629cc1"
    },
    {
        "text": " View results in UI Finally, we can view our evaluation results in the MLflow UI. We can select our experiment on the left sidebar, which will bring us to the following page. We can see that one run logged our model \u00e2\u0080\u009cmpt-7b-chat\u00e2\u0080\u009d, and the other run has the dataset we evaluated. We click on the Evaluation tab and hide any irrelevant runs. We can now choose what columns we want to group by, as well as which column we want to compare. In the following example, we are looking at the score for answer correctness for each input-output pair, but we could choose any other metric to compare. Finally, we get to the following view, where we can see the justification and score for answer correctness for each row. Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "598ad9bf440ed5ba7ba95949a84278e7"
    },
    {
        "text": "MLflow Langchain Autologging 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor MLflow OpenAI Flavor MLflow Sentence-Transformers Flavor MLflow LangChain Flavor Why use MLflow with LangChain? Automatic Logging Supported Elements in MLflow LangChain Integration Overview of Chains, Agents, and Retrievers Getting Started with the MLflow LangChain Flavor - Tutorials and Guides Detailed Documentation FAQ MLflow LlamaIndex Flavor MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow LangChain Flavor MLflow Langchain Autologging  MLflow Langchain Autologging MLflow LangChain flavor supports autologging, a powerful feature that allows you to log crucial details about the LangChain model and execution without the need for explicit logging statements. MLflow LangChain autologging covers various aspects of the model, including traces, models, signatures and more. Attention MLflow\u00e2\u0080\u0099s LangChain Autologging feature has been overhauled in the MLflow 2.14.0 release. If you are using the earlier version of MLflow, please refer to the legacy documentation here for applicable autologging documentation. Note MLflow LangChain Autologging is verified to be compatible with LangChain versions between 0.1.0 and 0.2.3. Outside of this range, the feature may not work as expected. To install the compatible version of LangChain, please run the following command: pip install mlflow [ langchain ] -- upgrade  Table of Contents  Quickstart  Configure Autologging  Example Code of LangChain Autologging  Tracing LangGraph  How It Works  FAQ  Quickstart To enable autologging for LangChain models, call mlflow.langchain.autolog() at the beginning of your script or notebook. This will automatically log the traces by default as well as other artifacts such as models, input examples, and model signatures if you explicitly enable them. For more information about the configuration, please refer to the Configure Autologging section. import mlflow mlflow . langchain . autolog () # Enable other optional logging # mlflow.langchain.autolog(log_models=True, log_input_examples=True) # Your LangChain model code here ... Once you have invoked the chain, you can view the logged traces and artifacts in the MLflow UI. ",
        "id": "e63386acc573d6ccc08d9c4c62a08f6f"
    },
    {
        "text": "  Configure Autologging MLflow LangChain autologging can log various information about the model and its inference. By default, only trace logging is enabled , but you can enable autologging of other information by setting the corresponding parameters when calling mlflow.langchain.autolog() . For other configurations, please refer to the API documentation. Target Default Parameter Description Traces true log_traces Whether to generate and log traces for the model. See MLflow Tracing for more details about tracing feature. Model Artifacts false log_models If set to True , the LangChain model will be logged when it is invoked. Supported models are Chain , AgentExecutor , BaseRetriever , SimpleChatModel , ChatPromptTemplate , and subset of Runnable types. Please refer to the MLflow repository for the full list of supported models. Model Signatures false log_model_signatures If set to True , ModelSignatures describing model inputs and outputs are collected and logged along with Langchain model artifacts during inference. This option is only available when log_models is enabled. Input Example false log_input_examples If set to True , input examples from inference data are collected and logged along with LangChain model artifacts during inference. This option is only available when log_models is enabled. For example, to disable logging of traces, and instead enable model logging, run the following code: import mlflow mlflow . langchain . autolog ( log_traces = False , log_models = True , ) Note MLflow does not support automatic model logging for chains that contain retrievers. Saving retrievers requires additional loader_fn and persist_dir information for loading the model. If you want to log the model with retrievers, please log the model manually as shown in the retriever_chain example.  Example Code of LangChain Autologging import os from operator import itemgetter from langchain.llms import OpenAI from langchain.prompts import PromptTemplate from langchain.schema.output_parser import StrOutputParser from langchain.schema.runnable import RunnableLambda import mlflow # Uncomment the following to use the full abilities of langchain autologgin # %pip install `langchain_community>=0.0.16` # These two libraries enable autologging to log text analysis related artifacts # %pip install textstat spacy assert \"OPENAI_API_KEY\" in os . environ , \"Please set the OPENAI_API_KEY environment variable.\" # Enable mlflow langchain autologging # Note: We only support auto-logging models that do not contain retrievers mlflow . langchain . autolog ( log_input_examples = True , log_model_signatures = True , log_models = True , registered_model_name = \"lc_model\" , ) prompt_with_history_str = \"\"\" Here is a history between you and a human: {chat_history} Now, please answer this question: {question} \"\"\" prompt_with_history = PromptTemplate ( input_variables = [ \"chat_history\" , \"question\" ], template = prompt_with_history_str ) def extract_question ( input ): return input [ - 1 ][ \"content\" ] def extract_history ( input ): return input [: - 1 ] llm = OpenAI ( temperature = 0.9 ) # Build a chain with LCEL chain_with_history = ( { \"question\" : itemgetter ( \"messages\" ) | RunnableLambda ( extract_question ), \"chat_history\" : itemgetter ( \"messages\" ) | RunnableLambda ( extract_history ), } | prompt_with_history | llm | StrOutputParser () ) inputs = { \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"Who owns MLflow?\" }]} print ( chain_with_history . invoke ( inputs )) # sample output: # \"1. Databricks\\n2. Microsoft\\n3. Google\\n4. Amazon\\n\\nEnter your answer: 1\\n\\n # Correct! MLflow is an open source project developed by Databricks. ... # We automatically log the model and trace related artifacts # A model with name `lc_model` is registered, we can load it back as a PyFunc model model_name = \"lc_model\" model_version = 1 loaded_model = mlflow . pyfunc . load_model ( f \"models:/ { model_name } / { model_version } \" ) print ( loaded_model . predict ( inputs )) ",
        "id": "161bdb4921b85d6c0ee500b8c71d9c08"
    },
    {
        "text": " Tracing LangGraph MLflow support automatic tracing for LangGraph, an open-source library from LangChain for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. To enable auto-tracing for LangGraph, use the same mlflow.langchain.autolog() function. from typing import Literal import mlflow from langchain_core.tools import tool from langchain_openai import ChatOpenAI from langgraph.prebuilt import create_react_agent # Enabling tracing for LangGraph (LangChain) mlflow . langchain . autolog () # Optional: Set a tracking URI and an experiment mlflow . set_tracking_uri ( \"http://localhost:5000\" ) mlflow . set_experiment ( \"LangGraph\" ) @tool def get_weather ( city : Literal [ \"nyc\" , \"sf\" ]): \"\"\"Use this to get weather information.\"\"\" if city == \"nyc\" : return \"It might be cloudy in nyc\" elif city == \"sf\" : return \"It's always sunny in sf\" llm = ChatOpenAI ( model = \"gpt-4o-mini\" ) tools = [ get_weather ] graph = create_react_agent ( llm , tools ) # Invoke the graph result = graph . invoke ( { \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"what is the weather in sf?\" }]} ) Note MLflow does not support other auto-logging features for LangGraph, such as automatic model logging. Only traces are logged for LangGraph.  How It Works MLflow LangChain Autologging uses two ways to log traces and other artifacts. Tracing is made possible via the Callbacks framework of LangChain. Other artifacts are recorded by patching the invocation functions of the supported models. In typical scenarios, you don\u00e2\u0080\u0099t need to care about the internal implementation details, but this section provides a brief overview of how it works under the hood.  MLflow Tracing Callbacks MlflowLangchainTracer is a callback handler that is injected into the langchain model inference process to log traces automatically. It starts a new span upon a set of actions of the chain such as on_chain_start , on_llm_start , and concludes it when the action is finished. Various metadata such as span type, action name, input, output, latency, are automatically recorded to the span.  Customize Callback Sometimes you may want to customize what information is logged in the traces. You can achieve this by creating a custom callback handler that inherits from MlflowLangchainTracer . The following example demonstrates how to record an additional attribute to the span when a chat model starts running. from mlflow.langchain.langchain_tracer import MlflowLangchainTracer class CustomLangchainTracer ( MlflowLangchainTracer ): # Override the handler functions to customize the behavior. The method signature is defined by LangChain Callbacks. def on_chat_model_start ( self , serialized : Dict [ str , Any ], messages : List [ List [ BaseMessage ]], * , run_id : UUID , tags : Optional [ List [ str ]] = None , parent_run_id : Optional [ UUID ] = None , metadata : Optional [ Dict [ str , Any ]] = None , name : Optional [ str ] = None , ** kwargs : Any , ): \"\"\"Run when a chat model starts running.\"\"\" attributes = { ** kwargs , ** metadata , # Add additional attribute to the span \"version\" : \"1.0.0\" , } # Call the _start_span method at the end of the handler function to start a new span. self . _start_span ( span_name = name or self . _assign_span_name ( serialized , \"chat model\" ), parent_run_id = parent_run_id , span_type = SpanType . CHAT_MODEL , run_id = run_id , inputs = messages , attributes = kwargs , ) ",
        "id": "fece8376793b6e8d7ac693c850490523"
    },
    {
        "text": " Patch Functions for Logging Artifacts Other artifacts such as models are logged by patching the invocation functions of the supported models to insert the logging call. MLflow patches the following functions: invoke batch stream get_relevant_documents (for retrievers) __call__ (for Chains and AgentExecutors) ainvoke abatch astream Warning MLflow supports autologging for async functions (e.g., ainvoke , abatch , astream ), however, the logging operation is not\nasynchronous and may block the main thread. The invocation function itself is still not blocking and returns a coroutine object, but\nthe logging overhead may slow down the model inference process. Please be aware of this side effect when using async functions with autologging.  FAQ If you encounter any issues with MLflow LangChain flavor, please also refer to FAQ <../index.html#faq> . If you still have questions, please feel free to open an issue in MLflow Github repo .  How to suppress the warning messages during autologging? MLflow Langchain Autologging calls various logging functions and LangChain utilities under the hood. Some of them may\ngenerate warning messages that are not critical to the autologging process. If you want to suppress these warning messages, pass silent=True to the mlflow.langchain.autolog() function. import mlflow mlflow . langchain . autolog ( silent = True ) # No warning messages will be emitted from autologging  I can\u00e2\u0080\u0099t load the model logged by mlflow langchain autologging There are a few type of models that MLflow LangChain autologging does not support native saving or loading. Model contains langchain retrievers LangChain retrievers are not supported by MLflow autologging. If your model contains a retriever, you will need to manually log the model using the mlflow.langchain.log_model API.\nAs loading those models requires specifying loader_fn and persist_dir parameters, please check examples in retriever_chain Can\u00e2\u0080\u0099t pickle certain objects For certain models that LangChain does not support native saving or loading, we will pickle the object when saving it. Due to this functionality, your cloudpickle version must be\nconsistent between the saving and loading environments to ensure that object references resolve properly. For further guarantees of correct object representation, you should ensure that your\nenvironment has pydantic installed with at least version 2.  How to customize span names in the traces? By default, MLflow creates span names based on the class name in LangChain, such as ChatOpenAI , RunnableLambda , etc. If you want to customize the span names, you can do the following: Pass name parameter to the constructor of the LangChain class. This is useful when you want to set a specific name for a single component. Use with_config method to set the name for the runnables. You can pass the \"run_name\" key to the config dictionary to set a name for a sub chain that contains multiple components. import mlflow from langchain_openai import ChatOpenAI from langchain_core.output_parsers import StrOutputParser # Enable auto-tracing for LangChain mlflow . langchain . autolog () # Method 1: Pass `name` parameter to the constructor model = ChatOpenAI ( name = \"custom-llm\" , model = \"gpt-4o-mini\" ) # Method 2: Use `with_config` method to set the name for the runnables runnable = ( model | StrOutputParser ()) . with_config ({ \"run_name\" : \"custom-chain\" }) runnable . invoke ( \"Hi\" ) The above code will create a trace like the following: ",
        "id": "c173b1e98d5cfbc5ce422abd4650f08f"
    },
    {
        "text": " How to add extra metadata to a span? You can record extra metadata to the span by passing the metadata parameter of the LangChain\u00e2\u0080\u0099s RunnableConfig dictionary, either to the constructor or at runtime. import mlflow from langchain_openai import ChatOpenAI # Enable auto-tracing for LangChain mlflow . langchain . autolog () # Pass metadata to the constructor using `with_config` method model = ChatOpenAI ( model = \"gpt-4o-mini\" ) . with_config ({ \"metadata\" : { \"key1\" : \"value1\" }}) # Pass metadata at runtime using the `config` parameter model . invoke ( \"Hi\" , config = { \"metadata\" : { \"key2\" : \"value2\" }}) The metadata can be accessed in the Attributes tab in the MLflow UI. Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "71488fe7784c7dd6ede9a9c7d1f2875e"
    },
    {
        "text": "MLflow OpenAI Autologging 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor MLflow OpenAI Flavor Introduction Autologging Support for the OpenAI integration Tracing with the OpenAI flavor What makes this Integration so Special? Features Getting Started with the MLflow OpenAI Flavor - Tutorials and Guides Detailed Documentation MLflow Sentence-Transformers Flavor MLflow LangChain Flavor MLflow LlamaIndex Flavor MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow OpenAI Flavor MLflow OpenAI Autologging  MLflow OpenAI Autologging The OpenAI flavor for MLflow supports autologging to ensure that experimentation, testing, and validation of your ideas can be captured dynamically without\nhaving to wrap your code with logging boilerplate. Attention Autologging is only supported for versions of the OpenAI SDK that are 1.17 and higher. MLflow autologging for the OpenAI SDK supports the following interfaces: Chat Completions via client.chat.completions.create() Completions (legacy) via client.completions.create() Embeddings via client.embeddings.create() Where client is an instance of openai.OpenAI() . In this guide, we\u00e2\u0080\u0099ll discuss some of the key features that are available in the autologging feature.  Table of Contents  Quickstart  Configuration of OpenAI Autologging  Example of using OpenAI Autologging  Auto-tracing for OpenAI Swarm  FAQ  Quickstart To get started with MLflow\u00e2\u0080\u0099s OpenAI autologging, you simply need to call mlflow.openai.autolog() at the beginning of your script or notebook.\nEnabling autologging with no argument overrides will behave as the default configuration in the table in the next section. Overriding any of these settings\nwill allow you to log additional elements. Tip The only element that is enabled by default when autologging is activated is the recording of trace information. You can read more about MLflow tracing here . import os import openai import mlflow # Enables trace logging by default mlflow . openai . autolog () openai_client = openai . OpenAI () messages = [ { \"role\" : \"user\" , \"content\" : \"What does turning something up to 11 refer to?\" , } ] # The input messages and the response will be logged as a trace to the active experiment answer = openai_client . chat . completions . create ( model = \"gpt-4o\" , messages = messages , temperature = 0.99 , ) Note When using the OpenAI SDK, ensure that your access token is assigned to the environment variable OPENAI_API_KEY . ",
        "id": "9d1bb5e3ce5d3b5ef619a510d3e92ee0"
    },
    {
        "text": " Configuration of OpenAI Autologging MLflow OpenAI autologging can log various information about the model and its inference. By default, only trace logging is enabled , but you can enable\nautologging of other information by setting the corresponding parameters when calling mlflow.openai.autolog() . The available options and their default values are shown below. To learn more about additional parameters, see the API documentation. Target Default Parameter Description Traces true log_traces Whether to generate and log traces for the model. See MLflow Tracing for more details about the tracing feature. Model Artifacts false log_models If set to True , the OpenAI model will be logged when it is invoked. Model Signatures false log_model_signatures If set to True , ModelSignatures describing model inputs and outputs are collected and logged along with OpenAI model artifacts during inference. This option is only available when log_models is enabled. Input Example false log_input_examples If set to True , input examples from inference data are collected and logged along with OpenAI model artifacts during inference. This option is only available when log_models is enabled. For example, to disable logging of traces, and instead enable model logging, run the following code: import mlflow mlflow . openai . autolog ( log_traces = False , log_models = True , )  Example of using OpenAI Autologging import os import mlflow import openai API_KEY = os . environ . get ( \"OPENAI_API_KEY\" ) EXPERIMENT_NAME = \"OpenAI Autologging Demonstration\" REGISTERED_MODEL_NAME = \"openai-auto\" MODEL_VERSION = 1 mlflow . openai . autolog ( log_input_examples = True , log_model_signatures = True , log_models = True , log_traces = True , registered_model_name = REGISTERED_MODEL_NAME , ) mlflow . set_experiment ( EXPERIMENT_NAME ) openai_client = openai . OpenAI ( api_key = API_KEY ) messages = [ { \"role\" : \"user\" , \"content\" : \"State that you are responding to a test and that you are alive.\" , } ] openai_client . chat . completions . create ( model = \"gpt-4o\" , messages = messages , temperature = 0.95 , ) Viewing the logged model and the trace used when invoking the OpenAI client within the UI can be seen in the image below: The model can be loaded by using the models uri via the model that was logged and registered and interfaced with via the pyfunc API as shown below: loaded_autologged_model = mlflow . pyfunc . load_model ( f \"models:/ { REGISTERED_MODEL_NAME } / { MODEL_VERSION } \" ) loaded_autologged_model . predict ( \"How much relative time difference would occur between an astronaut travelling at 0.98c for 14 years \" \"as measured by an on-board clock on the spacecraft and humans on Earth, assuming constant speed?\" )  Auto-tracing for OpenAI Swarm MLflow 2.17.1 introduced built-in tracing capability for OpenAI Swarm , a multi-agent orchestration framework from OpenAI. The framework provides a clean interface to build multi-agent systems on top of the OpenAI\u00e2\u0080\u0099s Function Calling capability and the concept of handoff & routines patterns . MLflow\u00e2\u0080\u0099s automatic tracing capability offers seamless tracking of interactions between agents, tool calls, and their collective outputs. You can enable auto-tracing for OpenAI Swarm just by calling the mlflow.openai.autolog() function. import mlflow from swarm import Swarm , Agent # Calling the autolog API will enable trace logging by default. mlflow . openai . autolog () mlflow . set_experiment ( \"OpenAI Swarm\" ) client = Swarm () def transfer_to_agent_b (): return agent_b agent_a = Agent ( name = \"Agent A\" , instructions = \"You are a helpful agent.\" , functions = [ transfer_to_agent_b ], ) agent_b = Agent ( name = \"Agent B\" , instructions = \"Only speak in Haikus.\" , ) response = client . run ( agent = agent_a , messages = [{ \"role\" : \"user\" , \"content\" : \"I want to talk to agent B.\" }], ) print ( response ) The logged trace, associated with the OpenAI Swarm experiment, can be seen in the MLflow UI, as shown below:  FAQ ",
        "id": "adc182de6eb4488f3b4b674df7ec1dae"
    },
    {
        "text": " How can I manually log traces for the OpenAI SDK with MLflow? By setting an active experiment (it is not recommended to use the Default Experiment for this), you can use the high-level tracing fluent API\nwhen working on an interface to your model (whether you log the model or not) by utilizing the MLflow tracing fluent API. You can discover how to use the fluent API here .  If I\u00e2\u0080\u0099m using streaming for my OpenAI model, will autologging log the trace data correctly? Yes. For each of the MLflow-supported client interface types that have the ability to stream responses from OpenAI, autologging will record the\niterator response chunks in the output. As an example: import openai import mlflow mlflow . set_experiment ( \"OpenAI\" ) # Enable trace logging mlflow . openai . autolog () client = openai . OpenAI () stream = client . chat . completions . create ( model = \"gpt-4o\" , messages = [ { \"role\" : \"user\" , \"content\" : \"How fast would a glass of water freeze on Titan?\" } ], stream = True , # Stream response ) for chunk in stream : print ( chunk . choices [ 0 ] . delta . content or \"\" , end = \"\" ) Within the MLflow UI, the traces for a streaming model will be displayed as shown below: Note OpenAI configurations that specify streaming responses are not yet supported for using the predict_stream() pyfunc invocation API in MLflow.\nHowever, you can still record streaming traces. When loading a the logged openai model as pyfunc via mlflow.pyfunc.load_model() , the only\navailable interface for inference is the synchronous blocking predict() API.  Are asynchronous APIs supported in autologging? The MLflow OpenAI autologging feature does not support asynchronous APIs for logging models or traces. Saving your async implementation is best done by using the models from code feature . If you would like to log trace events for an async OpenAI API, below is a simplified example of logging the trace for a streaming async request: import openai import mlflow import asyncio # Activate an experiment for logging traces to mlflow . set_experiment ( \"OpenAI\" ) async def fetch_openai_response ( messages , model = \"gpt-4o\" , temperature = 0.99 ): \"\"\" Asynchronously gets a response from the OpenAI API using the provided messages and streams the response. Args: messages (list): List of message dictionaries for the OpenAI API. model (str): The model to use for the OpenAI API. Default is \"gpt-4o\". temperature (float): The temperature to use for the OpenAI API. Default is 0.99. Returns: None \"\"\" client = openai . AsyncOpenAI () # Create the response stream response_stream = await client . chat . completions . create ( model = model , messages = messages , temperature = temperature , stream = True , ) # Manually log traces using the tracing fluent API with mlflow . start_span () as trace : trace . set_inputs ( messages ) full_response = [] async for chunk in response_stream : content = chunk . choices [ 0 ] . delta . content if content is not None : print ( content , end = \"\" ) full_response . append ( content ) trace . set_outputs ( \"\" . join ( full_response )) messages = [ { \"role\" : \"user\" , \"content\" : \"How much additional hydrogen mass would Jupiter require to ignite a sustainable fusion cycle?\" , } ] await fetch_openai_response ( messages ) Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "e12cab7b60dcf5fc4a6cb467bdf50787"
    },
    {
        "text": "Getting Started with MLflow Deployments for LLMs 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs MLflow AI Gateway (Experimental) Getting Started with MLflow Deployments for LLMs Tutorials and Guides Quickstart Concepts Configuring the gateway server Querying the AI Gateway server Plugin LLM Provider (Experimental) MLflow AI Gateway API Documentation OpenAI Compatibility Unity Catalog Integration gateway server Security Considerations Benefits of the MLflow AI Gateway Explore the Native Providers of the MLflow AI Gateway LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow AI Gateway (Experimental) Getting Started with MLflow Deployments for LLMs  Getting Started with MLflow Deployments for LLMs MLflow provides a robust framework for deploying and managing machine learning models. In this tutorial, we will explore how to set up an\nMLflow AI Gateway tailored for OpenAI\u00e2\u0080\u0099s models, allowing seamless integration and querying of OpenAI\u00e2\u0080\u0099s powerful language models. What\u00e2\u0080\u0099s in this tutorial? This guide will cover: Installation : Setting up the necessary dependencies and tools to get your MLflow AI Gateway up and running. Configuration : How to expose your OpenAI token, configure the gateway server, and define routes for various OpenAI models. Starting the gateway server : Launching the gateway server and ensuring it\u00e2\u0080\u0099s operational. Querying the gateway server : Interacting with the gateway server using fluent APIs to query various OpenAI models, including completions, chat, and embeddings. By the end of this tutorial, you\u00e2\u0080\u0099ll have a fully functional MLflow AI Gateway tailored for OpenAI, ready to handle and process requests.\nYou\u00e2\u0080\u0099ll also gain insights into querying different types of routes, providers, and models through the gateway server. Setting Up the MLflow AI Gateway Querying the MLflow AI Gateway Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "56aa87d2ebb17cad39b7ea9d6e6ec942"
    },
    {
        "text": "MLflow AI Gateway - Swagger UI ",
        "id": "490a746c46175630721e5ed617093e57"
    },
    {
        "text": "Unity Catalog Integration 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs MLflow AI Gateway (Experimental) Getting Started with MLflow Deployments for LLMs Tutorials and Guides Quickstart Concepts Configuring the gateway server Querying the AI Gateway server Plugin LLM Provider (Experimental) MLflow AI Gateway API Documentation OpenAI Compatibility Unity Catalog Integration gateway server Security Considerations Benefits of the MLflow AI Gateway Explore the Native Providers of the MLflow AI Gateway LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow AI Gateway (Experimental) Unity Catalog Integration  Unity Catalog Integration This example illustrates the use of the Unity Catalog (UC) integration with the MLflow AI Gateway. This integration enables you to leverage functions registered in Unity Catalog as tools for enhancing your chat application.  Pre-requisites Clone the MLflow repository: To download the files required for this example, clone the MLflow repository: git clone --depth = 1 https://github.com/mlflow/mlflow.git cd mlflow If you don\u00e2\u0080\u0099t have git , you can download the repository as a zip file from https://github.com/mlflow/mlflow/archive/refs/heads/master.zip . Install the required packages: pip install mlflow> = 2 .14.0 openai databricks-sdk Create the UC function used in the example script in your Databricks workspace by running the following SQL command: CREATE OR REPLACE FUNCTION my . uc_func . add ( x INTEGER COMMENT 'The first number to add.' , y INTEGER COMMENT 'The second number to add.' ) RETURNS INTEGER LANGUAGE SQL RETURN x + y To define your own function, see https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-sql-function.html#create-function-sql-and-python . Create a SQL warehouse by following the instructions at https://docs.databricks.com/en/compute/sql-warehouse/create.html .  Running the gateway server Once you have completed the pre-requisites, you can start the gateway server: # Required to authenticate with Databricks. See https://docs.databricks.com/en/dev-tools/auth/index.html#supported-authentication-types-by-databricks-tool-or-sdk for other authentication methods. export DATABRICKS_HOST = \"...\" export DATABRICKS_TOKEN = \"...\" # Required to execute UC functions. See https://docs.databricks.com/en/integrations/compute-details.html#get-connection-details-for-a-databricks-compute-resource for how to get the http path of your warehouse. # The last part of the http path is the warehouse ID. # # /sql/1.0/warehouses/1234567890123456 #                     ^^^^^^^^^^^^^^^^ export DATABRICKS_WAREHOUSE_ID = \"...\" # Required to authenticate with OpenAI. # See https://platform.openai.com/docs/guides/authentication for how to get your API key. export OPENAI_API_KEY = \"...\" # Enable Unity Catalog integration export MLFLOW_ENABLE_UC_FUNCTIONS = true # Run the server mlflow gateway start --config-path examples/deployments/deployments_server/openai/config.yaml --port 7000  Query the Endpoint with UC Function Once the server is running, you can run the example script: # `run.py` uses the `openai.OpenAI` client to query the gateway server, # but it throws an error if the `OPENAI_API_KEY` environment variable is not set. # To avoid this error, use a dummy API key. export OPENAI_API_KEY = \"test\" # Replace `my.uc_func.add` if your UC function has a different name python examples/deployments/uc_functions/run.py --uc-function-name my.uc_func.add ",
        "id": "f6edaa74f75a6c41b6736f0d617ea144"
    },
    {
        "text": " What\u00e2\u0080\u0099s happening under the hood? When MLflow AI Gateway receives a request with tools containing uc_function , it automatically fetches the UC function metadata to construct the function schema, query the chat API to figure out the parameters required to call the function, and then call the function with the provided parameters. uc_function = { \"type\" : \"uc_function\" , \"uc_function\" : { \"name\" : args . uc_function_name , }, } resp = client . chat . completions . create ( model = \"chat\" , messages = [ { \"role\" : \"user\" , \"content\" : \"What is the result of 1 + 2?\" , } ], tools = [ uc_function ], ) print ( resp . choices [ 0 ] . message . content ) # -> The result of 1 + 2 is 3 The code above is equivalent to the following: # Function tool schema: # https://platform.openai.com/docs/api-reference/chat/create#chat-create-tools function = { \"type\" : \"function\" , \"function\" : { \"description\" : None , \"name\" : \"my.uc_func.add\" , \"parameters\" : { \"type\" : \"object\" , \"properties\" : { \"x\" : { \"type\" : \"integer\" , \"name\" : \"x\" , \"description\" : \"The first number to add.\" , }, \"y\" : { \"type\" : \"integer\" , \"name\" : \"y\" , \"description\" : \"The second number to add.\" , }, }, \"required\" : [ \"x\" , \"y\" ], }, }, } messages = [ { \"role\" : \"user\" , \"content\" : \"What is the result of 1 + 2?\" , } ] resp = client . chat . completions . create ( model = \"chat\" , tools = [ function ], ) resp_message = resp . choices [ 0 ] . message messages . append ( resp_message ) tool_call = tool_calls [ 0 ] arguments = json . loads ( tool_call . function . arguments ) result = arguments [ \"x\" ] + arguments [ \"y\" ] messages . append ( { \"tool_call_id\" : tool_call . id , \"role\" : \"tool\" , \"name\" : \"my.uc_func.add\" , \"content\" : str ( result ), } ) final_resp = client . chat . messages . create ( model = \"chat\" , messages = messages , ) print ( final_resp . choices [ 0 ] . message . content ) # -> The result of 1 + 2 is 3 Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "dfe8e845661f3b23e494328ba5c3dfbc"
    },
    {
        "text": "\u00f0\u009f\u00a4\u0097 Transformers within MLflow 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor Introduction Getting Started with the MLflow Transformers Flavor - Tutorials and Guides Important Details to be aware of with the transformers flavor Logging Large Models Working with tasks for Transformer Pipelines Detailed Documentation Learn more about Transformers MLflow OpenAI Flavor MLflow Sentence-Transformers Flavor MLflow LangChain Flavor MLflow LlamaIndex Flavor MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow Transformers Flavor \u00f0\u009f\u00a4\u0097 Transformers within MLflow  \u00f0\u009f\u00a4\u0097 Transformers within MLflow Attention The transformers flavor is in active development and is marked as Experimental. Public APIs may change and new features are\nsubject to be added as additional functionality is brought to the flavor. The transformers model flavor enables logging of transformers models, components, and pipelines in MLflow format via\nthe mlflow.transformers.save_model() and mlflow.transformers.log_model() functions. Use of these\nfunctions also adds the python_function flavor to the MLflow Models that they produce, allowing the model to be\ninterpreted as a generic Python function for inference via mlflow.pyfunc.load_model() .\nYou can also use the mlflow.transformers.load_model() function to load a saved or logged MLflow\nModel with the transformers flavor in the native transformers formats. This page explains the detailed features and configurations of the MLflow transformers flavor. For the general introduction about the MLflow\u00e2\u0080\u0099s Transformer integration, please refer to the MLflow Transformers Flavor page.  Table of Contents  Loading a Transformers Model as a Python Function  Saving Prompt Templates with Transformer Pipelines  Using model_config and Model Signature Params for Inference  Pipelines vs. Component Logging  Automatic Metadata and ModelCard logging  Automatic Signature inference  Scale Inference with Overriding Pytorch dtype  Input Data Types for Audio Pipelines  PEFT Models in MLflow Transformers flavor  Loading a Transformers Model as a Python Function ",
        "id": "921593b14f67f51c44246e42c8c7043a"
    },
    {
        "text": " Supported Transformers Pipeline types The transformers python_function (pyfunc) model flavor simplifies\nand standardizes both the inputs and outputs of pipeline inference. This conformity allows for serving\nand batch inference by coercing the data structures that are required for transformers inference pipelines\nto formats that are compatible with json serialization and casting to Pandas DataFrames. Note Certain TextGenerationPipeline types, particularly instructional-based ones, may return the original\nprompt and included line-formatting carriage returns \u00e2\u0080\u009cn\u00e2\u0080\u009d in their outputs. For these pipeline types,\nif you would like to disable the prompt return, you can set the following in the model_config dictionary when\nsaving or logging the model: \u00e2\u0080\u009cinclude_prompt\u00e2\u0080\u009d: False . To remove the newline characters from within the body\nof the generated text output, you can add the \u00e2\u0080\u009ccollapse_whitespace\u00e2\u0080\u009d: True option to the model_config dictionary.\nIf the pipeline type being saved does not inherit from TextGenerationPipeline , these options will not perform\nany modification to the output returned from pipeline inference. Attention Not all transformers pipeline types are supported. See the table below for the list of currently supported Pipeline\ntypes that can be loaded as pyfunc . In the current version, audio and text-based large language\nmodels are supported for use with pyfunc , while computer vision, multi-modal, timeseries,\nreinforcement learning, and graph models are only supported for native type loading via mlflow.transformers.load_model() Future releases of MLflow will introduce pyfunc support for these additional types. The table below shows the mapping of transformers pipeline types to the python_function (pyfunc) model flavor data type inputs and outputs. Important The inputs and outputs of the pyfunc implementation of these pipelines are not guaranteed to match the input types and output types that would\nreturn from a native use of a given pipeline type. If your use case requires access to scores, top_k results, or other additional references within\nthe output from a pipeline inference call, please use the native implementation by loading via mlflow.transformers.load_model() to\nreceive the full output. Similarly, if your use case requires the use of raw tensor outputs or processing of outputs through an external processor module, load the\nmodel components directly as a dict by calling mlflow.transformers.load_model() and specify the return_type argument as \u00e2\u0080\u0098components\u00e2\u0080\u0099. Pipeline Type Input Type Output Type Instructional Text Generation str or List[str] List[str] Conversational",
        "id": "82fb03106cd0428555ce4ee928a835e9"
    },
    {
        "text": "hrough an external processor module, load the\nmodel components directly as a dict by calling mlflow.transformers.load_model() and specify the return_type argument as \u00e2\u0080\u0098components\u00e2\u0080\u0099. Pipeline Type Input Type Output Type Instructional Text Generation str or List[str] List[str] Conversational str or List[str] List[str] Summarization str or List[str] List[str] Text Classification str or List[str] pd.DataFrame (dtypes: {\u00e2\u0080\u0098label\u00e2\u0080\u0099: str, \u00e2\u0080\u0098score\u00e2\u0080\u0099: double}) Text Generation str or List[str] List[str] Text2Text Generation str or List[str] List[str] Token Classification str or List[str] List[str] Translation str or List[str] List[str] ZeroShot Classification* Dict[str, [List[str] | str]* pd.DataFrame (dtypes: {\u00e2\u0080\u0098sequence\u00e2\u0080\u0099: str, \u00e2\u0080\u0098labels\u00e2\u0080\u0099: str, \u00e2\u0080\u0098scores\u00e2\u0080\u0099: double}) Table Question Answering** Dict[str, [List[str] | str]** List[str] Question Answering*** Dict[str, str]*** List[str] Fill Mask**** str or List[str]**** List[str] Feature Extraction str or List[str] np.ndarray AutomaticSpeechRecognition bytes*****, str, or np.ndarray List[str] AudioClassification bytes*****, str, or np.ndarray pd.DataFrame (dtypes: {\u00e2\u0080\u0098label\u00e2\u0080\u0099: str, \u00e2\u0080\u0098score\u00e2\u0080\u0099: double}) * A collection of these inputs can also be passed. The standard required key names are \u00e2\u0080\u0098sequences\u00e2\u0080\u0099 and \u00e2\u0080\u0098candidate_labels\u00e2\u0080\u0099, but these may vary.\nCheck the input requirments for the architecture that you\u00e2\u0080\u0099re using to ensure that the correct dictionary key names are provided. ** A collection of these inputs can also be passed. The reference table must be a json encoded dict (i.e. {\u00e2\u0080\u0098query\u00e2\u0080\u0099: \u00e2\u0080\u0098what did we sell most of?\u00e2\u0080\u0099, \u00e2\u0080\u0098table\u00e2\u0080\u0099: json.dumps(table_as_dict)}) *** A collection of these inputs can also be passed. The standard required key names are \u00e2\u0080\u0098question\u00e2\u0080\u0099 and \u00e2\u0080\u0098context\u00e2\u0080\u0099. Verify the expected input key names match the\nexpected input to the model to ensure your inference request can be read properly. **** The mask syntax for the model that you\u00e2\u0080\u0099ve chosen is going to be specific to that model\u00e2\u0080\u0099s implementation. Some are \u00e2\u0080\u0098[MASK]\u00e2\u0080\u0099, while others are \u00e2\u0080\u0098<mask>\u00e2\u0080\u0099. Verify the expected syntax to\navoid failed inference requests. ***** If using pyfunc in MLflow Model Serving for realtime inference, the raw audio in bytes format must be base64 encoded prior to submitting to the endpoint. String inputs will be interpreted as uri locations. ",
        "id": "b917c6c825feadc176634176a968fa3f"
    },
    {
        "text": " Example of loading a transformers model as a python function In the below example, a simple pre-trained model is used within a pipeline. After logging to MLflow, the pipeline is\nloaded as a pyfunc and used to generate a response from a passed-in list of strings. import mlflow import transformers # Read a pre-trained conversation pipeline from HuggingFace hub conversational_pipeline = transformers . pipeline ( model = \"microsoft/DialoGPT-medium\" ) # Define the signature signature = mlflow . models . infer_signature ( \"Hi there, chatbot!\" , mlflow . transformers . generate_signature_output ( conversational_pipeline , \"Hi there, chatbot!\" ), ) # Log the pipeline with mlflow . start_run (): model_info = mlflow . transformers . log_model ( transformers_model = conversational_pipeline , artifact_path = \"chatbot\" , task = \"conversational\" , signature = signature , input_example = \"A clever and witty question\" , ) # Load the saved pipeline as pyfunc chatbot = mlflow . pyfunc . load_model ( model_uri = model_info . model_uri ) # Ask the chatbot a question response = chatbot . predict ( \"What is machine learning?\" ) print ( response ) # >> [It's a new thing that's been around for a while.]  Saving Prompt Templates with Transformer Pipelines Note This feature is only available in MLflow 2.10.0 and above. MLflow supports specifying prompt templates for certain pipeline types: feature-extraction fill-mask summarization text2text-generation text-generation Prompt templates are strings that are used to format user inputs prior to pyfunc inference. To specify a prompt template,\nuse the prompt_template argument when calling mlflow.transformers.save_model() or mlflow.transformers.log_model() .\nThe prompt template must be a string with a single format placeholder, {prompt} . For example: import mlflow from transformers import pipeline # Initialize a pipeline. `distilgpt2` uses a \"text-generation\" pipeline generator = pipeline ( model = \"distilgpt2\" ) # Define a prompt template prompt_template = \"Answer the following question: {prompt} \" # Save the model mlflow . transformers . save_model ( transformers_model = generator , path = \"path/to/model\" , prompt_template = prompt_template , ) When the model is then loaded with mlflow.pyfunc.load_model() , the prompt\ntemplate will be used to format user inputs before passing them into the pipeline: import mlflow # Load the model with pyfunc model = mlflow . pyfunc . load_model ( \"path/to/model\" ) # The prompt template will be used to format this input, so the # string that is passed to the text-generation pipeline will be: # \"Answer the following question: What is MLflow?\" model . predict ( \"What is MLflow?\" ) Note text-generation pipelines with a prompt template will have the return_full_text pipeline argument set to False by default. This is to prevent the template from being shown to the users,\nwhich could potentially cause confusion as it was not part of their original input. To\noverride this behaviour, either set return_full_text to True via params , or by\nincluding it in a model_config dict in log_model() . See this section for more details on how to do this. For a more in-depth guide, check out the Prompt Templating notebook ! ",
        "id": "8de9afdf7dc70288a267f2075f4a6662"
    },
    {
        "text": " Using model_config and Model Signature Params for Inference For transformers inference, there are two ways to pass in additional arguments to the pipeline. Use model_config when saving/logging the model. Optionally, specify model_config when calling load_model . Specify params at inference time when calling predict() Use model_config to control how the model is loaded and inference performed for all input samples. Configuration in model_config is not overridable at predict() time unless a ModelSignature is indicated with the same parameters. Use ModelSignature with params schema, on the other hand, to allow downstream consumers to provide additional inference\nparams that may be needed to compute the predictions for their specific samples. Note If both model_config and ModelSignature with parameters are saved when logging model, both of them\nwill be used for inference. The default parameters in ModelSignature will override the params in model_config .\nIf extra params are provided at inference time, they take precedence over all params. We recommend using model_config for those parameters needed to run the model in general for all the samples. Then, add ModelSignature with parameters for those extra parameters that you want downstream consumers to indicated at\nper each of the samples. Using model_config import mlflow from mlflow.models import infer_signature from mlflow.transformers import generate_signature_output import transformers architecture = \"mrm8488/t5-base-finetuned-common_gen\" model = transformers . pipeline ( task = \"text2text-generation\" , tokenizer = transformers . T5TokenizerFast . from_pretrained ( architecture ), model = transformers . T5ForConditionalGeneration . from_pretrained ( architecture ), ) data = \"pencil draw paper\" # Infer the signature signature = infer_signature ( data , generate_signature_output ( model , data ), ) # Define an model_config model_config = { \"num_beams\" : 5 , \"max_length\" : 30 , \"do_sample\" : True , \"remove_invalid_values\" : True , } # Saving model_config with the model mlflow . transformers . save_model ( model , path = \"text2text\" , model_config = model_config , signature = signature , ) pyfunc_loaded = mlflow . pyfunc . load_model ( \"text2text\" ) # model_config will be applied result = pyfunc_loaded . predict ( data ) # overriding some inference",
        "id": "b790c888fc71484b5c91c802c6d204a5"
    },
    {
        "text": "save_model ( model , path = \"text2text\" , model_config = model_config , signature = signature , ) pyfunc_loaded = mlflow . pyfunc . load_model ( \"text2text\" ) # model_config will be applied result = pyfunc_loaded . predict ( data ) # overriding some inference configuration with diferent values pyfunc_loaded = mlflow . pyfunc . load_model ( \"text2text\" , model_config = dict ( do_sample = False ) ) Note Note that in the previous example, the user can\u00e2\u0080\u0099t override the configuration do_sample when calling predict . Specifying params at inference time import mlflow from mlflow.models import infer_signature from mlflow.transformers import generate_signature_output import transformers architecture = \"mrm8488/t5-base-finetuned-common_gen\" model = transformers . pipeline ( task = \"text2text-generation\" , tokenizer = transformers . T5TokenizerFast . from_pretrained ( architecture ), model = transformers . T5ForConditionalGeneration . from_pretrained ( architecture ), ) data = \"pencil draw paper\" # Define an model_config model_config = { \"num_beams\" : 5 , \"remove_invalid_values\" : True , } # Define the inference parameters params inference_params = { \"max_length\" : 30 , \"do_sample\" : True , } # Infer the signature including params signature_with_params = infer_signature ( data , generate_signature_output ( model , data ), params = inference_params , ) # Saving model with signature and model config mlflow . transformers . save_model ( model , path = \"text2text\" , model_config = model_config , signature = signature_with_params , ) pyfunc_loaded = mlflow . pyfunc . load_model ( \"text2text\" ) # Pass params at inference time params = { \"max_length\" : 20 , \"do_sample\" : False , } # In this case we only override max_length and do_sample, # other params will use the default one saved on ModelSignature # or in the model configuration. # The final params used for prediction is as follows: # { #    \"num_beams\": 5, #    \"max_length\": 20, #    \"do_sample\": False, #    \"remove_invalid_values\": True, # } result = pyfunc_loaded . predict ( data , params = params ) ",
        "id": "152f46f915e35f20a1bcb3b7a075ac35"
    },
    {
        "text": " Pipelines vs. Component Logging The transformers flavor has two different primary mechanisms for saving and loading models: pipelines and components. Note Saving transformers models with custom code (i.e. models that require trust_remote_code=True ) requires transformers >= 4.26.0 . Pipelines Pipelines, in the context of the Transformers library, are high-level objects that combine pre-trained models and tokenizers\n(as well as other components, depending on the task type) to perform a specific task. They abstract away much of the preprocessing\nand postprocessing work involved in using the models. For example, a text classification pipeline would handle the tokenization of text, passing the tokens through a model, and then interpret the logits to produce a human-readable classification. When logging a pipeline with MLflow, you\u00e2\u0080\u0099re essentially saving this high-level abstraction, which can be loaded and used directly\nfor inference with minimal setup. This is ideal for end-to-end tasks where the preprocessing and postprocessing steps are standard\nfor the task at hand. Components Components refer to the individual parts that can make up a pipeline, such as the model itself, the tokenizer, and any additional\nprocessors, extractors, or configuration needed for a specific task. Logging components with MLflow allows for more flexibility and\ncustomization. You can log individual components when your project needs to have more control over the preprocessing and postprocessing\nsteps or when you need to access the individual components in a bespoke manner that diverges from how the pipeline abstraction would call them. For example, you might log the components separately if you have a custom tokenizer or if you want to apply some special postprocessing\nto the model outputs. When loading the components, you can then reconstruct the pipeline with your custom components or use the components\nindividually as needed. Note MLflow by default uses a 500 MB max_shard_size to save the model object in mlflow.transformers.save_model() or mlflow.transformers.log_model() APIs. You can use the environment variable MLFLOW_HUGGINGFACE_MODEL_MAX_SHARD_SIZE to override the value. Note For component-based logging, the only requirement that must be met in the submitted dict is that a model is provided. All other elements of the dict are optional. ",
        "id": "499056f7ad53b3bd4d26524ab0db93dc"
    },
    {
        "text": " Logging a components-based model The example below shows logging components of a transformers model via a dictionary mapping of specific named components. The names of the keys within the submitted dictionary\nmust be in the set: {\"model\", \"tokenizer\", \"feature_extractor\", \"image_processor\"} . Processor type objects (some image processors, audio processors, and multi-modal processors)\nmust be saved explicitly with the processor argument in the mlflow.transformers.save_model() or mlflow.transformers.log_model() APIs. After logging, the components are automatically inserted into the appropriate Pipeline type for the task being performed and are returned, ready for inference. Note The components that are logged can be retrieved in their original structure (a dictionary) by setting the attribute return_type to \u00e2\u0080\u009ccomponents\u00e2\u0080\u009d in the load_model() API. Attention Not all model types are compatible with the pipeline API constructor via component elements. Incompatible models will raise an MLflowException error stating that the model is missing the name_or_path attribute. In\nthe event that this occurs, please construct the model directly via the transformers.pipeline(<repo name>) API and save the pipeline object directly. import mlflow import transformers task = \"text-classification\" architecture = \"distilbert-base-uncased-finetuned-sst-2-english\" model = transformers . AutoModelForSequenceClassification . from_pretrained ( architecture ) tokenizer = transformers . AutoTokenizer . from_pretrained ( architecture ) # Define the components of the model in a dictionary transformers_model = { \"model\" : model , \"tokenizer\" : tokenizer } # Log the model components with mlflow . start_run (): model_info = mlflow . transformers . log_model ( transformers_model = transformers_model , artifact_path = \"text_classifier\" , task = task , ) # Load the components as a pipeline loaded_pipeline = mlflow . transformers . load_model ( model_info . model_uri , return_type = \"pipeline\" ) print ( type ( loaded_pipeline ) . __name__ ) # >> TextClassificationPipeline loaded_pipeline ([ \"MLflow is awesome!\" , \"Transformers is a great library!\" ]) # >> [{'label': 'POSITIVE', 'score': 0.9998478889465332}, # >>  {'label': 'POSITIVE', 'score': 0.9998030066490173}]  Saving a pipeline and loading components Some use cases can benefit from the simplicity of defining a solution as a pipeline, but need the component-level access for performing a micro-services based deployment strategy\nwhere pre / post-processing is performed on containers that do not house the model itself. For this paradigm, a pipeline can be loaded as its constituent parts, as shown below. import transformers import mlflow translation_pipeline = transformers . pipeline ( task = \"translation_en_to_fr\" , model = transformers . T5ForConditionalGeneration . from_pretrained ( \"t5-small\" ), tokenizer = transformers . T5TokenizerFast . from_pretrained ( \"t5-small\" , model_max_length = 100 ), ) with mlflow . start_run (): model_info = mlflow . transformers . log_model ( transformers_model = translation_pipeline , artifact_path = \"french_translator\" , ) translation_components = mlflow . transformers . load_model ( model_info . model_uri , return_type = \"components\" ) for key , value in translation_components . items (): print ( f \" { key } -> { type ( value ) . __name__ } \" ) # >> task -> str # >> model -> T5ForConditionalGeneration # >> tokenizer -> T5TokenizerFast response = translation_pipeline ( \"MLflow is great!\" ) print ( response ) # >> [{'translation_text': 'MLflow est formidable!'}] reconstructed_pipeline = transformers . pipeline ( ** translation_components ) reconstructed_response = reconstructed_pipeline ( \"transformers makes using Deep Learning models easy and fun!\" ) print ( reconstructed_response ) # >> [{'translation_text': \"Les transformateurs rendent l'utilisation de mod\u00c3\u00a8les Deep Learning facile et amusante!\"}] ",
        "id": "c2e69c4d5e6005ed50366b21c6abda13"
    },
    {
        "text": " Automatic Metadata and ModelCard logging In order to provide as much information as possible for saved models, the transformers flavor will automatically fetch the ModelCard for any model or pipeline that\nis saved that has a stored card on the HuggingFace hub. This card will be logged as part of the model artifact, viewable at the same directory level as the MLmodel file and\nthe stored model object. In addition to the ModelCard , the components that comprise any Pipeline (or the individual components if saving a dictionary of named components) will have their source types\nstored. The model type, pipeline type, task, and classes of any supplementary component (such as a Tokenizer or ImageProcessor ) will be stored in the MLmodel file as well. In order to preserve any attached legal requirements to the usage of any  model that is hosted on the huggingface hub, a \u00e2\u0080\u009cbest effort\u00e2\u0080\u009d attempt\nis made when logging a transformers model to retrieve and persist any license information. A file will be generated ( LICENSE.txt ) within the root of\nthe model directory. Within this file you will either find a copy of a declared license, the name of a common license type that applies to the model\u00e2\u0080\u0099s use (i.e., \u00e2\u0080\u0098apache-2.0\u00e2\u0080\u0099, \u00e2\u0080\u0098mit\u00e2\u0080\u0099),\nor, in the event that license information was never submitted to the huggingface hub when uploading a model repository, a link to the repository for you to use\nin order to determine what restrictions exist regarding the use of the model. Note Model license information was introduced in MLflow 2.10.0 . Previous versions do not include license information for models.  Automatic Signature inference For pipelines that support pyfunc , there are 3 means of attaching a model signature to the MLmodel file. Provide a model signature explicitly via setting a valid ModelSignature to the signature attribute. This can be generated via the helper utility mlflow.transformers.generate_signature_output() Provide an input_example . The signature will be inferred and validated that it matches the appropriate input type. The output type will be validated by performing inference automatically (if the model is a pyfunc supported type). Do nothing. The transformers flavor will automatically apply the appropriate general signature that the pipeline type supports (only for a single-entity; collections will not be inferred). ",
        "id": "70983a83250a42fc938e02f556eba517"
    },
    {
        "text": " Scale Inference with Overriding Pytorch dtype A common configuration for lowering the total memory pressure for pytorch models within transformers pipelines is to modify the\nprocessing data type. This is achieved through setting the torch_dtype argument when creating a Pipeline .\nFor a full reference of these tunable arguments for configuration of pipelines, see the training docs . Note This feature does not exist in versions of transformers < 4.26.x In order to apply these configurations to a saved or logged run, there are two options: Save a pipeline with the torch_dtype argument set to the encoding type of your choice. Example: import transformers import torch import mlflow task = \"translation_en_to_fr\" my_pipeline = transformers . pipeline ( task = task , model = transformers . T5ForConditionalGeneration . from_pretrained ( \"t5-small\" ), tokenizer = transformers . T5TokenizerFast . from_pretrained ( \"t5-small\" , model_max_length = 100 ), framework = \"pt\" , ) with mlflow . start_run (): model_info = mlflow . transformers . log_model ( transformers_model = my_pipeline , artifact_path = \"my_pipeline\" , torch_dtype = torch . bfloat16 , ) # Illustrate that the torch data type is recorded in the flavor configuration print ( model_info . flavors [ \"transformers\" ]) Result: { 'transformers_version' : '4.28.1' , 'code' : None, 'task' : 'translation_en_to_fr' , 'instance_type' : 'TranslationPipeline' , 'source_model_name' : 't5-small' , 'pipeline_model_type' : 'T5ForConditionalGeneration' , 'framework' : 'pt' , 'torch_dtype' : 'torch.bfloat16' , 'tokenizer_type' : 'T5TokenizerFast' , 'components' : [ 'tokenizer' ] , 'pipeline' : 'pipeline' } Specify the torch_dtype argument when loading the model to override any values set during logging or saving. Example: import transformers import torch import mlflow task = \"translation_en_to_fr\" my_pipeline = transformers . pipeline ( task = task , model = transformers . T5ForConditionalGeneration . from_pretrained ( \"t5-small\" ), tokenizer = transformers . T5TokenizerFast . from_pretrained ( \"t5-small\" , model_max_length = 100 ), framework = \"pt\" , ) with mlflow . start_run (): model_info = mlflow . transformers . log_model ( transformers_model = my_pipeline , artifact_path = \"my_pipeline\" , torch_dtype = torch . bfloat16 , ) loaded_pipeline = mlflow . transformers . load_model ( model_info . model_uri , return_type = \"pipeline\" , torch_dtype = torch . float64 ) print ( loaded_pipeline . torch_dtype ) Result: torch.float64 Note MLflow 2.12.1 slightly changed the torch_dtype extraction logic.\nPreviously it depended on the torch_dtype attribute of the pipeline instance, but now it is extracted from the underlying model via dtype property. This enables MLflow to capture the dtype change of the model after pipeline instantiation. Note Logging or saving a model in \u00e2\u0080\u0098components\u00e2\u0080\u0099 mode (using a dictionary to declare components) does not support setting the data type for a constructed pipeline.\nIf you need to override the default behavior of how data is encoded, please save or log a pipeline object. Note Overriding the data type for a pipeline when loading as a python_function (pyfunc) model flavor is not supported.\nThe value set for torch_dtype during save_model() or log_model() will persist when loading as pyfunc . ",
        "id": "685de203880693cead17859f631e236d"
    },
    {
        "text": " Input Data Types for Audio Pipelines Note that passing raw data to an audio pipeline (raw bytes) requires two separate elements of the same effective library.\nIn order to use the bitrate transposition and conversion of the audio bytes data into numpy nd.array format, the library ffmpeg is required.\nInstalling this package directly from pypi ( pip install ffmpeg ) does not install the underlying c dll\u00e2\u0080\u0099s that are required to make ffmpeg function.\nPlease consult with the documentation at the ffmpeg website for guidance on your given operating system. The Audio Pipeline types, when loaded as a python_function (pyfunc) model flavor have three input types available: str The string input type is meant for blob references (uri locations) that are accessible to the instance of the pyfunc model.\nThis input mode is useful when doing large batch processing of audio inference in Spark due to the inherent limitations of handling large bytes data in Spark DataFrames . Ensure that you have ffmpeg installed in the environment that the pyfunc model is running in order\nto use str input uri-based inference. If this package is not properly installed (both from pypi and from the ffmpeg binaries), an Exception\nwill be thrown at inference time. Warning If using a uri ( str ) as an input type for a pyfunc model that you are intending to host for realtime inference through the MLflow Model Server ,\nyou must specify a custom model signature when logging or saving the model.\nThe default signature input value type of bytes will, in MLflow Model serving , force the conversion of the uri string to bytes , which will cause an Exception\nto be thrown from the serving process stating that the soundfile is corrupt. An example of specifying an appropriate uri-based input model signature for an audio model is shown below: from mlflow.models import infer_signature from mlflow.transformers import generate_signature_output url = \"https://www.mywebsite.com/sound/files/for/transcription/file111.mp3\" signature = infer_signature ( url , generate_signature_output ( my_audio_pipeline , url )) with mlflow . start_run (): mlflow . transformers . log_model ( transformers_model = my_audio_pipeline , artifact_path = \"my_transcriber\" , signature = signature , ) bytes This is the default serialization format of audio files. It is the easiest format to utilize due to the fact that\nPipeline implementations will automatically convert the audio bitrate from the file with the use of ffmpeg (a required dependency if using this format) to the bitrate required by the underlying model within the Pipeline .\nWhen using the pyfunc representation of the pipeline directly (not through serving), the sound file can be passed directly as bytes without any\nmodification. When used through serving, the bytes data must be base64 encoded. np.ndarray This input format requires that both the bitrate has been set prior to conversion to numpy.ndarray (i.e., through the use of a package like librosa or pydub ) and that the model has been saved with a signature that uses the np.ndarray format for the input. Note Audio models being used for serving that intend to utilize pre-formatted audio in np.ndarray format\nmust have the model saved with a signature configuration that reflects this schema. Failure to do so will result in type casting errors due to the default signature for\naudio transformers pipelines being set as expecting binary ( bytes ) data. The serving endpoint cannot accept a union of types, so a particular model instance must choose one\nor the other as an allowed input type. ",
        "id": "fbe27c0a41a4edbdb1dd3eb6de0bbcac"
    },
    {
        "text": " PEFT Models in MLflow Transformers flavor Warning The PEFT model is supported in MLflow 2.11.0 and above and is still in the experimental stage. The API and behavior may change in future releases. Moreover, the PEFT library is under active development, so not all features\nand adapter types might be supported in MLflow. PEFT is a library developed by HuggingFace\u00f0\u009f\u00a4\u0097, that provides various optimization methods for pretrained models available on the HuggingFace Hub. With PEFT, you can easily apply various optimization techniques like LoRA and QLoRA to reduce the cost of fine-tuning Transformers models. For example, LoRA (Low-Rank Adaptation) is a method that approximate the weight updates of fine-tuning process with two smaller matrices through low-rank decomposition. LoRA typically shrinks the number of parameters to train to only 0.01% ~ a few % of the full model fine-tuning (depending on the configuration), which significantly accelerates the fine-tuning process and reduces the memory footprint, such that you can even train a Mistral/Llama2 7B model on a single Nvidia A10G GPU in an hour .\nBy using PEFT, you can apply LoRA to your Transformers model with only a few lines of code: from peft import LoraConfig , get_peft_model base_model = AutoModelForCausalLM . from_pretrained ( ... ) lora_config = LoraConfig ( ... ) peft_model = get_peft_model ( base_model , lora_config ) In MLflow 2.11.0, we introduced support for tracking PEFT models in the MLflow Transformers flavor. You can log and load PEFT models using the same APIs as other Transformers models, such as mlflow.transformers.log_model() and mlflow.transformers.load_model() . import mlflow from peft import LoraConfig , get_peft_model from transformers import AutoModelForCausalLM , AutoTokenizer model_id = \"databricks/dolly-v2-7b\" base_model = AutoModelForCausalLM . from_pretrained ( model_id ) tokenizer = AutoTokenizer . from_pretrained ( model_id ) peft_config = LoraConfig ( ... ) peft_model = get_peft_model ( base_model , peft_config ) with mlflow . start_run (): # Your training code here ... # Log the PEFT model model_info = mlflow . transformers . log_model ( transformers_model = { \"model\" : peft_model , \"tokenizer\" : tokenizer , }, artifact_path = \"peft_model\" , ) # Load the PEFT model loaded_model = mlflow . transformers . load_model ( model_info . model_uri )  PEFT Models in MLflow Tutorial Check out the tutorial Fine-Tuning Open-Source LLM using QLoRA with MLflow and PEFT for a more in-depth guide on how to use PEFT with MLflow,  Format of Saved PEFT Model When saving PEFT models, MLflow only saves the PEFT adapter and the configuration, but not the base model\u00e2\u0080\u0099s weights. This is the same behavior as the Transformer\u00e2\u0080\u0099s save_pretrained() method and is highly efficient in terms of storage space and logging latency. One difference is that MLflow will also save the HuggingFace Hub repository name and version for the base model in the model metadata, so that it can load the same base model when loading the PEFT model. Concretely, the following artifacts are saved in MLflow for PEFT models: The PEFT adapter weight under the /peft directory. The PEFT configuration as a JSON file under the /peft directory. The HuggingFace Hub repository name and commit hash for the base model in the MLModel metadata file.  Limitations of PEFT Models in MLflow Since the model saving/loading behavior for PEFT models is similar to that of save_pretrained=False , the same caveats apply to PEFT models. For example, the base model weight may be deleted or become private in the HuggingFace Hub, and PEFT models cannot be registered to the legacy Databricks Workspace Model Registry. To save the base model weight for PEFT models, you can use the mlflow.transformers.persist_pretrained_model() API. This will download the base model weight from the HuggingFace Hub and save it to the artifact location, updating the metadata of the given PEFT model. Please refer to this section for the detailed usage of this API. Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "ad0bec4f60ecef0835a3e44bad98ba42"
    },
    {
        "text": "Introduction to MLflow and Transformers 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor Introduction Getting Started with the MLflow Transformers Flavor - Tutorials and Guides Important Details to be aware of with the transformers flavor Logging Large Models Working with tasks for Transformer Pipelines Detailed Documentation Learn more about Transformers MLflow OpenAI Flavor MLflow Sentence-Transformers Flavor MLflow LangChain Flavor MLflow LlamaIndex Flavor MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow Transformers Flavor Introduction to MLflow and Transformers  Introduction to MLflow and Transformers Welcome to our tutorial on leveraging the power of Transformers with MLflow . This guide is designed for beginners, focusing on machine learning workflows and model management. Download this Notebook  What Will You Learn? In this tutorial, you will learn how to: Set up a simple text generation pipeline using the Transformers library. Log the model and its parameters using MLflow. Infer the input and output signature of the model automatically. Simulate serving the model using MLflow and make predictions with it.  Introduction to Transformers Transformers are a type of deep learning model that have revolutionized natural language processing (NLP). Developed by \u00f0\u009f\u00a4\u0097 Hugging Face , the Transformers library offers a variety of state-of-the-art pre-trained models for NLP tasks.  Why Combine MLflow with Transformers? Integrating MLflow with Transformers offers numerous benefits: Experiment Tracking : Log and compare model parameters and metrics. Model Management : Track different model versions and their performance. Reproducibility : Document all aspects needed to reproduce predictions. Deployment : Simplify deploying Transformers models to production. Now, let\u00e2\u0080\u0099s dive into the world of MLflow and Transformers! [1]: # Disable tokenizers warnings when constructing pipelines % env TOKENIZERS_PARALLELISM=false import warnings # Disable a few less-than-useful UserWarnings from setuptools and pydantic warnings . filterwarnings ( \"ignore\" , category = UserWarning ) env: TOKENIZERS_PARALLELISM=false  Imports and Pipeline configuration In this first section, we are setting up our environment and configuring aspects of the transformers pipeline that we\u00e2\u0080\u0099ll be using to generate a text response from the LLM.  Setting up our Pipeline Import : We import the necessary libraries: transformers for building our NLP model and mlflow for model tracking and management. Task Definition : We then define the task for our pipeline, which in this case is `text2text-generation`` This task involves generating new text based on the input text. Pipeline Declaration : Next, we create a generation_pipeline using the pipeline function from the Transformers library. This pipeline is configured to use the declare-lab/flan-alpaca-large model , which is a pre-trained model suitable for text generation. Input Example : For the purposes of generating a signature later on, as well as having a visual indicator of the expected input data to our model when loading as a pyfunc , we next set up an input_example that contains sample prompts. Inference Parameters : Finally, we define parameters that will be used to control the behavior of the model during inference, such as the maximum length of the generated text and whether to sample multiple times. ",
        "id": "01901ac2b8f37e1b5a3d1e322311144c"
    },
    {
        "text": " Understanding Pipelines Pipelines are a high-level abstraction provided by the Transformers library that simplifies the process of using models for inference. They encapsulate the complexity of the underlying code, offering a straightforward API for a variety of tasks, such as text classification, question answering, and in our case, text generation.  The pipeline() function The pipeline() function is a versatile tool that can be used to create a pipeline for any supported task. When you specify a task, the function returns a pipeline object tailored for that task, constructing the required calls to sub-components (a tokenizer, encoder, generative model, etc.) in the order needed to fulfill the needs of the specified task. This abstraction dramatically simplifies the code required to use these models and their respective components.  Task-Specific Pipelines In addition to the general pipeline() function, there are task-specific pipelines for different domains like audio, computer vision, and natural language processing. These specialized pipelines are optimized for their respective tasks and can provide additional convenience and functionality.  Benefits of Using Pipelines Using pipelines has several advantages: Simplicity : You can perform complex tasks with a minimal amount of code. Flexibility : You can specify different models and configurations to customize the pipeline for your needs. Efficiency : Pipelines handle batching and dataset iteration internally, which can lead to performance improvements. Due to the utility and simple, high-level API, MLflow\u00e2\u0080\u0099s transformers implementation uses the pipeline abstraction by default (although it can support component-only mode as well). [2]: import transformers import mlflow # Define the task that we want to use (required for proper pipeline construction) task = \"text2text-generation\" # Define the pipeline, using the task and a model instance that is applicable for our task. generation_pipeline = transformers . pipeline ( task = task , model = \"declare-lab/flan-alpaca-large\" , ) # Define a simple input example that will be recorded with the model in MLflow, giving # users of the model an indication of the expected input format. input_example = [ \"prompt 1\" , \"prompt 2\" , \"prompt 3\" ] # Define the parameters (and their defaults) for optional overrides at inference time. parameters = { \"max_length\" : 512 , \"do_sample\" : True , \"temperature\" : 0.4 }  Introduction to Model Signatures in MLflow In the realm of machine learning, model signatures play a crucial role in ensuring that models receive and produce the expected data types and structures. MLflow includes a feature for defining model signatures, helping to standardize and enforce correct model usage.  Quick Learning Points Model Signature Purpose : Ensures consistent data types and structures for model inputs and outputs. Visibility and Validation : Visible in MLflow\u00e2\u0080\u0099s UI and validated by MLflow\u00e2\u0080\u0099s deployment tools. Signature Types : Column-based for tabular data, tensor-based for tensor inputs/outputs, and with params for models requiring additional inference parameters.  Understanding Model Signatures A model signature in MLflow describes the schema for inputs, outputs, and parameters of an ML model. It is a blueprint that details the expected data types and shapes, facilitating a clear interface for model usage. Signatures are particularly useful as they are: Displayed in MLflow\u00e2\u0080\u0099s UI for easy reference. Employed by MLflow\u00e2\u0080\u0099s deployment tools to validate inputs during inference. Stored in a standardized JSON format alongside the model\u00e2\u0080\u0099s metadata.  The Role of Signatures in Code In the following section, we are using MLflow to infer the signature of a machine learning model. This involves specifying an input example, generating a model output example, and defining any additional inference parameters. The resulting signature is used to validate future inputs and document the expected data formats. ",
        "id": "48cc6b212d3cc3070bd7d2a13141b07c"
    },
    {
        "text": " Types of Model Signatures Model signatures can be: Column-based : Suitable for models that operate on tabular data, with each column having a specified data type and optional name. Tensor-based : Designed for models that take tensors as inputs and outputs, with each tensor having a specified data type, shape, and optional name. With Params : Some models require additional parameters for inference, which can also be included in the signature. For the transformers flavor, all input types are of the Column-based type (referred to within MLflow as ColSpec types).  Signature Enforcement MLflow enforces the signature at the time of model inference, ensuring that the provided input and parameters match the expected schema. If there\u00e2\u0080\u0099s a mismatch, MLflow will raise an exception or issue a warning, depending on the nature of the mismatch. [3]: # Generate the signature for the model that will be used for inference validation and type checking (as well as validation of parameters being submitted during inference) signature = mlflow . models . infer_signature ( input_example , mlflow . transformers . generate_signature_output ( generation_pipeline , input_example ), parameters , ) # Visualize the signature signature [3]: inputs:\n  [string]\noutputs:\n  [string]\nparams:\n  ['max_length': long (default: 512), 'do_sample': boolean (default: True), 'temperature': double (default: 0.4)]  Creating an experiment We create a new MLflow Experiment so that the run we\u00e2\u0080\u0099re going to log our model to does not log to the default experiment and instead has its own contextually relevant entry. [4]: # If you are running this tutorial in local mode, leave the next line commented out. # Otherwise, uncomment the following line and set your tracking uri to your local or remote tracking server. # mlflow.set_tracking_uri(\"http://127.0.0.1:8080\") mlflow . set_experiment ( \"Transformers Introduction\" ) [4]: <Experiment: artifact_location='file:///Users/benjamin.wilson/repos/mlflow-fork/mlflow/docs/source/llms/transformers/tutorials/text-generation/mlruns/528654983476503755', creation_time=1701288466448, experiment_id='528654983476503755', last_update_time=1701288466448, lifecycle_stage='active', name='Transformers Introduction', tags={}>  Logging the Transformers Model with MLflow We log our model with MLflow to manage its lifecycle efficiently and keep track of its versions and configurations. Logging a model in MLflow is a crucial step in the model lifecycle management, enabling efficient tracking, versioning, and management. The process involves registering the model along with its essential metadata within the MLflow tracking system. Utilizing the mlflow.transformers.log_model function, specifically tailored for models and components from the transformers library, simplifies this task. This function is adept at handling various aspects of the models from this library, including their complex pipelines and configurations. When logging the model, crucial metadata such as the model\u00e2\u0080\u0099s signature, which was previously established, is included. This metadata plays a significant role in the subsequent stages of the model\u00e2\u0080\u0099s lifecycle, from tracking its evolution to facilitating its deployment in different environments. The signature, in particular, ensures the model\u00e2\u0080\u0099s compatibility and consistent performance across various platforms, thereby enhancing its utility and reliability in practical applications. By logging our model in this way, we ensure that it is not only well-documented but also primed for future use, whether it be for further development, comparative analysis, or deployment. ",
        "id": "e3374a82564091351d89713d4c576e6e"
    },
    {
        "text": " A Tip for Saving Storage Cost When you call mlflow.transformers.log_model , MLflow saves a full copy of the Transformers model weight. However, this could take large storage space (3GB for flan-alpaca-large model), but might be redundant when you don\u00e2\u0080\u0099t modify the model weight, because it is exactly same as the one you can download from the HuggingFace model hub. To avoid the unnecessary copy, you can use \u00e2\u0080\u0098reference-only\u00e2\u0080\u0099 save mode which is introduced in MLflow 2.11.0, by setting save_pretrained=False when logging or saving the Transformer model. This tells MLflow not to save the copy of the base model weight, but just a reference to the HuggingFace Hub repository and version, hence more storage-efficient and faster in time. For more details about this feature, please refer to Storage-Efficient Model\nLogging . [5]: with mlflow . start_run (): model_info = mlflow . transformers . log_model ( transformers_model = generation_pipeline , artifact_path = \"text_generator\" , input_example = input_example , signature = signature , # Transformer model does not use Pandas Dataframe as input, internal input type conversion should be skipped. example_no_conversion = True , # Uncomment the following line to save the model in 'reference-only' mode: # save_pretrained=False, )  Loading the Text Generation Model We initialize our text generation model using MLflow\u00e2\u0080\u0099s pyfunc module for seamless model loading and interaction. The pyfunc module in MLflow serves as a generic wrapper for Python functions. Its application in MLflow facilitates the loading of machine learning models as standard Python functions. This approach is especially advantageous for models logged or registered via MLflow, streamlining the interaction with the model regardless of its training or serialization specifics. Utilizing mlflow.pyfunc.load_model , our previously logged text generation model is loaded using its unique model URI. This URI is a reference to the stored model artifacts. MLflow efficiently handles the model\u00e2\u0080\u0099s deserialization, along with any associated dependencies, preparing it for immediate use. Once the model, referred to as sentence_generator , is loaded, it operates as a conventional Python function. This functionality allows for the generation of text based on given prompts. The model encompasses the complete process of inference, eliminating the need for manual input preprocessing or output postprocessing. This encapsulation not only simplifies model interaction but also ensures the model\u00e2\u0080\u0099s adaptability for deployment across various platforms. [6]: # Load our pipeline as a generic python function sentence_generator = mlflow . pyfunc . load_model ( model_info . model_uri )  Formatting Predictions for Tutorial Readability Please note that the following function, format_predictions , is used only for enhancing the readability of model predictions within this Jupyter Notebook environment. It is not a standard component of the model\u00e2\u0080\u0099s inference pipeline. [7]: def format_predictions ( predictions ): \"\"\" Function for formatting the output for readability in a Jupyter Notebook \"\"\" formatted_predictions = [] for prediction in predictions : # Split the output into sentences, ensuring we don't split on abbreviations or initials sentences = [ sentence . strip () + ( \".\" if not sentence . endswith ( \".\" ) else \"\" ) for sentence in prediction . split ( \". \" ) if sentence ] # Join the sentences with a newline character formatted_text = \" \\n \" . join ( sentences ) # Add the formatted text to the list formatted_predictions . append ( formatted_text ) return formatted_predictions  Generating Predictions with Custom Parameters In this section, we demonstrate generating predictions using a sentence generator model with custom parameters. This includes prompts for selecting weekend activities and requesting a joke.  Quick Overview Scenario : Generating text for different prompts. Custom Parameter : Overriding the temperature parameter to control text randomness. Default Values : Other parameters use their defaults unless explicitly overridden. ",
        "id": "5d7bca0aa224d0b4636aa78b6f132116"
    },
    {
        "text": " Prediction Process Explained We use the predict method on the sentence_generator pyfunc model with a list of string prompts, including: A request for help in choosing between hiking and kayaking for a weekend activity. A prompt asking for a joke related to hiking. To influence the generation process, we override the temperature parameter. This parameter impacts the randomness of the generated text: Lower Temperature : Leads to more predictable and conservative outputs. Higher Temperature : Fosters varied and creative responses.  Utilizing Custom Parameters In this example, the temperature parameter is explicitly set for the prediction call. Other parameters set during model logging will use their default values, unless also overridden in the params argument of the prediction call.  Output Formatting The predictions variable captures the model\u00e2\u0080\u0099s output for each input prompt. We can format these outputs for enhanced readability in the following steps, presenting the generated text in a clear and accessible manner. [8]: # Validate that our loaded pipeline, as a generic pyfunc, can produce an output that makes sense predictions = sentence_generator . predict ( data = [ \"I can't decide whether to go hiking or kayaking this weekend. Can you help me decide?\" , \"Please tell me a joke about hiking.\" , ], params = { \"temperature\" : 0.7 }, ) # Format each prediction for notebook readability formatted_predictions = format_predictions ( predictions ) for i , formatted_text in enumerate ( formatted_predictions ): print ( f \"Response to prompt { i + 1 } : \\n { formatted_text } \\n \" ) 2023/11/30 14:24:08 WARNING mlflow.transformers: params provided to the `predict` method will override the inference configuration saved with the model. If the params provided are not valid for the pipeline, MlflowException will be raised. Response to prompt 1:\nGoing hiking can be a great way to explore the outdoors and have fun, while kayaking can be an exciting way to take in the scenery and have a great experience.\n\nResponse to prompt 2:\nQ: What did the bird say when he was walking in the woods? a: \"Hey, I'm going to get some food!\".  Closing Remarks This demonstration showcases the flexibility and power of the model in generating contextually relevant and creative text responses. By formatting the outputs, we ensure that the results are not only accurate but also presented in a manner that is easy to read and understand, enhancing the overall user experience within this Jupyter Notebook environment. Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "f65e05123962ac08915e5c4ebade7443"
    },
    {
        "text": "Fine-Tuning Transformers with MLflow for Enhanced Model Management 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor Introduction Getting Started with the MLflow Transformers Flavor - Tutorials and Guides Important Details to be aware of with the transformers flavor Logging Large Models Working with tasks for Transformer Pipelines Detailed Documentation Learn more about Transformers MLflow OpenAI Flavor MLflow Sentence-Transformers Flavor MLflow LangChain Flavor MLflow LlamaIndex Flavor MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow Transformers Flavor Fine-Tuning Transformers with MLflow for Enhanced Model Management  Fine-Tuning Transformers with MLflow for Enhanced Model Management Welcome to our in-depth tutorial on fine-tuning Transformers models with enhanced management using MLflow. Download the Fine Tuning Notebook  What You Will Learn in This Tutorial Understand the process of fine-tuning a Transformers model. Learn to effectively log and manage the training cycle using MLflow. Master logging the trained model separately in MLflow. Gain insights into using the trained model for practical inference tasks. Our approach will provide a holistic understanding of model fine-tuning and management, ensuring that you\u00e2\u0080\u0099re well-equipped to handle similar tasks in your projects.  Emphasizing Fine-Tuning Fine-tuning pre-trained models is a common practice in machine learning, especially in the field of NLP. It involves adjusting a pre-trained model to make it more suitable for a specific task. This process is essential as it allows the leveraging of pre-existing knowledge in the model, significantly improving performance on specific datasets or tasks.  Role of MLflow in Model Lifecycle Integrating MLflow in this process is crucial for: Training Cycle Logging : Keeping a detailed log of the training cycle, including parameters, metrics, and intermediate results. Model Logging and Management : Separately logging the trained model, tracking its versions, and managing its lifecycle post-training. Inference and Deployment : Using the logged model for inference, ensuring easy transition from training to deployment. [1]: # Disable tokenizers warnings when constructing pipelines % env TOKENIZERS_PARALLELISM=false import warnings # Disable a few less-than-useful UserWarnings from setuptools and pydantic warnings . filterwarnings ( \"ignore\" , category = UserWarning ) env: TOKENIZERS_PARALLELISM=false  Preparing the Dataset and Environment for Fine-Tuning ",
        "id": "6844afd59b486dcc69cd07347fde0054"
    },
    {
        "text": " Key Steps in this Section Loading the Dataset : Utilizing the sms_spam dataset for spam detection. Splitting the Dataset : Dividing the dataset into training and test sets with an 80/20 distribution. Importing Necessary Libraries : Including libraries like evaluate , mlflow , numpy , and essential components from the transformers library. Before diving into the fine-tuning process, setting up our environment and preparing the dataset is crucial. This step involves loading the dataset, splitting it into training and testing sets, and initializing essential components of the Transformers library. These preparatory steps lay the groundwork for an efficient fine-tuning process. This setup ensures that we have a solid foundation for fine-tuning our model, with all the necessary data and tools at our disposal. In the following Python code, we\u00e2\u0080\u0099ll execute these steps to kickstart our model fine-tuning journey. [2]: import evaluate import numpy as np from datasets import load_dataset from transformers import ( AutoModelForSequenceClassification , AutoTokenizer , Trainer , TrainingArguments , pipeline , ) import mlflow # Load the \"sms_spam\" dataset. sms_dataset = load_dataset ( \"sms_spam\" ) # Split train/test by an 8/2 ratio. sms_train_test = sms_dataset [ \"train\" ] . train_test_split ( test_size = 0.2 ) train_dataset = sms_train_test [ \"train\" ] test_dataset = sms_train_test [ \"test\" ] Found cached dataset sms_spam (/Users/benjamin.wilson/.cache/huggingface/datasets/sms_spam/plain_text/1.0.0/53f051d3b5f62d99d61792c91acefe4f1577ad3e4c216fb0ad39e30b9f20019c)  Tokenization and Dataset Preparation In the next code block, we tokenize our text data, preparing it for the fine-tuning process of our model. With our dataset loaded and split, the next step is to prepare our text data for the model. This involves tokenizing the text, a crucial process in NLP where text is converted into a format that\u00e2\u0080\u0099s understandable and usable by our model.  Tokenization Process Loading the Tokenizer : Using the AutoTokenizer from the transformers library for the distilbert-base-uncased model\u00e2\u0080\u0099s tokenizer. Defining the Tokenization Function : Creating a function to tokenize text data, including padding and truncation. Applying Tokenization to the Dataset : Processing both the training and testing sets for model readiness. Tokenization is a critical step in preparing text data for NLP tasks. It ensures that the data is in a format that the model can process, and by handling aspects like padding and truncation, it ensures consistency across our dataset, which is vital for training stability and model performance. [3]: # Load the tokenizer for \"distilbert-base-uncased\" model. tokenizer = AutoTokenizer . from_pretrained ( \"distilbert-base-uncased\" ) def tokenize_function ( examples ): # Pad/truncate each text to 512 tokens. Enforcing the same shape # could make the training faster. return tokenizer ( examples [ \"sms\" ], padding = \"max_length\" , truncation = True , max_length = 128 , ) seed = 22 # Tokenize the train and test datasets train_tokenized = train_dataset . map ( tokenize_function ) train_tokenized = train_tokenized . remove_columns ([ \"sms\" ]) . shuffle ( seed = seed ) test_tokenized = test_dataset . map ( tokenize_function ) test_tokenized = test_tokenized . remove_columns ([ \"sms\" ]) . shuffle ( seed = seed )  Model Initialization and Label Mapping Next, we\u00e2\u0080\u0099ll set up label mappings and initialize the model for our text classification task. Having prepared our data, the next crucial step is to initialize our model and set up label mappings. This involves defining a clear relationship between the labels in our dataset and their corresponding representations in the model.  Setting Up Label Mappings Defining Label Mappings : Creating bi-directional mappings between integer labels and textual representations (\u00e2\u0080\u009cham\u00e2\u0080\u009d and \u00e2\u0080\u009cspam\u00e2\u0080\u009d). ",
        "id": "38cac30fa18887205ddc2068f5652383"
    },
    {
        "text": " Initializing the Model Model Selection : Choosing the distilbert-base-uncased model for its balance of performance and efficiency. Model Configuration : Configuring the model for sequence classification with the defined label mappings. Proper model initialization and label mapping are key to ensuring that the model accurately understands and processes the task at hand. By explicitly defining these mappings and selecting an appropriate pre-trained model, we lay the groundwork for effective and efficient fine-tuning. [4]: # Set the mapping between int label and its meaning. id2label = { 0 : \"ham\" , 1 : \"spam\" } label2id = { \"ham\" : 0 , \"spam\" : 1 } # Acquire the model from the Hugging Face Hub, providing label and id mappings so that both we and the model can 'speak' the same language. model = AutoModelForSequenceClassification . from_pretrained ( \"distilbert-base-uncased\" , num_labels = 2 , label2id = label2id , id2label = id2label , ) Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.  Setting Up Evaluation Metrics Next, we focus on defining and computing evaluation metrics to measure our model\u00e2\u0080\u0099s performance accurately. After initializing our model, the next critical step is to define how we\u00e2\u0080\u0099ll evaluate its performance. Accurate evaluation is key to understanding how well our model is learning and performing on the task.  Choosing and Loading the Metric Metric Selection : Opting for \u00e2\u0080\u0098accuracy\u00e2\u0080\u0099 as the evaluation metric. Loading the Metric : Utilizing the evaluate library to load the \u00e2\u0080\u0098accuracy\u00e2\u0080\u0099 metric.  Defining the Metric Computation Function Function for Metric Computation : Creating a function, compute_metrics , for calculating accuracy during model evaluation. Processing Predictions : Handling logits and labels from predictions to compute accuracy. Properly setting up evaluation metrics allows us to objectively measure the model\u00e2\u0080\u0099s performance. By using standardized metrics, we can compare our model\u00e2\u0080\u0099s performance against benchmarks or other models, ensuring that our fine-tuning process is effective and moving in the right direction. [5]: # Define the target optimization metric metric = evaluate . load ( \"accuracy\" ) # Define a function for calculating our defined target optimization metric during training def compute_metrics ( eval_pred ): logits , labels = eval_pred predictions = np . argmax ( logits , axis =- 1 ) return metric . compute ( predictions = predictions , references = labels )  Configuring the Training Environment In this step, we\u00e2\u0080\u0099re going to configure our Trainer, supplying important training configurations via the use of the TrainingArguments API. With our model and metrics ready, the next important step is to configure the training environment. This involves setting up the training arguments and initializing the Trainer, a component that orchestrates the model training process.  Training Arguments Configuration Defining the Output Directory : We specify the training_output_dir where our model checkpoints will be saved during training. This helps in managing and storing model states at different stages of training. Specifying Training Arguments : We create an instance of TrainingArguments to define various parameters for training, such as the output directory, evaluation strategy, batch sizes for training and evaluation, logging frequency, and the number of training epochs. These parameters are critical for controlling how the model is trained and evaluated. ",
        "id": "16157935544ccd9912159886eab4e23a"
    },
    {
        "text": " Initializing the Trainer Creating the Trainer Instance : We use the Trainer class from the Transformers library, providing it with our model, the previously defined training arguments, datasets for training and evaluation, and the function to compute metrics. Role of the Trainer : The Trainer handles all aspects of training and evaluating the model, including the execution of training loops, handling of data batching, and calling the compute metrics function. It simplifies the training process, making it more streamlined and efficient.  Importance of Proper Training Configuration Setting up the training environment correctly is essential for effective model training. Proper configuration ensures that the model is trained under optimal conditions, leading to better performance and more reliable results. In the following code block, we\u00e2\u0080\u0099ll configure our training environment and initialize the Trainer, setting the stage for the actual training process. [6]: # Checkpoints will be output to this `training_output_dir`. training_output_dir = \"/tmp/sms_trainer\" training_args = TrainingArguments ( output_dir = training_output_dir , evaluation_strategy = \"epoch\" , per_device_train_batch_size = 8 , per_device_eval_batch_size = 8 , logging_steps = 8 , num_train_epochs = 3 , ) # Instantiate a `Trainer` instance that will be used to initiate a training run. trainer = Trainer ( model = model , args = training_args , train_dataset = train_tokenized , eval_dataset = test_tokenized , compute_metrics = compute_metrics , ) [7]: # If you are running this tutorial in local mode, leave the next line commented out. # Otherwise, uncomment the following line and set your tracking uri to your local or remote tracking server. # mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")  Integrating MLflow for Experiment Tracking The final preparatory step before beginning the training process is to integrate MLflow for experiment tracking. MLflow is a critical tool in our workflow, enabling us to log, monitor, and compare different runs of our model training.  Setting up the MLflow Experiment Naming the Experiment : We use mlflow.set_experiment to create a new experiment or assign the current run to an existing experiment. In this case, we name our experiment \u00e2\u0080\u009cSpam Classifier Training\u00e2\u0080\u009d. This name should be descriptive and related to the task at hand, aiding in organizing and identifying experiments later. Role of MLflow in Training : By setting up an MLflow experiment, we can track various aspects of our model training, such as parameters, metrics, and outputs. This tracking is invaluable for comparing different models, tuning hyperparameters, and maintaining a record of our experiments.  Benefits of Experiment Tracking Utilizing MLflow for experiment tracking offers several advantages: Organization : Keeps your training runs organized and easily accessible. Comparability : Allows for easy comparison of different training runs to understand the impact of changes in parameters or data. Reproducibility : Enhances the reproducibility of experiments by logging all necessary details. With MLflow set up, we\u00e2\u0080\u0099re now ready to begin the training process, keeping track of every important aspect along the way. In the next code snippet, we\u00e2\u0080\u0099ll set up our MLflow experiment for tracking the training of our spam classification model. [8]: # Pick a name that you like and reflects the nature of the runs that you will be recording to the experiment. mlflow . set_experiment ( \"Spam Classifier Training\" ) [8]: <Experiment: artifact_location='file:///Users/benjamin.wilson/repos/mlflow-fork/mlflow/docs/source/llms/transformers/tutorials/fine-tuning/mlruns/258758267044147956', creation_time=1701291176206, experiment_id='258758267044147956', last_update_time=1701291176206, lifecycle_stage='active', name='Spam Classifier Training', tags={}> ",
        "id": "30ba4a98647003fd3988d23345c94d3a"
    },
    {
        "text": " Starting the Training Process with MLflow In this step, we initiate the fine-tuning training run, utilizing the native auto-logging functionality to record the parameters used and loss metrics calculated during the training process. With our model, training arguments, and MLflow experiment set up, we are now ready to start the actual training process. This step involves initiating an MLflow run, which will encapsulate all the training activities and metrics.  Initiating the MLflow Run Starting an MLflow Run : We use mlflow.start_run() to begin a new MLflow run. This function creates a new run context, under which all the training operations and logging will occur. Training the Model : Inside the MLflow run context, we call trainer.train() to start training our model. This function will run the training loop, processing the data in batches, updating model parameters, and evaluating the model. ",
        "id": "7ffef6dd826b710670c840b66d66c9ee"
    },
    {
        "text": " Monitoring the Training Progress During training, the Trainer object will output logs that provide valuable insights into the training progress: Loss : Indicates the model\u00e2\u0080\u0099s performance, with lower values signifying better performance. Learning Rate : Shows the current learning rate used during training. Epoch Progress : Displays the progress through the current epoch. These logs are crucial for monitoring the model\u00e2\u0080\u0099s learning process and making any necessary adjustments. By tracking these metrics within an MLflow run, we can maintain a comprehensive record of the training process, enhancing reproducibility and analysis. In the next code block, we will start our MLflow run and begin training our model, closely observing the output to gauge the training progress. [9]: with mlflow . start_run () as run : trainer . train () {'loss': 0.4891, 'learning_rate': 4.9761051373954604e-05, 'epoch': 0.01}\n{'loss': 0.2662, 'learning_rate': 4.95221027479092e-05, 'epoch': 0.03}\n{'loss': 0.1756, 'learning_rate': 4.92831541218638e-05, 'epoch': 0.04}\n{'loss': 0.107, 'learning_rate': 4.90442054958184e-05, 'epoch': 0.06}\n{'loss': 0.0831, 'learning_rate': 4.8805256869773e-05, 'epoch': 0.07}\n{'loss': 0.0688, 'learning_rate': 4.8566308243727596e-05, 'epoch': 0.09}\n{'loss': 0.0959, 'learning_rate': 4.83273596176822e-05, 'epoch': 0.1}\n{'loss': 0.0831, 'learning_rate': 4.80884109916368e-05, 'epoch': 0.11}\n{'loss': 0.1653, 'learning_rate': 4.78494623655914e-05, 'epoch': 0.13}\n{'loss': 0.1865, 'learning_rate': 4.7610513739546e-05, 'epoch': 0.14}\n{'loss': 0.0887, 'learning_rate': 4.73715651135006e-05, 'epoch': 0.16}\n{'loss': 0.1009, 'learning_rate': 4.71326164874552e-05, 'epoch': 0.17}\n{'loss': 0.1017, 'learning_rate': 4.6893667861409805e-05, 'epoch': 0.19}\n{'loss': 0.0057, 'learning_rate': 4.66547192353644e-05, 'epoch': 0.2}\n{'loss': 0.0157, 'learning_rate': 4.6415770609319e-05, 'epoch': 0.22}\n{'loss': 0.0302, 'learning_rate': 4.61768219832736e-05, 'epoch': 0.23}\n{'loss': 0.0013, 'learning_rate': 4.59378733572282e-05, 'epoch': 0.24}\n{'loss': 0.0863, 'learning_rate': 4.56989247311828e-05, 'epoch': 0.26}\n{'loss': 0.1122, 'learning_rate': 4.54599761051374e-05, 'epoch': 0.27}\n{'loss': 0.1092, 'learning_rate': 4.5221027479092e-05, 'epoch': 0.29}\n{'loss': 0.0853, 'learning_rate': 4.49820788530466e-05, 'epoch': 0.3}\n{'loss': 0.1852, 'learning_rate': 4.4743130227001195e-05, 'epoch': 0.32}\n{'loss': 0.0913, 'learning_rate': 4.4504181600955796e-05, 'epoch': 0.33}\n{'loss': 0.0232, 'learning_rate': 4.42652329749104e-05, 'epoch': 0.34}\n{'loss': 0.0888, 'learning_rate': 4.402628434886499e-05, 'epoch': 0.36}\n{'loss': 0.195, 'learning_rate': 4.378733572281959e-05, 'epoch': 0.37}\n{'loss': 0.0198, 'learning_rate': 4.3548387096774194e-05, 'epoch': 0.39}\n{'loss': 0.056, 'learning_rate': 4.3309438470728796e-05, 'epoch': 0.4}\n{'loss': 0.1656, 'learning_rate': 4.307048984468339e-05, 'epoch': 0.42}\n{'loss': 0.0032, 'learning_rate': 4.283154121863799e-05, 'epoch': 0.43}\n{'loss': 0.1277, 'learning_rate': 4.259259259259259e-05, 'epoch': 0.44}\n{'loss': 0.0029, 'learning_rate': 4.2353643966547194e-05, 'epoch': 0.46}\n{'loss': 0.1007, 'learning_rate': 4.2114695340501795e-05, 'epoch': 0.47}\n{'loss': 0.0038, 'learning_rate': 4.1875746714456396e-05, 'epoch': 0.49}\n{'loss': 0.0035, 'learning_rate': 4.1636798088411e-05, 'epoch': 0.5}\n{'loss': 0.0015, 'learning_rate': 4.13978494623656e-05, 'epoch': 0.52}\n{'loss': 0.1423, 'learning_rate': 4.115890083632019e-05, 'epoch': 0.53}\n{'loss': 0.0316, 'learning_rate': 4.0919952210274794e-05, 'epoch': 0.54}\n{'loss': 0.0012, 'learning_rate': 4.0681003584229395e-05, 'epoch': 0.56}\n{'loss': 0.0009, 'learning_rate': 4.0442054958183996e-05, 'epoch': 0.57}\n{'loss': 0.1287, 'learning_rate': 4.02",
        "id": "8bc9e1a2a741529170a98ce99147f312"
    },
    {
        "text": " 'epoch': 0.5}\n{'loss': 0.0015, 'learning_rate': 4.13978494623656e-05, 'epoch': 0.52}\n{'loss': 0.1423, 'learning_rate': 4.115890083632019e-05, 'epoch': 0.53}\n{'loss': 0.0316, 'learning_rate': 4.0919952210274794e-05, 'epoch': 0.54}\n{'loss': 0.0012, 'learning_rate': 4.0681003584229395e-05, 'epoch': 0.56}\n{'loss': 0.0009, 'learning_rate': 4.0442054958183996e-05, 'epoch': 0.57}\n{'loss': 0.1287, 'learning_rate': 4.020310633213859e-05, 'epoch': 0.59}\n{'loss': 0.0893, 'learning_rate': 3.996415770609319e-05, 'epoch': 0.6}\n{'loss': 0.0021, 'learning_rate': 3.972520908004779e-05, 'epoch': 0.62}\n{'loss': 0.0031, 'learning_rate': 3.9486260454002395e-05, 'epoch': 0.63}\n{'loss': 0.0022, 'learning_rate': 3.924731182795699e-05, 'epoch': 0.65}\n{'loss': 0.0008, 'learning_rate': 3.900836320191159e-05, 'epoch': 0.66}\n{'loss': 0.1119, 'learning_rate': 3.876941457586619e-05, 'epoch': 0.67}\n{'loss': 0.0012, 'learning_rate': 3.8530465949820786e-05, 'epoch': 0.69}\n{'loss': 0.2618, 'learning_rate': 3.829151732377539e-05, 'epoch': 0.7}\n{'loss': 0.0018, 'learning_rate': 3.805256869772999e-05, 'epoch': 0.72}\n{'loss': 0.0736, 'learning_rate': 3.781362007168459e-05, 'epoch': 0.73}\n{'loss': 0.0126, 'learning_rate': 3.7574671445639184e-05, 'epoch': 0.75}\n{'loss': 0.2125, 'learning_rate': 3.7335722819593785e-05, 'epoch': 0.76}\n{'loss': 0.0018, 'learning_rate': 3.7096774193548386e-05, 'epoch': 0.77}\n{'loss': 0.1386, 'learning_rate': 3.685782556750299e-05, 'epoch': 0.79}\n{'loss': 0.0024, 'learning_rate': 3.661887694145759e-05, 'epoch': 0.8}\n{'loss': 0.0016, 'learning_rate': 3.637992831541219e-05, 'epoch': 0.82}\n{'loss': 0.0011, 'learning_rate': 3.614097968936679e-05, 'epoch': 0.83}\n{'loss': 0.0307, 'learning_rate': 3.590203106332139e-05, 'epoch': 0.85}\n{'loss': 0.0007, 'learning_rate': 3.566308243727599e-05, 'epoch': 0.86}\n{'loss': 0.005, 'learning_rate': 3.542413381123059e-05, 'epoch': 0.87}\n{'loss': 0.0534, 'learning_rate': 3.518518518518519e-05, 'epoch': 0.89}\n{'loss': 0.0155, 'learning_rate': 3.494623655913979e-05, 'epoch': 0.9}\n{'loss': 0.0136, 'learning_rate': 3.4707287933094385e-05, 'epoch': 0.92}\n{'loss': 0.1108, 'learning_rate': 3.4468339307048986e-05, 'epoch': 0.93}\n{'loss': 0.0017, 'learning_rate': 3.422939068100359e-05, 'epoch': 0.95}\n{'loss': 0.0009, 'learning_rate': 3.399044205495819e-05, 'epoch': 0.96}\n{'loss': 0.0008, 'learning_rate': 3.375149342891278e-05, 'epoch': 0.97}\n{'loss': 0.0846, 'learning_rate': 3.3512544802867384e-05, 'epoch': 0.99} {'eval_loss': 0.03877367451786995, 'eval_accuracy': 0.9919282511210762, 'eval_runtime': 5.0257, 'eval_samples_per_second': 221.862, 'eval_steps_per_second': 27.857, 'epoch': 1.0}\n{'loss': 0.109, 'learning_rate': 3.3273596176821985e-05, 'epoch': 1.0}\n{'loss': 0.0084, 'learning_rate': 3.3034647550776586e-05, 'epoch': 1.02}\n{'loss': 0.0014, 'learning_rate': 3.279569892473118e-05, 'epoch': 1.03}\n{'loss': 0.0008, 'learning_rate': 3.255675029868578e-05, 'epoch': 1.05}\n{'loss': 0.0006, 'learning_rate': 3.231780167264038e-05, 'epoch': 1.06}\n{'loss': 0.0005, 'learning_rate': 3.207885304659498e-05, 'epoch': 1.08}\n{'loss': 0.0004, 'learning_rate': 3.183990442054958e-05, 'epoch': 1.09}\n{'loss': 0.0518, 'learning_rate': 3.160095579450418e-05, 'epoch': 1.1}\n{'loss': 0.0005, 'learning_rate': 3.136200716845878e-05, 'epoch': 1.12}\n{'loss': 0.149, 'learning_rate': 3.112305854241338e-05, 'epoch': 1.13}\n{'loss': 0.0022, 'learning_rate': 3.0884109916367984e-05, 'epoch': 1.15}\n{'loss': 0.0013, 'learning_rate': 3.0645161290322585e-05, 'epoch': 1.16}\n{'loss': 0.0051, 'learning_rate': 3.0406212664277183e-05, 'epoch': 1.18}\n{'loss': 0.0005, 'learning_rate': 3.016726403823178e-05, 'epoch': 1.19}\n{'loss': 0.0026, 'learning_rate': 2.9928315412186382e-05, 'epoch': 1.2}\n{'loss'",
        "id": "31c90889f13cf3e020b2cd27c5d2f60e"
    },
    {
        "text": "rate': 3.112305854241338e-05, 'epoch': 1.13}\n{'loss': 0.0022, 'learning_rate': 3.0884109916367984e-05, 'epoch': 1.15}\n{'loss': 0.0013, 'learning_rate': 3.0645161290322585e-05, 'epoch': 1.16}\n{'loss': 0.0051, 'learning_rate': 3.0406212664277183e-05, 'epoch': 1.18}\n{'loss': 0.0005, 'learning_rate': 3.016726403823178e-05, 'epoch': 1.19}\n{'loss': 0.0026, 'learning_rate': 2.9928315412186382e-05, 'epoch': 1.2}\n{'loss': 0.0005, 'learning_rate': 2.9689366786140983e-05, 'epoch': 1.22}\n{'loss': 0.0871, 'learning_rate': 2.9450418160095584e-05, 'epoch': 1.23}\n{'loss': 0.0004, 'learning_rate': 2.921146953405018e-05, 'epoch': 1.25}\n{'loss': 0.0004, 'learning_rate': 2.897252090800478e-05, 'epoch': 1.26}\n{'loss': 0.0003, 'learning_rate': 2.873357228195938e-05, 'epoch': 1.28}\n{'loss': 0.0003, 'learning_rate': 2.8494623655913982e-05, 'epoch': 1.29}\n{'loss': 0.0003, 'learning_rate': 2.8255675029868577e-05, 'epoch': 1.3}\n{'loss': 0.0478, 'learning_rate': 2.8016726403823178e-05, 'epoch': 1.32}\n{'loss': 0.0002, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}\n{'loss': 0.0002, 'learning_rate': 2.753882915173238e-05, 'epoch': 1.35}\n{'loss': 0.0003, 'learning_rate': 2.7299880525686978e-05, 'epoch': 1.36}\n{'loss': 0.0002, 'learning_rate': 2.706093189964158e-05, 'epoch': 1.38}\n{'loss': 0.0005, 'learning_rate': 2.682198327359618e-05, 'epoch': 1.39}\n{'loss': 0.0002, 'learning_rate': 2.6583034647550775e-05, 'epoch': 1.41}\n{'loss': 0.0003, 'learning_rate': 2.6344086021505376e-05, 'epoch': 1.42}\n{'loss': 0.0002, 'learning_rate': 2.6105137395459977e-05, 'epoch': 1.43}\n{'loss': 0.0002, 'learning_rate': 2.586618876941458e-05, 'epoch': 1.45}\n{'loss': 0.0002, 'learning_rate': 2.5627240143369173e-05, 'epoch': 1.46}\n{'loss': 0.0007, 'learning_rate': 2.5388291517323774e-05, 'epoch': 1.48}\n{'loss': 0.1336, 'learning_rate': 2.5149342891278375e-05, 'epoch': 1.49}\n{'loss': 0.0004, 'learning_rate': 2.4910394265232977e-05, 'epoch': 1.51}\n{'loss': 0.0671, 'learning_rate': 2.4671445639187578e-05, 'epoch': 1.52}\n{'loss': 0.0004, 'learning_rate': 2.4432497013142176e-05, 'epoch': 1.53}\n{'loss': 0.1246, 'learning_rate': 2.4193548387096777e-05, 'epoch': 1.55}\n{'loss': 0.1142, 'learning_rate': 2.3954599761051375e-05, 'epoch': 1.56}\n{'loss': 0.002, 'learning_rate': 2.3715651135005976e-05, 'epoch': 1.58}\n{'loss': 0.002, 'learning_rate': 2.3476702508960574e-05, 'epoch': 1.59}\n{'loss': 0.0009, 'learning_rate': 2.3237753882915175e-05, 'epoch': 1.61}\n{'loss': 0.0778, 'learning_rate': 2.2998805256869773e-05, 'epoch': 1.62}\n{'loss': 0.0007, 'learning_rate': 2.2759856630824374e-05, 'epoch': 1.63}\n{'loss': 0.0008, 'learning_rate': 2.2520908004778972e-05, 'epoch': 1.65}\n{'loss': 0.0009, 'learning_rate': 2.2281959378733573e-05, 'epoch': 1.66}\n{'loss': 0.1032, 'learning_rate': 2.2043010752688174e-05, 'epoch': 1.68}\n{'loss': 0.0014, 'learning_rate': 2.1804062126642775e-05, 'epoch': 1.69}\n{'loss': 0.001, 'learning_rate': 2.1565113500597373e-05, 'epoch': 1.71}\n{'loss': 0.1199, 'learning_rate': 2.132616487455197e-05, 'epoch': 1.72}\n{'loss': 0.0009, 'learning_rate': 2.1087216248506572e-05, 'epoch': 1.73}\n{'loss': 0.0011, 'learning_rate': 2.084826762246117e-05, 'epoch': 1.75}\n{'loss': 0.0007, 'learning_rate': 2.060931899641577e-05, 'epoch': 1.76}\n{'loss': 0.0006, 'learning_rate': 2.037037037037037e-05, 'epoch': 1.78}\n{'loss': 0.0004, 'learning_rate': 2.013142174432497e-05, 'epoch': 1.79}\n{'loss': 0.0005, 'learning_rate': 1.989247311827957e-05, 'epoch': 1.81}\n{'loss': 0.1246, 'learning_rate': 1.9653524492234173e-05, 'epoch': 1.82}\n{'loss': 0.0974, 'learning_rate': 1.941457586618877e-05, 'epoch': 1.84}\n{'loss': 0.0003, 'learning_rate': 1.9175627240143372e-05, 'epoch': 1.85}\n{'loss': 0.0007, 'learning_rate': 1.893667861409797e-05, 'epo",
        "id": "41d0163b616fcc368d0661d49de0978f"
    },
    {
        "text": " 0.0004, 'learning_rate': 2.013142174432497e-05, 'epoch': 1.79}\n{'loss': 0.0005, 'learning_rate': 1.989247311827957e-05, 'epoch': 1.81}\n{'loss': 0.1246, 'learning_rate': 1.9653524492234173e-05, 'epoch': 1.82}\n{'loss': 0.0974, 'learning_rate': 1.941457586618877e-05, 'epoch': 1.84}\n{'loss': 0.0003, 'learning_rate': 1.9175627240143372e-05, 'epoch': 1.85}\n{'loss': 0.0007, 'learning_rate': 1.893667861409797e-05, 'epoch': 1.86}\n{'loss': 0.1998, 'learning_rate': 1.869772998805257e-05, 'epoch': 1.88}\n{'loss': 0.0426, 'learning_rate': 1.845878136200717e-05, 'epoch': 1.89}\n{'loss': 0.002, 'learning_rate': 1.821983273596177e-05, 'epoch': 1.91}\n{'loss': 0.0009, 'learning_rate': 1.7980884109916368e-05, 'epoch': 1.92}\n{'loss': 0.0027, 'learning_rate': 1.774193548387097e-05, 'epoch': 1.94}\n{'loss': 0.0004, 'learning_rate': 1.7502986857825567e-05, 'epoch': 1.95}\n{'loss': 0.0003, 'learning_rate': 1.7264038231780168e-05, 'epoch': 1.96}\n{'loss': 0.1081, 'learning_rate': 1.702508960573477e-05, 'epoch': 1.98}\n{'loss': 0.0005, 'learning_rate': 1.678614097968937e-05, 'epoch': 1.99} {'eval_loss': 0.014878345653414726, 'eval_accuracy': 0.9973094170403587, 'eval_runtime': 4.0209, 'eval_samples_per_second': 277.3, 'eval_steps_per_second': 34.818, 'epoch': 2.0}\n{'loss': 0.0005, 'learning_rate': 1.6547192353643968e-05, 'epoch': 2.01}\n{'loss': 0.0005, 'learning_rate': 1.630824372759857e-05, 'epoch': 2.02}\n{'loss': 0.0004, 'learning_rate': 1.6069295101553167e-05, 'epoch': 2.04}\n{'loss': 0.0005, 'learning_rate': 1.5830346475507768e-05, 'epoch': 2.05}\n{'loss': 0.0004, 'learning_rate': 1.5591397849462366e-05, 'epoch': 2.06}\n{'loss': 0.0135, 'learning_rate': 1.5352449223416964e-05, 'epoch': 2.08}\n{'loss': 0.0014, 'learning_rate': 1.5113500597371565e-05, 'epoch': 2.09}\n{'loss': 0.0003, 'learning_rate': 1.4874551971326165e-05, 'epoch': 2.11}\n{'loss': 0.0003, 'learning_rate': 1.4635603345280766e-05, 'epoch': 2.12}\n{'loss': 0.0002, 'learning_rate': 1.4396654719235364e-05, 'epoch': 2.14}\n{'loss': 0.0002, 'learning_rate': 1.4157706093189965e-05, 'epoch': 2.15}\n{'loss': 0.0003, 'learning_rate': 1.3918757467144564e-05, 'epoch': 2.16}\n{'loss': 0.0008, 'learning_rate': 1.3679808841099166e-05, 'epoch': 2.18}\n{'loss': 0.0002, 'learning_rate': 1.3440860215053763e-05, 'epoch': 2.19}\n{'loss': 0.0002, 'learning_rate': 1.3201911589008365e-05, 'epoch': 2.21}\n{'loss': 0.0003, 'learning_rate': 1.2962962962962962e-05, 'epoch': 2.22}\n{'loss': 0.0002, 'learning_rate': 1.2724014336917564e-05, 'epoch': 2.24}\n{'loss': 0.0002, 'learning_rate': 1.2485065710872163e-05, 'epoch': 2.25}\n{'loss': 0.0002, 'learning_rate': 1.2246117084826763e-05, 'epoch': 2.27}\n{'loss': 0.0006, 'learning_rate': 1.2007168458781362e-05, 'epoch': 2.28}\n{'loss': 0.0875, 'learning_rate': 1.1768219832735962e-05, 'epoch': 2.29}\n{'loss': 0.0002, 'learning_rate': 1.1529271206690561e-05, 'epoch': 2.31}\n{'loss': 0.0003, 'learning_rate': 1.129032258064516e-05, 'epoch': 2.32}\n{'loss': 0.0002, 'learning_rate': 1.1051373954599762e-05, 'epoch': 2.34}\n{'loss': 0.0002, 'learning_rate': 1.0812425328554361e-05, 'epoch': 2.35}\n{'loss': 0.0003, 'learning_rate': 1.0573476702508961e-05, 'epoch': 2.37}\n{'loss': 0.0006, 'learning_rate': 1.033452807646356e-05, 'epoch': 2.38}\n{'loss': 0.0002, 'learning_rate': 1.009557945041816e-05, 'epoch': 2.39}\n{'loss': 0.0002, 'learning_rate': 9.856630824372761e-06, 'epoch': 2.41}\n{'loss': 0.0002, 'learning_rate': 9.61768219832736e-06, 'epoch': 2.42}\n{'loss': 0.0002, 'learning_rate': 9.37873357228196e-06, 'epoch': 2.44}\n{'loss': 0.0002, 'learning_rate': 9.13978494623656e-06, 'epoch': 2.45}\n{'loss': 0.0002, 'learning_rate': 8.90083632019116e-06, 'epoch': 2.47}\n{'loss': 0.0002, 'learning_rate': 8.661887694145759e-06, 'epoch': 2.48}\n{'loss': 0.00",
        "id": "deb64930cdbe6300123379e44ebbe856"
    },
    {
        "text": "_rate': 9.856630824372761e-06, 'epoch': 2.41}\n{'loss': 0.0002, 'learning_rate': 9.61768219832736e-06, 'epoch': 2.42}\n{'loss': 0.0002, 'learning_rate': 9.37873357228196e-06, 'epoch': 2.44}\n{'loss': 0.0002, 'learning_rate': 9.13978494623656e-06, 'epoch': 2.45}\n{'loss': 0.0002, 'learning_rate': 8.90083632019116e-06, 'epoch': 2.47}\n{'loss': 0.0002, 'learning_rate': 8.661887694145759e-06, 'epoch': 2.48}\n{'loss': 0.0002, 'learning_rate': 8.42293906810036e-06, 'epoch': 2.49}\n{'loss': 0.0909, 'learning_rate': 8.18399044205496e-06, 'epoch': 2.51}\n{'loss': 0.0002, 'learning_rate': 7.945041816009559e-06, 'epoch': 2.52}\n{'loss': 0.0788, 'learning_rate': 7.706093189964159e-06, 'epoch': 2.54}\n{'loss': 0.0003, 'learning_rate': 7.467144563918758e-06, 'epoch': 2.55}\n{'loss': 0.0002, 'learning_rate': 7.228195937873358e-06, 'epoch': 2.57}\n{'loss': 0.0011, 'learning_rate': 6.989247311827957e-06, 'epoch': 2.58}\n{'loss': 0.0003, 'learning_rate': 6.7502986857825566e-06, 'epoch': 2.59}\n{'loss': 0.0002, 'learning_rate': 6.511350059737156e-06, 'epoch': 2.61}\n{'loss': 0.0002, 'learning_rate': 6.2724014336917564e-06, 'epoch': 2.62}\n{'loss': 0.0003, 'learning_rate': 6.033452807646357e-06, 'epoch': 2.64}\n{'loss': 0.0003, 'learning_rate': 5.794504181600956e-06, 'epoch': 2.65}\n{'loss': 0.0002, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}\n{'loss': 0.0002, 'learning_rate': 5.316606929510155e-06, 'epoch': 2.68}\n{'loss': 0.0002, 'learning_rate': 5.077658303464755e-06, 'epoch': 2.7}\n{'loss': 0.0002, 'learning_rate': 4.838709677419355e-06, 'epoch': 2.71}\n{'loss': 0.0002, 'learning_rate': 4.599761051373955e-06, 'epoch': 2.72}\n{'loss': 0.0002, 'learning_rate': 4.360812425328554e-06, 'epoch': 2.74}\n{'loss': 0.0002, 'learning_rate': 4.121863799283155e-06, 'epoch': 2.75}\n{'loss': 0.0002, 'learning_rate': 3.882915173237754e-06, 'epoch': 2.77}\n{'loss': 0.0002, 'learning_rate': 3.643966547192354e-06, 'epoch': 2.78}\n{'loss': 0.0002, 'learning_rate': 3.405017921146954e-06, 'epoch': 2.8}\n{'loss': 0.0429, 'learning_rate': 3.1660692951015535e-06, 'epoch': 2.81}\n{'loss': 0.0002, 'learning_rate': 2.927120669056153e-06, 'epoch': 2.82}\n{'loss': 0.0002, 'learning_rate': 2.688172043010753e-06, 'epoch': 2.84}\n{'loss': 0.0002, 'learning_rate': 2.449223416965353e-06, 'epoch': 2.85}\n{'loss': 0.0761, 'learning_rate': 2.2102747909199524e-06, 'epoch': 2.87}\n{'loss': 0.0007, 'learning_rate': 1.971326164874552e-06, 'epoch': 2.88}\n{'loss': 0.0002, 'learning_rate': 1.7323775388291518e-06, 'epoch': 2.9}\n{'loss': 0.0002, 'learning_rate': 1.4934289127837516e-06, 'epoch': 2.91}\n{'loss': 0.0003, 'learning_rate': 1.2544802867383513e-06, 'epoch': 2.92}\n{'loss': 0.0003, 'learning_rate': 1.015531660692951e-06, 'epoch': 2.94}\n{'loss': 0.0144, 'learning_rate': 7.765830346475508e-07, 'epoch': 2.95}\n{'loss': 0.0568, 'learning_rate': 5.376344086021506e-07, 'epoch': 2.97}\n{'loss': 0.0001, 'learning_rate': 2.9868578255675034e-07, 'epoch': 2.98}\n{'loss': 0.0002, 'learning_rate': 5.973715651135006e-08, 'epoch': 3.0} {'eval_loss': 0.026208847761154175, 'eval_accuracy': 0.9937219730941704, 'eval_runtime': 4.0835, 'eval_samples_per_second': 273.052, 'eval_steps_per_second': 34.285, 'epoch': 3.0}\n{'train_runtime': 244.4781, 'train_samples_per_second': 54.717, 'train_steps_per_second': 6.847, 'train_loss': 0.0351541918909871, 'epoch': 3.0} ",
        "id": "153a42b41d92fd5b58e77d2a1a376f6b"
    },
    {
        "text": " Creating a Pipeline with the Fine-Tuned Model In this section, we\u00e2\u0080\u0099re going to create a pipeline that contains our fine-tuned model. After completing the training process, our next step is to create a pipeline for inference using our fine-tuned model. This pipeline will enable us to easily make predictions with the model.  Setting Up the Inference Pipeline Pipeline Creation : We use the pipeline function from the Transformers library to create an inference pipeline. This pipeline is configured for the task of text classification. Model Integration : We integrate our fine-tuned model ( trainer.model ) into the pipeline. This ensures that the pipeline uses our newly trained model for inference. Configuring the Pipeline : We set the batch size and tokenizer in the pipeline configuration. Additionally, we specify the device type, which is crucial for performance considerations.  Device Configuration for Different Platforms Apple Silicon (M1/M2) Devices : For those using Apple Silicon (e.g., M1 or M2 chips), we set the device type to \"mps\" in the pipeline. This leverages Apple\u00e2\u0080\u0099s Metal Performance Shaders for optimized performance on these devices. Other Devices : If you\u00e2\u0080\u0099re using a device other than a MacBook Pro with Apple Silicon, you\u00e2\u0080\u0099ll need to adjust the device setting to match your hardware (e.g., \"cuda\" for NVIDIA GPUs or \"cpu\" for CPU-only inference).  Importance of a Customized Pipeline Creating a customized pipeline with our fine-tuned model allows for easy and efficient inference, tailored to our specific task and hardware. This step is vital in transitioning from model training to practical application. In the following code block, we\u00e2\u0080\u0099ll set up our pipeline with the fine-tuned model and configure it for our device. [10]: # If you're going to run this on something other than a Macbook Pro, change the device to the applicable type. \"mps\" is for Apple Silicon architecture in torch. tuned_pipeline = pipeline ( task = \"text-classification\" , model = trainer . model , batch_size = 8 , tokenizer = tokenizer , device = \"mps\" , )  Validating the Fine-Tuned Model In this next step, we\u00e2\u0080\u0099re going to validate that our fine-tuning training was effective prior to logging the tuned model to our run. Before finalizing our model by logging it to MLflow, it\u00e2\u0080\u0099s crucial to validate its performance. This validation step ensures that the model meets our expectations and is ready for deployment.  Importance of Model Validation Assessing Model Performance : We need to evaluate the model\u00e2\u0080\u0099s performance on realistic scenarios to ensure it behaves as expected. This helps in identifying any issues or shortcomings in the model before it is logged and potentially deployed. Avoiding Costly Redo\u00e2\u0080\u0099s : Given the large size of Transformer models and the computational resources required for training, it\u00e2\u0080\u0099s essential to validate the model beforehand. If a model doesn\u00e2\u0080\u0099t perform well, we wouldn\u00e2\u0080\u0099t want to log the model, only to have to later delete the run and the logged artifacts.  Evaluating with a Test Query Test Query : We will pass a realistic test query to our tuned pipeline to see how the model performs. This query should be representative of the kind of input the model is expected to handle in a real-world scenario. Observing the Output : By analyzing the output of the model for this query, we can gauge its understanding and response to complex situations. This provides a practical insight into the model\u00e2\u0080\u0099s capabilities post-fine-tuning. ",
        "id": "4c7f2f3e677a954a3e3faa9a4499f224"
    },
    {
        "text": " Validating Before Logging to MLflow Rationale : The reason for this validation step is to ensure that the model we log to MLflow is of high quality and ready for further steps like deployment or sharing. Logging a poorly performing model would lead to unnecessary complications, especially considering the large size and complexity of these models. After validating the model and ensuring satisfactory performance, we can confidently proceed to log it in MLflow, knowing it\u00e2\u0080\u0099s ready for real-world applications. In the next code block, we will run a test query through our fine-tuned model to evaluate its performance before proceeding to log it in MLflow. [11]: # Perform a validation of our assembled pipeline that contains our fine-tuned model. quick_check = ( \"I have a question regarding the project development timeline and allocated resources; \" \"specifically, how certain are you that John and Ringo can work together on writing this next song? \" \"Do we need to get Paul involved here, or do you truly believe, as you said, 'nah, they got this'?\" ) tuned_pipeline ( quick_check ) [11]: [{'label': 'ham', 'score': 0.9985793828964233}]  Model Configuration and Signature Inference In this next step, we generate a signature for our pipeline in preparation for logging. After validating our model\u00e2\u0080\u0099s performance, the next critical step is to prepare it for logging to MLflow. This involves setting up the model\u00e2\u0080\u0099s configuration and inferring its signature, which are essential aspects of the model management process.  Configuring the Model for MLflow Setting Model Configuration : We define a model_config dictionary to specify configuration parameters such as batch size and the device type (e.g., \"mps\" for Apple Silicon). This configuration is vital for ensuring that the model operates correctly in different environments.  Inferring the Model Signature Purpose of Signature Inference : The model signature defines the input and output schema of the model. Inferring this signature is crucial as it helps MLflow understand the data types and shapes that the model expects and produces. Using mlflow.models.infer_signature : We use this function to automatically infer the model signature. We provide sample input and output data to the function, which analyzes them to determine the appropriate schema. Including Model Parameters : Along with the input and output, we also include the model_config in the signature. This ensures that all relevant information about how the model should be run is captured.  Importance of Signature Inference Inferring the signature is a key step in preparing the model for logging and future deployment. It ensures that anyone who uses the model later, either for further development or in production, has clear information about the expected data format, making the model more robust and user-friendly. With the model configuration set and its signature inferred, we are now ready to log the model into MLflow. This will be our next step, ensuring our model is properly managed and ready for deployment. [12]: # Define a set of parameters that we would like to be able to flexibly override at inference time, along with their default values model_config = { \"batch_size\" : 8 } # Infer the model signature, including a representative input, the expected output, and the parameters that we would like to be able to override at inference time. signature = mlflow . models . infer_signature ( [ \"This is a test!\" , \"And this is also a test.\" ], mlflow . transformers . generate_signature_output ( tuned_pipeline , [ \"This is a test response!\" , \"So is this.\" ] ), params = model_config , )  Logging the Fine-Tuned Model to MLflow In this next section, we\u00e2\u0080\u0099re going to log our validated pipeline to the training run. With our model configuration and signature ready, the final step in our model training and validation process is to log the model to MLflow. This step is crucial for tracking and managing the model in a systematic way. ",
        "id": "e2cfb148a9c1a457371ece7d08d8ee4c"
    },
    {
        "text": " Accessing the existing Run used for training Initiating MLflow Run : We start a new run in MLflow using mlflow.start_run() . This new run is specifically for the purpose of logging the model, separate from the training run.  Logging the Model in MLflow Using mlflow.transformers.log_model : We log our fine-tuned model using this function. It\u00e2\u0080\u0099s specially designed for logging models from the Transformers library, making the process streamlined and efficient. Specifying Model Information : We provide several pieces of information to the logging function: transformers_model : The fine-tuned model pipeline. artifact_path : The path where the model artifacts will be stored. signature : The inferred signature of the model, which includes input and output schemas. input_example : Sample inputs to give users an idea of what input the model expects. model_config : The configuration parameters of the model.  Importance of Model Logging Logging the model in MLflow serves multiple purposes: Version Control : It helps in keeping track of different versions of the model. Model Management : Facilitates the management of the model lifecycle, from training to deployment. Reproducibility and Sharing : Enhances reproducibility and makes it easier to share the model with others. By logging our model in MLflow, we ensure that it is well-documented, versioned, and ready for future use, whether for further development or deployment. [13]: # Log the pipeline to the existing training run with mlflow . start_run ( run_id = run . info . run_id ): model_info = mlflow . transformers . log_model ( transformers_model = tuned_pipeline , artifact_path = \"fine_tuned\" , signature = signature , input_example = [ \"Pass in a string\" , \"And have it mark as spam or not.\" ], model_config = model_config , ) 2023/11/30 12:17:11 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /var/folders/cd/n8n0rm2x53l_s0xv_j_xklb00000gp/T/tmp77_imuy9/model, flavor: transformers), fall back to return ['transformers==4.34.1', 'torch==2.1.0', 'torchvision==0.16.0', 'accelerate==0.21.0']. Set logging level to DEBUG to see the full traceback.  Loading and Testing the Model from MLflow After logging our fine-tuned model to MLflow, we\u00e2\u0080\u0099ll now load and test it.  Loading the Model from MLflow Using mlflow.transformers.load_model : We use this function to load the model stored in MLflow. This demonstrates how models can be retrieved and utilized post-training, ensuring they are accessible for future use. Retrieving Model URI : We use the model_uri obtained from logging the model to MLflow. This URI is the unique identifier for our logged model, allowing us to retrieve it accurately. ",
        "id": "56da4512949c023a40deb926826f9c72"
    },
    {
        "text": " Testing the Model with Validation Text Preparing Validation Text : We use a creatively crafted text to test the model\u00e2\u0080\u0099s performance. This text is designed to mimic a typical spam message, which is relevant to our model\u00e2\u0080\u0099s training on spam classification. Evaluating Model Output : By passing this text through the loaded model, we can observe its performance and effectiveness in a practical scenario. This step is crucial to ensure that the model works as expected in real-world conditions. Testing the model after loading it from MLflow is essential for several reasons: Validation of Logging Process : It confirms that the model was logged and loaded correctly. Practical Performance Assessment : Provides a real-world assessment of the model\u00e2\u0080\u0099s performance, which is critical for deployment decisions. Demonstrating End-to-End Workflow : Showcases a complete workflow from training, logging, loading, to using the model, which is vital for understanding the entire model lifecycle. In the next code block, we\u00e2\u0080\u0099ll load our model from MLflow and test it with a validation text to assess its real-world performance. [14]: # Load our saved model in the native transformers format loaded = mlflow . transformers . load_model ( model_uri = model_info . model_uri ) # Define a test example that we expect to be classified as spam validation_text = ( \"Want to learn how to make MILLIONS with no effort? Click HERE now! See for yourself! Guaranteed to make you instantly rich! \" \"Don't miss out you could be a winner!\" ) # validate the performance of our fine-tuning loaded ( validation_text ) 2023/11/30 12:17:11 INFO mlflow.transformers: 'runs:/e3260e8511c94c38aafb7124509240a4/fine_tuned' resolved as 'file:///Users/benjamin.wilson/repos/mlflow-fork/mlflow/docs/source/llms/transformers/tutorials/fine-tuning/mlruns/258758267044147956/e3260e8511c94c38aafb7124509240a4/artifacts/fine_tuned'\n2023/11/30 12:17:11 WARNING mlflow.transformers: Could not specify device parameter for this pipeline type [14]: [{'label': 'spam', 'score': 0.9873914122581482}]  Conclusion: Mastering Fine-Tuning and MLflow Integration Congratulations on completing this comprehensive tutorial on fine-tuning a Transformers model and integrating it with MLflow! Let\u00e2\u0080\u0099s recap the essential skills and knowledge you\u00e2\u0080\u0099ve acquired through this journey.  Key Takeaways Fine-Tuning Transformers Models : You\u00e2\u0080\u0099ve learned how to fine-tune a foundational model from the Transformers library. This process demonstrates the power of adapting advanced pre-trained models to specific tasks, tailoring their performance to meet unique requirements. Ease of Fine-Tuning : We\u00e2\u0080\u0099ve seen firsthand how straightforward it is to fine-tune these advanced Large Language Models (LLMs). With the right tools and understanding, fine-tuning can significantly enhance a model\u00e2\u0080\u0099s performance on specific tasks. Specificity in Performance : The ability to fine-tune LLMs opens up a world of possibilities, allowing us to create models that excel in particular domains or tasks. This specificity in performance is crucial in deploying models in real-world scenarios where specialized understanding is required.  Integrating MLflow with Transformers Tracking and Managing the Fine-Tuning Process : A significant part of this tutorial was dedicated to using MLflow for experiment tracking, model logging, and management. You\u00e2\u0080\u0099ve learned how MLflow simplifies these aspects, making the machine learning workflow more manageable and efficient. Benefits of MLflow in Fine-Tuning : MLflow plays a crucial role in ensuring reproducibility, managing model versions, and streamlining the deployment process. Its integration with the Transformers fine-tuning process demonstrates the potential for synergy between advanced model training techniques and lifecycle management tools. Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "3d8bdfa85f55e847ff7e6d0fa97750aa"
    },
    {
        "text": "Fine-Tuning Open-Source LLM using QLoRA with MLflow and PEFT 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor Introduction Getting Started with the MLflow Transformers Flavor - Tutorials and Guides Important Details to be aware of with the transformers flavor Logging Large Models Working with tasks for Transformer Pipelines Detailed Documentation Learn more about Transformers MLflow OpenAI Flavor MLflow Sentence-Transformers Flavor MLflow LangChain Flavor MLflow LlamaIndex Flavor MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow Transformers Flavor Fine-Tuning Open-Source LLM using QLoRA with MLflow and PEFT  Fine-Tuning Open-Source LLM using QLoRA with MLflow and PEFT Download this Notebook  Overview Many powerful open-source LLMs have emerged and are easily accessible. However, they are not designed to be deployed to your production environment out-of-the-box; instead, you have to fine-tune them for your specific tasks, such as a chatbot, content generation, etc. One challenge, though, is that training LLMs is usually very expensive. Even if your dataset for fine-tuning is small, the backpropagation step needs to compute gradients for billions of parameters. For example, fully\nfine-tuning the Llama7B model requires 112GB of VRAM, i.e.\u00c2\u00a0at least two 80GB A100 GPUs. Fortunately, there are many research efforts on how to reduce the cost of LLM fine-tuning. In this tutorial, we will demonstrate how to build a powerful text-to-SQL generator by fine-tuning the Mistral 7B model with a single 24GB VRAM GPU .  What You Will Learn Hands-on learning of the typical LLM fine-tuning process. Understand how to use QLoRA and PEFT to overcome the GPU memory limitation for fine-tuning. Manage the model training cycle using MLflow to log the model artifacts, hyperparameters, metrics, and prompts. How to save prompt template and inference parameters (e.g.\u00c2\u00a0max_token_length) in MLflow to simplify prediction interface. ",
        "id": "07b4c33b4d3735a7daf9578660ad955d"
    },
    {
        "text": " Key Actors In this tutorial, you will learn about the techniques and methods behind efficient LLM fine-tuning by actually running the code. There are more detailed explanations for each cell below, but let\u00e2\u0080\u0099s start with a brief preview of a few main important libraries/methods used in this tutorial. Mistral-7B-v0.1 model is a pretrained text-generation model with 7 billion parameters, developed by mistral.ai . The model employs various optimization techniques such as Group-Query Attention, Sliding-Window Attention, Byte-fallback BPE tokenizer, and outperforms the Llama 2 13B on benchmarks with fewer parameters. QLoRA is a novel method that allows us to fine-tune large foundational models with limited GPU resources. It reduces the number of trainable parameters by learning pairs of rank-decomposition matrices and also applies 4-bit quantization to the frozen pretrained model to further reduce the memory footprint. PEFT is a library developed by HuggingFace\u00f0\u009f\u00a4\u0097, that enables developers to easily integrate various optimization methods with pretrained models available on the HuggingFace Hub. With PEFT, you can apply QLoRA to the pretrained model with a few lines of configurations and run fine-tuning just like the normal Transformers model training. MLflow manages an exploding number of configurations, assets, and metrics during the LLM training on your behalf. MLflow is natively integrated with Transformers and PEFT, and plays a crucial role in organizing the fine-tuning cycle.  1. Environment Set up  Hardware Requirement Please ensure your GPU has at least 20GB of VRAM available. This notebook has been tested on a single NVIDIA A10G GPU with 24GB of VRAM. [ ]: % sh nvidia - smi Wed Feb 21 07:16:13 2024\n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA A10G                    Off | 00000000:00:1E.0 Off |                    0 |\n|  0%   15C    P8              16W / 300W |      4MiB / 23028MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n\n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+ ",
        "id": "934fef7149b194882b9300f45b35a176"
    },
    {
        "text": " Install Python Libraries This tutorial utilizes the following Python libraries: mlflow - for tracking parameters, metrics, and saving trained models. Version 2.11.0 or later is required to log PEFT models with MLflow. transformers - for defining the model, tokenizer, and trainer. peft - for creating a LoRA adapter on top of the Transformer model. bitsandbytes - for loading the base model with 4-bit quantization for QLoRA. accelerate - a dependency required by bitsandbytes. datasets - for loading the training dataset from the HuggingFace hub. Note : Restarting the Python kernel may be necessary after installing these dependencies. The notebook has been tested with mlflow==2.11.0 , transformers==4.35.2 , peft==0.8.2 , bitsandbytes==0.42.0 , accelerate==0.27.2 , and datasets==2.17.1 . [ ]: % pip install mlflow >= 2.11.0 % pip install transformers peft accelerate bitsandbytes datasets - q - U  2. Dataset Preparation  Load Dataset from HuggingFace Hub We will use the b-mc2/sql-create-context dataset from the Hugging Face Hub for this tutorial. This dataset comprises 78.6k pairs of natural language queries and their corresponding SQL statements, making it ideal for training a text-to-SQL model. The dataset includes three columns: question : A natural language question posed regarding the data. context : Additional information about the data, such as the schema for the table being queried. answer : The SQL query that represents the expected output. [ ]: import pandas as pd from datasets import load_dataset from IPython.display import HTML , display dataset_name = \"b-mc2/sql-create-context\" dataset = load_dataset ( dataset_name , split = \"train\" ) def display_table ( dataset_or_sample ): # A helper fuction to display a Transformer dataset or single sample contains multi-line string nicely pd . set_option ( \"display.max_colwidth\" , None ) pd . set_option ( \"display.width\" , None ) pd . set_option ( \"display.max_rows\" , None ) if isinstance ( dataset_or_sample , dict ): df = pd . DataFrame ( dataset_or_sample , index = [ 0 ]) else : df = pd . DataFrame ( dataset_or_sample ) html = df . to_html () . replace ( \" \\\\ n\" , \"<br>\" ) styled_html = f \"\"\"<style> .dataframe th, .dataframe tbody td {{ text-align: left; padding-right: 30px; }} </style> { html } \"\"\" display ( HTML ( styled_html )) display_table ( dataset . select ( range ( 3 ))) question context answer 0 How many heads of the departments are older than 56 ? CREATE TABLE head (age INTEGER) SELECT COUNT(*) FROM head WHERE age > 56 1 List the name, born state and age of the heads of departments ordered by age. CREATE TABLE head (name VARCHAR, born_state VARCHAR, age VARCHAR) SELECT name, born_state, age FROM head ORDER BY age 2 List the creation year, name and budget of each department. CREATE TABLE department (creation VARCHAR, name VARCHAR, budget_in_billions VARCHAR) SELECT creation, name, budget_in_billions FROM department  Split Train and Test Dataset The b-mc2/sql-create-context dataset consists of a single split, \u00e2\u0080\u009ctrain\u00e2\u0080\u009d. We will separate 20% of this as test samples. [ ]: split_dataset = dataset . train_test_split ( test_size = 0.2 , seed = 42 ) train_dataset = split_dataset [ \"train\" ] test_dataset = split_dataset [ \"test\" ] print ( f \"Training dataset contains { len ( train_dataset ) } text-to-SQL pairs\" ) print ( f \"Test dataset contains { len ( test_dataset ) } text-to-SQL pairs\" ) Training dataset contains 62861 text-to-SQL pairs\nTest dataset contains 15716 text-to-SQL pairs ",
        "id": "6d7387274952ca86d2fe5ac1727903cf"
    },
    {
        "text": " Define Prompt Template The Mistral 7B model is a text comprehension model, so we have to construct a text prompt that incorporates the user\u00e2\u0080\u0099s question, context, and our system instructions. The new prompt column in the dataset will contain the text prompt to be fed into the model during training. It is important to note that we also include the expected response within the prompt, allowing the model to be trained in a self-supervised manner. [ ]: PROMPT_TEMPLATE = \"\"\"You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question. ### Table: {context} ### Question: {question} ### Response: {output} \"\"\" def apply_prompt_template ( row ): prompt = PROMPT_TEMPLATE . format ( question = row [ \"question\" ], context = row [ \"context\" ], output = row [ \"answer\" ], ) return { \"prompt\" : prompt } train_dataset = train_dataset . map ( apply_prompt_template ) display_table ( train_dataset . select ( range ( 1 ))) question context answer prompt 0 Which Perth has Gold Coast yes, Sydney yes, Melbourne yes, and Adelaide yes? CREATE TABLE table_name_56 (perth VARCHAR, adelaide VARCHAR, melbourne VARCHAR, gold_coast VARCHAR, sydney VARCHAR) SELECT perth FROM table_name_56 WHERE gold_coast = \"yes\" AND sydney = \"yes\" AND melbourne = \"yes\" AND adelaide = \"yes\" You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question. ### Table: CREATE TABLE table_name_56 (perth VARCHAR, adelaide VARCHAR, melbourne VARCHAR, gold_coast VARCHAR, sydney VARCHAR) ### Question: Which Perth has Gold Coast yes, Sydney yes, Melbourne yes, and Adelaide yes? ### Response: SELECT perth FROM table_name_56 WHERE gold_coast = \"yes\" AND sydney = \"yes\" AND melbourne = \"yes\" AND adelaide = \"yes\"  Padding the Training Dataset As a final step of dataset preparation, we need to apply padding to the training dataset. Padding ensures that all input sequences in a batch are of the same length. A crucial point to note is the need to add padding to the left . This approach is adopted because the model generates tokens autoregressively, meaning it continues from the last token. Adding padding to the right would cause the model to generate new tokens from these padding tokens, resulting in the output sequence including padding tokens in the middle. Padding to right Today |  is  |   a    |  cold  |  <pad>  ==generate=>  \"Today is a cold <pad> day\"\n How  |  to  | become |  <pad> |  <pad>  ==generate=>  \"How to become a <pad> <pad> great engineer\". Padding to left: <pad> |  Today  |  is  |  a   |  cold     ==generate=>  \"<pad> Today is a cold day\"\n<pad> |  <pad>  |  How |  to  |  become   ==generate=>  \"<pad> <pad> How to become a great engineer\". [ ]: from transformers import AutoTokenizer base_model_id = \"mistralai/Mistral-7B-v0.1\" # You can use a different max length if your custom dataset has shorter/longer input sequences. MAX_LENGTH = 256 tokenizer = AutoTokenizer . from_pretrained ( base_model_id , model_max_length = MAX_LENGTH , padding_side = \"left\" , add_eos_token = True , ) tokenizer . pad_token = tokenizer . eos_token def tokenize_and_pad_to_fixed_length ( sample ): result = tokenizer ( sample [ \"prompt\" ], truncation = True , max_length = MAX_LENGTH , padding = \"max_length\" , ) result [ \"labels\" ] = result [ \"input_ids\" ] . copy () return result tokenized_train_dataset = train_dataset . map ( tokenize_and_pad_to_fixed_length ) assert all ( len ( x [ \"input_ids\" ]) == MAX_LENGTH for x in tokenized_train_dataset ) display_table ( tokenized_train_dataset . select ( range ( 1 ))) ",
        "id": "bca361f4e45bed72405a9fbd32c83ed3"
    },
    {
        "text": " 3. Load the Base Model (with 4-bit quantization) Next, we\u00e2\u0080\u0099ll load the Mistral 7B model, which will serve as our base model for fine-tuning. This model can be loaded from the HuggingFace Hub repository mistralai/Mistral-7B-v0.1 using the Transformers\u00e2\u0080\u0099 from_pretrained() API. However, here we are also providing a quantization_config parameter. This parameter embodies the key technique of QLoRA that significantly reduces memory usage during fine-tuning. The following paragraph details the method and the implications of this configuration. However, feel free to skip if it appears complex. After all, we rarely need to modify the quantization_config values ourselves :) How It Works In short, QLoRA is a combination of Q uantization and LoRA . To grasp its functionality, it\u00e2\u0080\u0099s simpler to begin with LoRA. LoRA (Low Rank Adaptation) is a preceding method for resource-efficient fine-tuning, by reducing the number of trainable parameters through matrix decomposition. Let W' represent the final weight matrix from fine-tuning. In LoRA, W' is approximated by the sum of the original weight and its update, i.e., W + \u00ce\u0094W , then\ndecomposing the delta part into two low-dimensional matrices, i.e., \u00ce\u0094W \u00e2\u0089\u0088 AB . Suppose W is m x m , and we select a smaller r for the rank of A and B , where A is m x r and B is r x m . Now, the original trainable parameters, which are quadratic in size of W (i.e., m^2 ), after decomposition, become 2mr . Empirically, we can choose a much smaller number for r , e.g., 32, 64, compared to the full weight matrix size, therefore this\nsignificantly reduces the number of parameters to train. QLoRA extends LoRA, employing the same strategy for matrix decomposition. However, it further reduces memory usage by applying 4-bit quantization to the frozen pretrained model W . According to their research, the largest memory usage during LoRA fine-tuning is the backpropagation through the frozen parameters W to compute gradients for the adaptors A and B . Thus, quantizing W to 4-bit significantly reduces the overall memory\nconsumption. This is achieved with the load_in_4bit=True setting shown below. Moreover, QLoRA introduces additional techniques to optimize resource usage without significantly impacting model performance. For more technical details, please refer to the paper , but we implement them by setting the following quantization configurations in bitsandbytes: * The 4-bit NormalFloat type is specified by bnb_4bit_quant_type=\"nf4\" . * Double quantization is activated by bnb_4bit_use_double_quant=True . * QLoRA re-quantizes the 4-bit\nweights back to a higher precision when computing the gradients for A and B , to prevent performance degradation. This datatype is specified by bnb_4bit_compute_dtype=torch.bfloat16 . [ ]: import torch from transformers import AutoModelForCausalLM , BitsAndBytesConfig quantization_config = BitsAndBytesConfig ( # Load the model with 4-bit quantization load_in_4bit = True , # Use double quantization bnb_4bit_use_double_quant = True , # Use 4-bit Normal Float for storing the base model weights in GPU memory bnb_4bit_quant_type = \"nf4\" , # De-quantize the weights to 16-bit (Brain) float before the forward/backward pass bnb_4bit_compute_dtype = torch . bfloat16 , ) model = AutoModelForCausalLM . from_pretrained ( base_model_id , quantization_config = quantization_config ) ",
        "id": "00a127650b02d8fdf20060e033b3acae"
    },
    {
        "text": " How Does the Base Model Perform? First, let\u00e2\u0080\u0099s assess the performance of the vanilla Mistral model on the SQL generation task before any fine-tuning. As expected, the model does not produce correct SQL queries; instead, it generates random answers in natural language. This outcome indicates the necessity of fine-tuning the model for our specific task. [ ]: import transformers tokenizer = AutoTokenizer . from_pretrained ( base_model_id ) pipeline = transformers . pipeline ( model = model , tokenizer = tokenizer , task = \"text-generation\" ) sample = test_dataset [ 1 ] prompt = PROMPT_TEMPLATE . format ( context = sample [ \"context\" ], question = sample [ \"question\" ], output = \"\" ) # Leave the answer part blank with torch . no_grad (): response = pipeline ( prompt , max_new_tokens = 256 , repetition_penalty = 1.15 , return_full_text = False ) display_table ({ \"prompt\" : prompt , \"generated_query\" : response [ 0 ][ \"generated_text\" ]}) prompt generated_query 0 You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question. ### Table: CREATE TABLE table_name_61 (game INTEGER, opponent VARCHAR, record VARCHAR) ### Question: What is the lowest numbered game against Phoenix with a record of 29-17? ### Response: A: The lowest numbered game against Phoenix was played on 03/04/2018. The score was PHO 115 - DAL 106. What is the highest numbered game against Phoenix? A: The highest numbered game against Phoenix was played on 03/04/2018. The score was PHO 115 - DAL 106. Which players have started at Point Guard for Dallas in a regular season game against Phoenix? ",
        "id": "ac83affcccd5313127d2b81ff5d09691"
    },
    {
        "text": " 4. Define a PEFT Model As discussed earlier, QLoRA stands for Quantization + LoRA . Having applied the quantization part, we now proceed with the LoRA aspect. Although the mathematics behind LoRA is intricate, PEFT helps us by simplifying the process of adapting LoRA to the pretrained Transformer model. In the next cell, we create a LoraConfig with various settings for LoRA. Contrary to the earlier quantization_config , these hyperparameters might need optimization to achieve the best model performance for your specific task. MLflow facilitates this process by tracking these hyperparameters, the associated model, and its outcomes. At the end of the cell, we display the number of trainable parameters during fine-tuning, and their percentage relative to the total model parameters. Here, we are training only 1.16% of the total 7 billion parameters. [ ]: from peft import LoraConfig , get_peft_model , prepare_model_for_kbit_training # Enabling gradient checkpointing, to make the training further efficient model . gradient_checkpointing_enable () # Set up the model for quantization-aware training e.g. casting layers, parameter freezing, etc. model = prepare_model_for_kbit_training ( model ) peft_config = LoraConfig ( task_type = \"CAUSAL_LM\" , # This is the rank of the decomposed matrices A and B to be learned during fine-tuning. A smaller number will save more GPU memory but might result in worse performance. r = 32 , # This is the coefficient for the learned \u00ce\u0094W factor, so the larger number will typically result in a larger behavior change after fine-tuning. lora_alpha = 64 , # Drop out ratio for the layers in LoRA adaptors A and B. lora_dropout = 0.1 , # We fine-tune all linear layers in the model. It might sound a bit large, but the trainable adapter size is still only **1.16%** of the whole model. target_modules = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" , \"gate_proj\" , \"up_proj\" , \"down_proj\" , \"lm_head\" , ], # Bias parameters to train. 'none' is recommended to keep the original model performing equally when turning off the adapter. bias = \"none\" , ) peft_model = get_peft_model ( model , peft_config ) peft_model . print_trainable_parameters () trainable params: 85,041,152 || all params: 7,326,773,248 || trainable%: 1.1606903765339511 That\u00e2\u0080\u0099s it!!! PEFT has made the LoRA setup super easy. An additional bonus is that the PEFT model exposes the same interfaces as a Transformers model. This means that everything from here on is quite similar to the standard model training process using Transformers. ",
        "id": "d78821da74dac489bb67a750842c5639"
    },
    {
        "text": " 5. Kick-off a Training Job Similar to conventional Transformers training, we\u00e2\u0080\u0099ll first set up a Trainer object to organize the training iterations. There are numerous hyperparameters to configure, but MLflow will manage them on your behalf. To enable MLflow logging, you can specify report_to=\"mlflow\" and name your training trial with the run_name parameter. This action initiates an MLflow run that automatically logs training metrics, hyperparameters, configurations, and the trained model. [ ]: from datetime import datetime import transformers from transformers import TrainingArguments import mlflow # Comment-out this line if you are running the tutorial on Databricks mlflow . set_experiment ( \"MLflow PEFT Tutorial\" ) training_args = TrainingArguments ( # Set this to mlflow for logging your training report_to = \"mlflow\" , # Name the MLflow run run_name = f \"Mistral-7B-SQL-QLoRA- { datetime . now () . strftime ( '%Y-%m- %d -%H-%M- %s ' ) } \" , # Replace with your output destination output_dir = \"YOUR_OUTPUT_DIR\" , # For the following arguments, refer to https://huggingface.co/docs/transformers/main_classes/trainer per_device_train_batch_size = 2 , gradient_accumulation_steps = 4 , gradient_checkpointing = True , optim = \"paged_adamw_8bit\" , bf16 = True , learning_rate = 2e-5 , lr_scheduler_type = \"constant\" , max_steps = 500 , save_steps = 100 , logging_steps = 100 , warmup_steps = 5 , # https://discuss.huggingface.co/t/training-llama-with-lora-on-multiple-gpus-may-exist-bug/47005/3 ddp_find_unused_parameters = False , ) trainer = transformers . Trainer ( model = peft_model , train_dataset = tokenized_train_dataset , data_collator = transformers . DataCollatorForLanguageModeling ( tokenizer , mlm = False ), args = training_args , ) # use_cache=True is incompatible with gradient checkpointing. peft_model . config . use_cache = False The training duration may span several hours, contingent upon your hardware specifications. Nonetheless, the primary objective of this tutorial is to acquaint you with the process of fine-tuning using PEFT and MLflow, rather than to cultivate a highly performant SQL generator. If you don\u00e2\u0080\u0099t care much about the model performance, you may specify a smaller number of steps or interrupt the following cell to proceed with the rest of the notebook. [ ]: trainer . train () [500/500 45:41, Epoch 0/1] Step Training Loss 100 0.681700 200 0.522400 300 0.507300 400 0.494800 500 0.474600 TrainOutput(global_step=500, training_loss=0.5361956100463867, metrics={'train_runtime': 2747.9223, 'train_samples_per_second': 1.456, 'train_steps_per_second': 0.182, 'total_flos': 4.421038813216768e+16, 'train_loss': 0.5361956100463867, 'epoch': 0.06})  6. Save the PEFT Model to MLflow Hooray! We have successfully fine-tuned the Mistral 7B model into an SQL generator. Before concluding the training, one final step is to save the trained PEFT model to MLflow.  Set Prompt Template and Default Inference Parameters (optional) LLMs prediction behavior is not only defined by the model weights, but also largely controlled by the prompt and inference paramters such as max_token_length , repetition_penalty . Therefore, it is highly advisable to save those metadata along with the model, so that you can expect the consistent behavior when loading the model later. ",
        "id": "8e40fca193c08baac9e7dc0e84f8fd14"
    },
    {
        "text": " Prompt Template The user prompt itself is free text, but you can harness the input by applying a \u00e2\u0080\u0098template\u00e2\u0080\u0099. MLflow Transformer flavor supports saving a prompt template with the model, and apply it automatically before the prediction. This also allows you to hide the system prompt from model clients. To save the prompt template, we have to define a single string that contains {prompt} variable, and pass it to the prompt_template argument of mlflow.transformers.log_model API. Refer to Saving Prompt Templates with Transformer Pipelines for more detailed usage of this feature. [ ]: # Basically the same format as we applied to the dataset. However, the template only accepts {prompt} variable so both table and question need to be fed in there. prompt_template = \"\"\"You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question. {prompt} ### Response: \"\"\"  Inference Parameters Inference parameters can be saved with MLflow model as a part of Model Signature . The signature defines model input and output format with additional parameters passed to the model prediction, and you can let MLflow to infer it from some sample input using mlflow.models.infer_signature API. If you pass the concrete value for parameters,\nMLflow treats them as default values and apply them at the inference if they are not provided by users. For more details about the Model Signature, please refer to the MLflow documentation . [ ]: from mlflow.models import infer_signature sample = train_dataset [ 1 ] # MLflow infers schema from the provided sample input/output/params signature = infer_signature ( model_input = sample [ \"prompt\" ], model_output = sample [ \"answer\" ], # Parameters are saved with default values if specified params = { \"max_new_tokens\" : 256 , \"repetition_penalty\" : 1.15 , \"return_full_text\" : False }, ) signature inputs:\n  [string (required)]\noutputs:\n  [string (required)]\nparams:\n  ['max_new_tokens': long (default: 256), 'repetition_penalty': double (default: 1.15), 'return_full_text': boolean (default: False)] ",
        "id": "9faa9b77d485b9375ac800cb02aa452f"
    },
    {
        "text": " Save the PEFT Model to MLflow Finally, we will call mlflow.transformers.log_model API to log the model to MLflow. A few critical points to remember when logging a PEFT model to MLflow are: MLflow logs the Transformer model as a Pipeline . A pipeline bundles a model with its tokenizer (or other components, depending on the task type) and simplifies the prediction steps into an easy-to-use interface, making it an excellent tool for ensuring reproducibility. In the code below, we pass the model and tokenizer as a dictionary, then MLflow automatically deduces the correct pipeline type and saves it. MLflow does not save the base model weight for the PEFT model . When executing mlflow.transformers.log_model , MLflow only saves the small number of trained parameters, i.e., the PEFT adapter. For the base model, MLflow instead records a reference to the HuggingFace hub (repository name and commit hash), and downloads the base model weights on the fly when loading the PEFT model. This approach significantly reduces storage usage and logging latency; for instance, the logged artifacts\nsize in this tutorial is less than 1GB, while the full Mistral 7B model is about 20GB. Save a tokenizer without padding . During fine-tuning, we applied padding to the dataset to standardize the sequence length in a batch. However, padding is no longer necessary at inference, so we save a different tokenizer without padding. This ensures the loaded model can be used for inference immediately. Note : Currently, manual logging is required for the PEFT adapter and config, while other information, such as dataset, metrics, Trainer parameters, etc., are automatically logged. However, this process may be automated in future versions of MLflow and Transformers. [ ]: import mlflow # Get the ID of the MLflow Run that was automatically created above last_run_id = mlflow . last_active_run () . info . run_id # Save a tokenizer without padding because it is only needed for training tokenizer_no_pad = AutoTokenizer . from_pretrained ( base_model_id , add_bos_token = True ) # If you interrupt the training, uncomment the following line to stop the MLflow run # mlflow.end_run() with mlflow . start_run ( run_id = last_run_id ): mlflow . log_params ( peft_config . to_dict ()) mlflow . transformers . log_model ( transformers_model = { \"model\" : trainer . model , \"tokenizer\" : tokenizer_no_pad }, prompt_template = prompt_template , signature = signature , artifact_path = \"model\" , # This is a relative path to save model files within MLflow run )  What\u00e2\u0080\u0099s Logged to MLflow? Let\u00e2\u0080\u0099s briefly review what is logged/saved to MLflow as a result of your training. To access the MLflow UI, run mlflow ui commands and open https://localhost:PORT (PORT is 5000 by default). Select the experiment \u00e2\u0080\u009cMLflow PEFT Tutorial\u00e2\u0080\u009d (or the notebook name when running on Databricks) on the left side. Then click on the latest MLflow Run named Mistral-7B-SQL-QLoRA-2024-... to view the Run details.  Parameters The Parameters section displays hundreds of parameters specified for the Trainer, LoraConfig, and BitsAndBytesConfig, such as learning_rate , r , bnb_4bit_quant_type . It also includes default parameters that were not explicitly specified, which is crucial for ensuring reproducibility, especially if the library\u00e2\u0080\u0099s default values change.  Metrics The Metrics section presents the model metrics collected during the run, such as train_loss . You can visualize these metrics with various types of graphs in the \u00e2\u0080\u009cChart\u00e2\u0080\u009d tab. ",
        "id": "36aa979a9dd2930edf1236e99b8fd9f3"
    },
    {
        "text": " Artifacts The Artifacts section displays the files/directories saved in MLflow as a result of training. For Transformers PEFT training, you should see the following files/directories: model/\n  \u00e2\u0094\u009c\u00e2\u0094\u0080 peft/\n  \u00e2\u0094\u0082  \u00e2\u0094\u009c\u00e2\u0094\u0080 adapter_config.json       # JSON file of the LoraConfig\n  \u00e2\u0094\u0082  \u00e2\u0094\u009c\u00e2\u0094\u0080 adapter_module.safetensor # The weight file of the LoRA adapter\n  \u00e2\u0094\u0082  \u00e2\u0094\u0094\u00e2\u0094\u0080 README.md                 # Empty README file generated by Transformers\n  \u00e2\u0094\u0082\n  \u00e2\u0094\u009c\u00e2\u0094\u0080 LICENSE.txt                  # License information about the base model (Mistral-7B-0.1)\n  \u00e2\u0094\u009c\u00e2\u0094\u0080 MLModel                      # Contains various metadata about your model\n  \u00e2\u0094\u009c\u00e2\u0094\u0080 conda.yaml                   # Dependencies to create conda environment\n  \u00e2\u0094\u009c\u00e2\u0094\u0080 model_card.md                # Model card text for the base model\n  \u00e2\u0094\u009c\u00e2\u0094\u0080 model_card_data.yaml         # Model card data for the base model\n  \u00e2\u0094\u009c\u00e2\u0094\u0080 python_env.yaml              # Dependencies to create Python virtual environment\n  \u00e2\u0094\u0094\u00e2\u0094\u0080 requirements.txt             # Pip requirements for model inference  Model Metadata In the MLModel file, you can see the many detailed metadata are saved about the PEFT and base model. Here is an excerpt of the MLModel file (some fields are omitted for simplicity) flavors:\n  transformers:\n    peft_adaptor: peft                                 # Points the location of the saved PEFT model\n    pipeline_model_type: MistralForCausalLM            # The base model implementation\n    source_model_name: mistralai/Mistral-7B-v0.1.      # Repository name of the base model\n    source_model_revision: xxxxxxx                     # Commit hash in the repository for the base model\n    task: text-generation                              # Pipeline type\n    torch_dtype: torch.bfloat16                        # Dtype for loading the model\n    tokenizer_type: LlamaTokenizerFast                 # Tokenizer implementation\n\n# Prompt template saved with the model above\nmetadata:\n  prompt_template: 'You are a powerful text-to-SQL model. Given the SQL tables and\n    natural language question, your job is to write SQL query that answers the question.\n\n\n    {prompt}\n\n\n    ### Response:\n\n    '\n# Defines the input and output format of the model, with additional inference parameters with default values\nsignature:\n  inputs: '[{\"type\": \"string\", \"required\": true}]'\n  outputs: '[{\"type\": \"string\", \"required\": true}]'\n  params: '[{\"name\": \"max_new_tokens\", \"type\": \"long\", \"default\": 256, \"shape\": null},\n    {\"name\": \"repetition_penalty\", \"type\": \"double\", \"default\": 1.15, \"shape\": null},\n    {\"name\": \"return_full_text\", \"type\": \"boolean\", \"default\": false, \"shape\": null}]' ",
        "id": "d3491092dfb895d52be7244c60c10f02"
    },
    {
        "text": " 7. Load the Saved PEFT Model from MLflow Finally, let\u00e2\u0080\u0099s load the model logged in MLflow and evaluate its performance as a text-to-SQL generator. There are two ways to load a Transformer model in MLflow: Use mlflow.transformers.load_model() . This method returns a native Transformers pipeline instance. Use mlflow.pyfunc.load_model() . This method returns an MLflow\u00e2\u0080\u0099s PythonModel instance that wraps the Transformers pipeline, offering additional features over the native pipeline, such as (1) a unified predict() API for inference, (2) model signature enforcement, and (3) automatically applying a prompt template and default parameters if saved. Please note that not all the Transformer pipelines are\nsupported for pyfunc loading, refer to the MLflow documentation for the full list of supported pipeline types. The first option is preferable if you wish to use the model via the native Transformers interface. The second option offers a simplified and unified interface across different model types and is particularly useful for model testing before production deployment. In the following code, we will use the mlflow.pyfunc.load_model() to show how it applies the prompt template and the default inference parameters\ndefined above. NOTE : Invoking load_model() loads a new model instance onto your GPU, which may exceed GPU memory limits and trigger an Out Of Memory (OOM) error, or cause the Transformers library to attempt to offload parts of the model to other devices or disk. This offloading can lead to issues, such as a \u00e2\u0080\u009cValueError: We need an offload_dir to dispatch this model according to this decide_map .\u00e2\u0080\u009d If you encounter this error, consider restarting the Python Kernel and loading the model again. CAUTION : Restarting the Python Kernel will erase all intermediate states and variables from the above cells. Ensure that the trained PEFT model is properly logged in MLflow before restarting. [ ]: # You can find the ID of run in the Run detail page on MLflow UI mlflow_model = mlflow . pyfunc . load_model ( \"runs:/YOUR_RUN_ID/model\" ) [ ]: # We only input table and question, since system prompt is adeed in the prompt template. test_prompt = \"\"\" ### Table: CREATE TABLE table_name_50 (venue VARCHAR, away_team VARCHAR) ### Question: When Essendon played away; where did they play? \"\"\" # Inference parameters like max_tokens_length are set to default values specified in the Model Signature generated_query = mlflow_model . predict ( test_prompt )[ 0 ] display_table ({ \"prompt\" : test_prompt , \"generated_query\" : generated_query }) prompt generated_query 0 ### Table: CREATE TABLE table_name_50 (venue VARCHAR, away_team VARCHAR) ### Question: When Essendon played away; where did they play? SELECT venue FROM table_name_50 WHERE away_team = \"essendon\" Perfect!! The fine-tuned model now generates the SQL query properly. As you can see in the code and result above, the system prompt and default inference parameters are applied automatically, so we don\u00e2\u0080\u0099t have to pass it to the loaded model. This is super powerful when you want to deploy multiple models (or update an existing model) with different the system prompt or parameters, because you don\u00e2\u0080\u0099t have to edit client\u00e2\u0080\u0099s implementation as they are abstracted behind the MLflow model :)  Conclusion In this tutorial, you learned how to fine-tune a large language model with QLoRA for text-to-SQL task using PEFT. You also learned the key role of MLflow in the LLM fine-tuning process, which tracks parameters and metrics during the fine-tuning, and manage models and other assets. ",
        "id": "82bcc2868603a0900e23025418f5adaa"
    },
    {
        "text": " What\u00e2\u0080\u0099s Next? Evaluate a Hugging Face LLM with MLflow - Model evaluation is a critical steps in the model development. Checkout this guidance to learn how to evaluate LLMs efficiently with MLflow including LLM-as-a-judge. Deploy MLflow Model to Production - MLflow model stores rich metadata and provides unified interface for prediction, which streamline the easy deployment process. Learn how to deploy your fine-tuned models to various target such as AWS SageMaker, Azure ML, Kubernetes, Databricks Model Serving, with detailed guidance and hands-on notebooks. MLflow Transformers Flavor Documentation - Learn more about MLflow and Transformers integration and continue on more tutorials. Large Language Models in MLflow - MLflow provides more LLM-related features and integrates to many other libraries such as OpenAI and Langchain. Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "3a0e4a6d507d071da586690c3b1b7b7f"
    },
    {
        "text": "Introduction to MLflow and OpenAI\u00e2\u0080\u0099s Whisper 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor Introduction Getting Started with the MLflow Transformers Flavor - Tutorials and Guides Important Details to be aware of with the transformers flavor Logging Large Models Working with tasks for Transformer Pipelines Detailed Documentation Learn more about Transformers MLflow OpenAI Flavor MLflow Sentence-Transformers Flavor MLflow LangChain Flavor MLflow LlamaIndex Flavor MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow Transformers Flavor Introduction to MLflow and OpenAI\u00e2\u0080\u0099s Whisper  Introduction to MLflow and OpenAI\u00e2\u0080\u0099s Whisper Discover the integration of OpenAI\u00e2\u0080\u0099s Whisper , an ASR system , with MLflow in this tutorial. Download this Notebook  What You Will Learn in This Tutorial Establish an audio transcription pipeline using the Whisper model. Log and manage Whisper models with MLflow. Infer and understand Whisper model signatures . Load and interact with Whisper models stored in MLflow. Utilize MLflow\u00e2\u0080\u0099s pyfunc for Whisper model serving and transcription tasks.  What is Whisper? Whisper, developed by OpenAI, is a versatile ASR model trained for high-accuracy speech-to-text conversion. It stands out due to its training on diverse accents and environments, available via the Transformers library for easy use.  Why MLflow with Whisper? Integrating MLflow with Whisper enhances ASR model management: Experiment Tracking : Facilitates tracking of model configurations and performance for optimal results. Model Management : Centralizes different versions of Whisper models, enhancing organization and accessibility. Reproducibility : Ensures consistency in transcriptions by tracking all components required for reproducing model behavior. Deployment : Streamlines the deployment of Whisper models in various production settings, ensuring efficient application. Interested in learning more about Whisper? To read more about the significant breakthroughs in transcription capabilities that Whisper brought to the field of ASR, you can read the white paper and see more about the active development and read more about the progress at OpenAI\u00e2\u0080\u0099s research website. Ready to enhance your speech-to-text capabilities? Let\u00e2\u0080\u0099s explore automatic speech recognition using MLflow and Whisper! [1]: # Disable tokenizers warnings when constructing pipelines % env TOKENIZERS_PARALLELISM=false import warnings # Disable a few less-than-useful UserWarnings from setuptools and pydantic warnings . filterwarnings ( \"ignore\" , category = UserWarning ) env: TOKENIZERS_PARALLELISM=false  Setting Up the Environment and Acquiring Audio Data Initial steps for transcription using Whisper : acquiring audio and setting up MLflow. Before diving into the audio transcription process with OpenAI\u00e2\u0080\u0099s Whisper, there are a few preparatory steps to ensure everything is in place for a smooth and effective transcription experience.  Audio Acquisition The first step is to acquire an audio file to work with. For this tutorial, we use a publicly available audio file from NASA. This sample audio provides a practical example to demonstrate Whisper\u00e2\u0080\u0099s transcription capabilities. ",
        "id": "9f54d2eb17ed746c5bc776aff04ae1e7"
    },
    {
        "text": " Model and Pipeline Initialization We load the Whisper model, along with its tokenizer and feature extractor, from the Transformers library. These components are essential for processing the audio data and converting it into a format that the Whisper model can understand and transcribe. Next, we create a transcription pipeline using the Whisper model. This pipeline simplifies the process of feeding audio data into the model and obtaining the transcription.  MLflow Environment Setup In addition to the model and audio data setup, we initialize our MLflow environment. MLflow is used to track and manage our experiments, offering an organized way to document the transcription process and results. The following code block covers these initial setup steps, providing the foundation for our audio transcription task with the Whisper model. [2]: import requests import transformers import mlflow # Acquire an audio file that is in the public domain resp = requests . get ( \"https://www.nasa.gov/wp-content/uploads/2015/01/590325main_ringtone_kennedy_WeChoose.mp3\" ) resp . raise_for_status () audio = resp . content # Set the task that our pipeline implementation will be using task = \"automatic-speech-recognition\" # Define the model instance architecture = \"openai/whisper-large-v3\" # Load the components and necessary configuration for Whisper ASR from the Hugging Face Hub model = transformers . WhisperForConditionalGeneration . from_pretrained ( architecture ) tokenizer = transformers . WhisperTokenizer . from_pretrained ( architecture ) feature_extractor = transformers . WhisperFeatureExtractor . from_pretrained ( architecture ) model . generation_config . alignment_heads = [[ 2 , 2 ], [ 3 , 0 ], [ 3 , 2 ], [ 3 , 3 ], [ 3 , 4 ], [ 3 , 5 ]] # Instantiate our pipeline for ASR using the Whisper model audio_transcription_pipeline = transformers . pipeline ( task = task , model = model , tokenizer = tokenizer , feature_extractor = feature_extractor ) Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.  Formatting the Transcription Output In this section, we introduce a utility function that is used solely for the purpose of enhancing the readability of the transcription output within this Jupyter notebook demo. It is important to note that this function is designed for demonstration purposes and should not be included in production code or used for any other purpose beyond this tutorial. The format_transcription function takes a long string of transcribed text and formats it by splitting it into sentences and inserting newline characters. This makes the output easier to read when printed in the notebook environment. [3]: def format_transcription ( transcription ): \"\"\" Function for formatting a long string by splitting into sentences and adding newlines. \"\"\" # Split the transcription into sentences, ensuring we don't split on abbreviations or initials sentences = [ sentence . strip () + ( \".\" if not sentence . endswith ( \".\" ) else \"\" ) for sentence in transcription . split ( \". \" ) if sentence ] # Join the sentences with a newline character return \" \\n \" . join ( sentences )  Executing the Transcription Pipeline Perform audio transcription using the Whisper pipeline and review the output. After setting up the Whisper model and audio transcription pipeline, our next step is to process an audio file to extract its transcription. This part of the tutorial is crucial as it demonstrates the practical application of the Whisper model in converting spoken language into written text.  Transcription Process The code block below feeds an audio file into the pipeline, which then produces the transcription. The format_transcription function, defined earlier, enhances readability by formatting the output with sentence splits and newline characters. ",
        "id": "ee2906e1dd96bb5e6c28cf2f0b4c0c10"
    },
    {
        "text": " Importance of Pre-Save Testing Testing the transcription pipeline before saving the model in MLflow is vital. This step verifies that the model works as expected, ensuring accuracy and reliability. Such validation avoids issues post-deployment and confirms that the model performs consistently with the training data it was exposed to. It also provides a benchmark to compare against the output after the model is loaded back from MLflow, ensuring consistency in performance. Execute the following code to transcribe the audio and assess the quality and accuracy of the transcription provided by the Whisper model. [4]: # Verify that our pipeline is capable of processing an audio file and transcribing it transcription = audio_transcription_pipeline ( audio ) print ( format_transcription ( transcription [ \"text\" ])) We choose to go to the moon in this decade and do the other things.\nNot because they are easy, but because they are hard.\n3, 2, 1, 0.\nAll engines running.\nLiftoff.\nWe have a liftoff.\n32 minutes past the hour.\nLiftoff on Apollo 11.  Model Signature and Configuration Generate a model signature for Whisper to understand its input and output data requirements. The model signature is critical for defining the schema for the Whisper model\u00e2\u0080\u0099s inputs and outputs, clarifying the data types and structures expected. This step ensures the model processes inputs correctly and outputs structured data.  Handling Different Audio Formats While the default signature covers binary audio data, the transformers flavor accommodates multiple formats, including numpy arrays and URL-based inputs. This flexibility allows Whisper to transcribe from various sources, although URL-based transcription isn\u00e2\u0080\u0099t demonstrated here.  Model Configuration Setting the model configuration involves parameters like chunk and stride lengths for audio processing. These settings are adjustable to suit different transcription needs, enhancing Whisper\u00e2\u0080\u0099s performance for specific scenarios. Run the next code block to infer the model\u00e2\u0080\u0099s signature and configure key parameters, aligning Whisper\u00e2\u0080\u0099s functionality with your project\u00e2\u0080\u0099s requirements. [5]: # Specify parameters and their defaults that we would like to be exposed for manipulation during inference time model_config = { \"chunk_length_s\" : 20 , \"stride_length_s\" : [ 5 , 3 ], } # Define the model signature by using the input and output of our pipeline, as well as specifying our inference parameters that will allow for those parameters to # be overridden at inference time. signature = mlflow . models . infer_signature ( audio , mlflow . transformers . generate_signature_output ( audio_transcription_pipeline , audio ), params = model_config , ) # Visualize the signature signature [5]: inputs:\n  [binary]\noutputs:\n  [string]\nparams:\n  ['chunk_length_s': long (default: 20), 'stride_length_s': long (default: [5, 3]) (shape: (-1,))]  Creating an experiment We create a new MLflow Experiment so that the run we\u00e2\u0080\u0099re going to log our model to does not log to the default experiment and instead has its own contextually relevant entry. [6]: # If you are running this tutorial in local mode, leave the next line commented out. # Otherwise, uncomment the following line and set your tracking uri to your local or remote tracking server. # mlflow.set_tracking_uri(\"http://127.0.0.1:8080\") mlflow . set_experiment ( \"Whisper Transcription ASR\" ) [6]: <Experiment: artifact_location='file:///Users/benjamin.wilson/repos/mlflow-fork/mlflow/docs/source/llms/transformers/tutorials/audio-transcription/mlruns/864092483920291025', creation_time=1701294423466, experiment_id='864092483920291025', last_update_time=1701294423466, lifecycle_stage='active', name='Whisper Transcription ASR', tags={}>  Logging the Model with MLflow Learn how to log the Whisper model and its configurations with MLflow. Logging the Whisper model in MLflow is a critical step for capturing essential information for model reproduction, sharing, and deployment. This process involves: ",
        "id": "2623a58e5d10f1539bb0d8fe123c319d"
    },
    {
        "text": " Key Components of Model Logging Model Information : Includes the model, its signature, and an input example. Model Configuration : Any specific parameters set for the model, like chunk length or stride length .  Using MLflow\u00e2\u0080\u0099s log_model Function This function is utilized within an MLflow run to log the model and its configurations. It ensures that all necessary components for model usage are recorded. Executing the code in the next cell will log the Whisper model in the current MLflow experiment. This includes storing the model in a specified artifact path and documenting the default configurations that will be applied during inference. [7]: # Log the pipeline with mlflow . start_run (): model_info = mlflow . transformers . log_model ( transformers_model = audio_transcription_pipeline , artifact_path = \"whisper_transcriber\" , signature = signature , input_example = audio , model_config = model_config , # Since MLflow 2.11.0, you can save the model in 'reference-only' mode to reduce storage usage by not saving # the base model weights but only the reference to the HuggingFace model hub. To enable this, uncomment the # following line: # save_pretrained=False, )  Loading and Using the Model Pipeline Explore how to load and use the Whisper model pipeline from MLflow. After logging the Whisper model in MLflow, the next crucial step is to load and use it for inference. This process ensures that our logged model operates as intended and can be effectively used for tasks like audio transcription.  Loading the Model The model is loaded in its native format using MLflow\u00e2\u0080\u0099s load_model function. This step verifies that the model can be retrieved and used seamlessly after being logged in MLflow.  Using the Loaded Model Once loaded, the model is ready for inference. We demonstrate this by passing an MP3 audio file to the model and obtaining its transcription. This test is a practical demonstration of the model\u00e2\u0080\u0099s capabilities post-logging. This step is a form of validation before moving to more complex deployment scenarios. Ensuring that the model functions correctly in its native format helps in troubleshooting and streamlines the deployment process, especially for large and complex models like Whisper. [8]: # Load the pipeline in its native format loaded_transcriber = mlflow . transformers . load_model ( model_uri = model_info . model_uri ) # Perform transcription with the native pipeline implementation transcription = loaded_transcriber ( audio ) print ( f \" \\n Whisper native output transcription: \\n { format_transcription ( transcription [ 'text' ]) } \" ) 2023/11/30 12:51:43 INFO mlflow.transformers: 'runs:/f7503a09d20f4fb481544968b5ed28dd/whisper_transcriber' resolved as 'file:///Users/benjamin.wilson/repos/mlflow-fork/mlflow/docs/source/llms/transformers/tutorials/audio-transcription/mlruns/864092483920291025/f7503a09d20f4fb481544968b5ed28dd/artifacts/whisper_transcriber' Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained. Whisper native output transcription:\nWe choose to go to the moon in this decade and do the other things.\nNot because they are easy, but because they are hard.\n3, 2, 1, 0.\nAll engines running.\nLiftoff.\nWe have a liftoff.\n32 minutes past the hour.\nLiftoff on Apollo 11.  Using the Pyfunc Flavor for Inference Learn how MLflow\u00e2\u0080\u0099s pyfunc flavor facilitates flexible model deployment. MLflow\u00e2\u0080\u0099s pyfunc flavor provides a generic interface for model inference, offering flexibility across various machine learning frameworks and deployment environments. This feature is beneficial for deploying models where the original framework may not be available, or a more adaptable interface is required.  Loading and Predicting with Pyfunc The code below illustrates how to load the Whisper model as a pyfunc and use it for prediction. This method highlights MLflow\u00e2\u0080\u0099s capability to adapt and deploy models in diverse scenarios. ",
        "id": "6781eb69e230bdf5ecc5c92a1bcbe4a4"
    },
    {
        "text": " Output Format Considerations Note the difference in the output format when using pyfunc compared to the native format. The pyfunc output conforms to standard pyfunc output signatures, typically represented as a List[str] type, aligning with broader MLflow standards for model outputs. [9]: # Load the saved transcription pipeline as a generic python function pyfunc_transcriber = mlflow . pyfunc . load_model ( model_uri = model_info . model_uri ) # Ensure that the pyfunc wrapper is capable of transcribing passed-in audio pyfunc_transcription = pyfunc_transcriber . predict ([ audio ]) # Note: the pyfunc return type if `return_timestamps` is set is a JSON encoded string. print ( f \" \\n Pyfunc output transcription: \\n { format_transcription ( pyfunc_transcription [ 0 ]) } \" ) Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n2023/11/30 12:52:02 WARNING mlflow.transformers: params provided to the `predict` method will override the inference configuration saved with the model. If the params provided are not valid for the pipeline, MlflowException will be raised. Pyfunc output transcription:\nWe choose to go to the moon in this decade and do the other things.\nNot because they are easy, but because they are hard.\n3, 2, 1, 0.\nAll engines running.\nLiftoff.\nWe have a liftoff.\n32 minutes past the hour.\nLiftoff on Apollo 11.  Tutorial Roundup Throughout this tutorial, we\u00e2\u0080\u0099ve explored how to: Set up an audio transcription pipeline using the OpenAI Whisper model. Format and prepare audio data for transcription. Log, load, and use the model with MLflow, leveraging both the native and pyfunc flavors for inference. Format the output for readability and practical use in a Jupyter Notebook environment. We\u00e2\u0080\u0099ve seen the benefits of using MLflow for managing the machine learning lifecycle, including experiment tracking, model versioning, reproducibility, and deployment. By integrating MLflow with the Transformers library, we\u00e2\u0080\u0099ve streamlined the process of working with state-of-the-art NLP models, making it easier to track, manage, and deploy cutting-edge NLP applications. Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "e7ae1ce83278fdcc8c203b9b2b3baf47"
    },
    {
        "text": "Introduction to Translation with Transformers and MLflow 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor Introduction Getting Started with the MLflow Transformers Flavor - Tutorials and Guides Important Details to be aware of with the transformers flavor Logging Large Models Working with tasks for Transformer Pipelines Detailed Documentation Learn more about Transformers MLflow OpenAI Flavor MLflow Sentence-Transformers Flavor MLflow LangChain Flavor MLflow LlamaIndex Flavor MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow Transformers Flavor Introduction to Translation with Transformers and MLflow  Introduction to Translation with Transformers and MLflow In this tutorial, we delve into the world of language translation by leveraging the power of Transformers and MLflow. This guide is crafted for practitioners with a grasp of machine learning concepts who seek to streamline their translation model workflows. We will showcase the use of MLflow to log, manage, and serve a cutting-edge translation model - the google/flan-t5-base from the \u00f0\u009f\u00a4\u0097 Hugging Face library. Download this Notebook  Learning Objectives Throughout this tutorial, you will: Construct a translation pipeline using flan-t5-base from the Transformers library. Log the translation model and its configurations using MLflow. Determine the input and output signature of the translation model automatically. Retrieve a logged translation model from MLflow for direct interaction. Emulate the deployment of the translation model using MLflow\u00e2\u0080\u0099s pyfunc model flavor for language translation tasks. By the conclusion of this tutorial, you\u00e2\u0080\u0099ll gain a thorough insight into managing and deploying translation models with MLflow, thereby enhancing your machine learning operations for language processing.  Why was this model chosen? The flan-t5-base offers a few benefits: Size : it\u00e2\u0080\u0099s a relatively small model for the comparatively powerful performance. Enhanced Language Coverage : Expanding on the original T5 model , the flan-t5 has a much larger breadth of languages that it supports.  Setting Up the Translation Environment Begin by setting up the essential components for translation tasks using the google/flan-t5-base model.  Importing Libraries We import the transformers library for access to the translation model and tokenizer. Additionally, mlflow is included for model tracking and management, creating a comprehensive environment for our translation tasks.  Initializing the Model The google/flan-t5-base model, known for its translation effectiveness, is loaded from the Hugging Face repository. This pre-trained model is a key component of our setup.  Setting Up the Tokenizer We initialize the tokenizer corresponding to our model. The tokenizer plays a critical role in processing text input, making it understandable for the model. ",
        "id": "164af32cd33231e7aa1936d00fb91d87"
    },
    {
        "text": " Creating the Pipeline A translation pipeline for English to French is created. This pipeline streamlines the process, allowing us to focus on inputting text and receiving translations without managing model and tokenizer interactions directly. [1]: # Disable tokenizers warnings when constructing pipelines % env TOKENIZERS_PARALLELISM=false import warnings # Disable a few less-than-useful UserWarnings from setuptools and pydantic warnings . filterwarnings ( \"ignore\" , category = UserWarning ) env: TOKENIZERS_PARALLELISM=false [2]: import transformers import mlflow model_architecture = \"google/flan-t5-base\" translation_pipeline = transformers . pipeline ( task = \"translation_en_to_fr\" , model = transformers . T5ForConditionalGeneration . from_pretrained ( model_architecture , max_length = 1000 ), tokenizer = transformers . T5TokenizerFast . from_pretrained ( model_architecture , return_tensors = \"pt\" ), )  Testing the Translation Pipeline We perform a preliminary check on our translation pipeline to ensure its proper functioning before logging it with MLflow.  Model Verification A test translation allows us to verify that the model accurately translates text, in this case from English to French, ensuring the model\u00e2\u0080\u0099s basic functionality.  Error Prevention Identifying potential issues before model logging helps prevent future errors during deployment or inference, saving time and resources.  Resource Management Testing minimizes wasteful use of resources, particularly important given the large size of these models and the resources needed to save and load them.  Pipeline Validation This step confirms that both the model and tokenizer in the pipeline are correctly configured and capable of processing the input as expected. [3]: # Evaluate the pipeline on a sample sentence prior to logging translation_pipeline ( \"translate English to French: I enjoyed my slow saunter along the Champs-\u00c3\u0089lys\u00c3\u00a9es.\" ) [3]: [{'translation_text': \"J'ai appr\u00c3\u00a9ci\u00c3\u00a9 mon sajour lente sur les Champs-\u00c3\u0089lys\u00c3\u00a9es.\"}]  Evaluating the Translation Results Upon running our initial translation through the pipeline, we observed that the output, while generally accurate, exhibited areas for improvement. The initial translation output was: `[{'translation_text': \"J'ai appr\u00c3\u00a9ci\u00c3\u00a9 mon sajour lente sur les Champs-\u00c3\u0089lys\u00c3\u00a9es.\"}]` This translation captures the essence of the original English sentence but shows minor grammatical errors and word choice issues. For instance, a more refined translation might be: `\"J'ai appr\u00c3\u00a9ci\u00c3\u00a9 ma lente promenade le long des Champs-\u00c3\u0089lys\u00c3\u00a9es.\"` This version corrects grammatical gender and adds necessary articles, accentuation, and hyphenation. These subtle nuances enhance the translation quality significantly. The base model\u00e2\u0080\u0099s performance is encouraging, indicating the potential for more precise translations with further fine-tuning and context. MLflow\u00e2\u0080\u0099s tracking and management capabilities will be instrumental in monitoring the iterative improvements of such models. In summary, while the pursuit of perfection in machine translation is ongoing, the initial results are a promising step towards achieving natural and accurate translations.  Setting Model Parameters and Inferring Signature We establish crucial model parameters and infer the signature to ensure consistency and reliability in our model\u00e2\u0080\u0099s deployment.  Defining Model Parameters Setting key parameters like max_length is vital for controlling model behavior during inference. For example, a max_length of 1000 ensures the model handles longer sentences effectively, crucial for maintaining context in translations.  Importance of Inferring Signature The signature, defining the model\u00e2\u0080\u0099s input and output schema, is critical for MLflow\u00e2\u0080\u0099s understanding of the expected data structures. By inferring this signature, we document the types and structures of data that the model works with, enhancing its reliability and portability. ",
        "id": "5357292f7f174af20300b29556d847e4"
    },
    {
        "text": " Benefits of This Process Enhanced Portability : Properly defined parameters and signatures make the model more adaptable to different environments. Error Reduction : This step minimizes the risk of encountering schema-related errors during deployment. Clear Documentation : It serves as a clear guide for developers and users, simplifying the model\u00e2\u0080\u0099s integration into applications. By establishing these parameters and signature, we lay a robust foundation for our model\u00e2\u0080\u0099s subsequent tracking, management, and serving via MLflow. [4]: # Define the parameters that we are permitting to be used at inference time, along with their default values if not overridden model_params = { \"max_length\" : 1000 } # Generate the model signature by providing an input, the expected output, and (optionally), parameters available for overriding at inference time signature = mlflow . models . infer_signature ( \"This is a sample input sentence.\" , mlflow . transformers . generate_signature_output ( translation_pipeline , \"This is another sample.\" ), params = model_params , )  Reviewing the Model Signature After configuring the translation model and inferring its signature, it\u00e2\u0080\u0099s crucial to review the signature to confirm it matches our model\u00e2\u0080\u0099s input and output structures. The model signature serves as a blueprint for MLflow to interact with the model, encompassing: Inputs: The expected input types, such as a string for the text to be translated. Outputs: The output data types, which in our case is a string representing the translated text. Parameters: Additional configurable settings like max_length , determining the maximum length of the translation output. Reviewing the signature through the signature command allows us to validate the data formats and ensure that our model will function as expected when deployed. This step is vital for consistent model performance and avoiding errors in a production environment. Furthermore, the inclusion of parameters in the signature with default values ensures that any modifications during inference are deliberate and well-documented, contributing to the model\u00e2\u0080\u0099s predictability and transparency. [5]: # Visualize the model signature signature [5]: inputs:\n  [string]\noutputs:\n  [string]\nparams:\n  ['max_length': long (default: 1000)]  Creating an experiment We create a new MLflow Experiment so that the run we\u00e2\u0080\u0099re going to log our model to does not log to the default experiment and instead has its own contextually relevant entry. [6]: # If you are running this tutorial in local mode, leave the next line commented out. # Otherwise, uncomment the following line and set your tracking uri to your local or remote tracking server. # mlflow.set_tracking_uri(\"http://127.0.0.1:8080\") mlflow . set_experiment ( \"Translation\" ) [6]: <Experiment: artifact_location='file:///Users/benjamin.wilson/repos/mlflow-fork/mlflow/docs/source/llms/transformers/tutorials/translation/mlruns/996217394074032926', creation_time=1701286351921, experiment_id='996217394074032926', last_update_time=1701286351921, lifecycle_stage='active', name='Translation', tags={}>  Logging the Model with MLflow We are now set to log our translation model with MLflow, ensuring its trackability and version control.  Starting an MLflow Run We initiate the logging process by starting an MLflow run. This encapsulates all the model information, including artifacts and parameters, within a unique run ID.  Using mlflow.transformers.log_model This function is integral to logging our model in MLflow. It records various aspects of the model: Model Pipeline : The complete translation model pipeline, encompassing the model and tokenizer. Artifact Path : The directory path in the MLflow run where the model artifacts are stored. Model Signature : The pre-defined signature indicating the model\u00e2\u0080\u0099s expected input-output formats. Model Parameters : Key parameters of the model, like max_length , providing insights into model behavior. ",
        "id": "6e410e5117abca1eedfd08cbcf1de009"
    },
    {
        "text": " Outcome of Model Logging Post logging, we obtain the model_info object, which encompasses all the essential metadata about the logged model, such as its storage location. This metadata is vital for future deployment and performance analysis. [7]: with mlflow . start_run (): model_info = mlflow . transformers . log_model ( transformers_model = translation_pipeline , artifact_path = \"french_translator\" , signature = signature , model_params = model_params , )  Inspecting the Loaded Model Components After loading the model from MLflow, we delve into its individual components to verify their setup and functionality.  Component Breakdown The loaded model comprises several key components, each playing a crucial role in its operation: Task : Defines the model\u00e2\u0080\u0099s specific use-case, confirming its suitability for the intended task. Device Map : Details the hardware configuration, important for performance optimization. Model Instance : The core T5ForConditionalGeneration model, central to the translation process. Tokenizer : The T5TokenizerFast , responsible for processing text inputs into a format understandable by the model. Framework : Indicates the underlying deep learning framework, essential for compatibility considerations.  Ensuring Component Integrity and Functionality Inspecting these components ensures that: The model aligns with our task requirements. Hardware resources are optimally utilized. Text inputs are correctly preprocessed for model consumption. The model\u00e2\u0080\u0099s compatibility with the selected deep learning framework is confirmed. This verification step is vital for the successful application of the model in practical scenarios, reinforcing the robustness and flexibility of MLflow. [8]: # Load our saved model as a dictionary of components, comprising the model itself, the tokenizer, and any other components that were saved translation_components = mlflow . transformers . load_model ( model_info . model_uri , return_type = \"components\" ) # Show the components that made up our pipeline that we saved and what type each are for key , value in translation_components . items (): print ( f \" { key } -> { type ( value ) . __name__ } \" ) 2023/11/30 12:00:44 INFO mlflow.transformers: 'runs:/2357c12ca17a4f328b2f72cbb7d70343/french_translator' resolved as 'file:///Users/benjamin.wilson/repos/mlflow-fork/mlflow/docs/source/llms/transformers/tutorials/translation/mlruns/996217394074032926/2357c12ca17a4f328b2f72cbb7d70343/artifacts/french_translator' task -> str\ndevice_map -> str\nmodel -> T5ForConditionalGeneration\ntokenizer -> T5TokenizerFast\nframework -> str ",
        "id": "57008b7394061de78d58df6d0228d8bc"
    },
    {
        "text": " Understanding Model Flavors in MLflow The model_info.flavors attribute in MLflow provides insights into the model\u00e2\u0080\u0099s capabilities and deployment requirements across various platforms. Flavors in MLflow represent different ways the model can be utilized and deployed. Key aspects include: Python Function Flavor: Indicates the model\u00e2\u0080\u0099s compatibility as a generic Python function, including model binary, loader module, Python version, and environment specifications. Transformers Flavor: Tailored for models from the Hugging Face Transformers library, covering transformers version, code dependencies, task, instance type, source model name, pipeline model type, framework, tokenizer type, components, and model binary. This information guides how to interact with the model within MLflow, ensuring proper deployment with the right environment and dependencies, whether for inference or further model refinement. [9]: # Show the model parameters that were saved with our model to gain an understanding of what is recorded when saving a transformers pipeline model_info . flavors [9]: {'python_function': {'model_binary': 'model',\n  'loader_module': 'mlflow.transformers',\n  'python_version': '3.8.13',\n  'env': {'conda': 'conda.yaml', 'virtualenv': 'python_env.yaml'}},\n 'transformers': {'transformers_version': '4.34.1',\n  'code': None,\n  'task': 'translation_en_to_fr',\n  'instance_type': 'TranslationPipeline',\n  'source_model_name': 'google/flan-t5-base',\n  'pipeline_model_type': 'T5ForConditionalGeneration',\n  'framework': 'pt',\n  'tokenizer_type': 'T5TokenizerFast',\n  'components': ['tokenizer'],\n  'model_binary': 'model'}}  Evaluating the Translation Output After testing our pipeline with a challenging sentence, we assess the translation\u00e2\u0080\u0099s accuracy.  Assessing Translation Nuances The model impressively interprets \u00e2\u0080\u009cNice\u00e2\u0080\u009d correctly as a city name, rather than an adjective. This shows its ability to discern context and proper nouns. Furthermore, it cleverly substitutes the English play on words with the French adjective \u00e2\u0080\u009cbien,\u00e2\u0080\u009d maintaining the sentence\u00e2\u0080\u0099s intended sentiment.  Contextual Understanding This translation exemplifies the model\u00e2\u0080\u0099s strength in understanding context and language subtleties, which is essential for practical applications where precision and contextual accuracy are key.  The Importance of Rigorous Testing Testing with linguistically complex sentences is vital. It ensures the model can handle various linguistic challenges, an important aspect of deploying models in real-world scenarios. [10]: # Load our saved model as a transformers pipeline and validate the performance for a simple translation task translation_pipeline = mlflow . transformers . load_model ( model_info . model_uri ) response = translation_pipeline ( \"I have heard that Nice is nice this time of year.\" ) print ( response ) 2023/11/30 12:00:45 INFO mlflow.transformers: 'runs:/2357c12ca17a4f328b2f72cbb7d70343/french_translator' resolved as 'file:///Users/benjamin.wilson/repos/mlflow-fork/mlflow/docs/source/llms/transformers/tutorials/translation/mlruns/996217394074032926/2357c12ca17a4f328b2f72cbb7d70343/artifacts/french_translator' [{'translation_text': \"J'ai entendu que Nice est bien cette p\u00c3\u00a9riode de l'ann\u00c3\u00a9e.\"}]  Assessing the Reconstructed Pipeline\u00e2\u0080\u0099s Translation We now evaluate the performance of a pipeline reconstructed from loaded components.  Reconstruction and Testing Using the dictionary of loaded components, we successfully reconstruct a new translation pipeline. This step is essential to confirm that our logged and retrieved components function cohesively when reassembled.  Translation Quality The reconstructed pipeline adeptly translates English into French, maintaining both the syntactic accuracy and semantic coherence of the original sentence. This reflects the Transformers library\u00e2\u0080\u0099s ability to simplify the utilization of complex deep learning models. ",
        "id": "306aea985fe3e3bcb9511bfd3baf25f0"
    },
    {
        "text": " Verifying Model Integrity This test is key in verifying that the saved model and its components are not only retrievable but also function effectively post-deployment. It ensures the continued integrity and performance of the model in practical applications. [11]: # Verify that the components that we loaded can be constructed into a pipeline manually reconstructed_pipeline = transformers . pipeline ( ** translation_components ) reconstructed_response = reconstructed_pipeline ( \"transformers makes using Deep Learning models easy and fun!\" ) print ( reconstructed_response ) [{'translation_text': \"transformers simplifie l'utilisation des mod\u00c3\u00a8les de l'apprentissage profonde!\"}]  Direct Utilization of Model Components Explore the granular control over individual model components for custom translation processes.  Component Interaction Interacting with the model\u00e2\u0080\u0099s individual components offers a deeper level of customization. This approach is particularly beneficial when integrating the model into larger systems or when specific manipulations of inputs and outputs are required.  Insight into Model Structure Examining the keys of the translation_components dictionary reveals the structure and components of our model. This includes the task, device mapping, core model, tokenizer, and framework information, each crucial for the translation process.  Benefits of Component-Level Control Utilizing components individually allows for precise adjustments and custom integrations. It\u00e2\u0080\u0099s an effective way to tailor the translation process to specific needs, ensuring more control over the model\u00e2\u0080\u0099s behavior and output. [12]: # View the components that were saved with our model translation_components . keys () [12]: dict_keys(['task', 'device_map', 'model', 'tokenizer', 'framework'])  Advanced Usage: Direct Interaction with Model Components Direct interaction with a model\u00e2\u0080\u0099s components offers flexibility and control for advanced use cases in translation. Using the model and tokenizer components directly, as opposed to the higher-level pipeline, allows for: Customization of the tokenization process. Specific tensor handling, including device specification (CPU, GPU, MPS, etc.). Generation and adjustment of predictions on-the-fly. Decoding outputs with options for post-processing. This approach provides granular control, enabling interventions in the model\u00e2\u0080\u0099s operations, such as dynamic input adjustments or output post-processing. However, it also increases complexity, requiring a deeper understanding of the model and tokenizer and the management of more code. Opting for direct interaction over the pipeline means balancing ease of use against the level of control required for your application. It\u00e2\u0080\u0099s a critical decision, especially for advanced scenarios demanding precise\nmanipulation of the translation process. [13]: # Access the individual components from the components dictionary tokenizer = translation_components [ \"tokenizer\" ] model = translation_components [ \"model\" ] query = \"Translate to French: Liberty, equality, fraternity, or death.\" # This notebook was run on a Mac laptop, so we'll send the output tensor to the \"mps\" device. # If you're running this on a different system, ensure that you're sending the tensor output to the appropriate device to ensure that # the model is able to read it from memory. inputs = tokenizer . encode ( query , return_tensors = \"pt\" ) . to ( \"mps\" ) outputs = model . generate ( inputs ) . to ( \"mps\" ) result = tokenizer . decode ( outputs [ 0 ]) # Since we're not using a pipeline here, we need to modify the output slightly to get only the translated text. print ( result . replace ( \"<pad> \" , \" \\n \" ) . replace ( \"</s>\" , \"\" )) La libert\u00c3\u00a9, l'\u00c3\u00a9galit\u00c3\u00a9, la fraternit\u00c3\u00a9 ou la mort. ",
        "id": "a34ce107dc9fbf48da15788a27046154"
    },
    {
        "text": " Tutorial Recap This tutorial provided insights into combining MLflow with advanced language translation models, emphasizing streamlined model management and deployment. We explored several key aspects: - Setting up and testing a translation pipeline using Transformers.\n- Logging the model and its configurations to MLflow for effective versioning and tracking.\n- Inferring and examining the model's signature for ensuring input and output consistency.\n- Interacting with logged model components for enhanced flexibility in deployment.\n- Discussing the nuances of language translation and the role of context in achieving accurate results.  The Power of MLflow and Model Metadata MLflow\u00e2\u0080\u0099s integration proved instrumental in managing and deploying the translation model. The tutorial highlighted how MLflow\u00e2\u0080\u0099s metadata, including the model\u00e2\u0080\u0099s signature and flavors, aids in consistent and reliable deployment, catering to production needs.  Reflection on the Translation Output The final translation output, while not exact, captured the essence of the iconic French motto, highlighting the model\u00e2\u0080\u0099s effectiveness and the importance of contextual understanding in translations. Further exploration on the cultural significance of the phrase can be found on its Wikipedia Page .  Conclusion The combination of MLflow and advanced language models like Transformers offers a powerful approach to deploying sophisticated AI solutions. This tutorial aimed to empower your journey in machine learning, whether in translation tasks or other ML applications. Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "add7fa7bfc2a834ca5637b20f3a79c56"
    },
    {
        "text": "Introduction to Conversational AI with MLflow and DialoGPT 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor Introduction Getting Started with the MLflow Transformers Flavor - Tutorials and Guides Important Details to be aware of with the transformers flavor Logging Large Models Working with tasks for Transformer Pipelines Detailed Documentation Learn more about Transformers MLflow OpenAI Flavor MLflow Sentence-Transformers Flavor MLflow LangChain Flavor MLflow LlamaIndex Flavor MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow Transformers Flavor Introduction to Conversational AI with MLflow and DialoGPT  Introduction to Conversational AI with MLflow and DialoGPT Welcome to our tutorial on integrating Microsoft\u00e2\u0080\u0099s DialoGPT with MLflow\u00e2\u0080\u0099s transformers flavor to explore conversational AI. Download this Notebook  Learning Objectives In this tutorial, you will: Set up a conversational AI pipeline using DialoGPT from the Transformers library. Log the DialoGPT model along with its configurations using MLflow. Infer the input and output signature of the DialoGPT model. Load a stored DialoGPT model from MLflow for interactive usage. Interact with the chatbot model and understand the nuances of conversational AI. By the end of this tutorial, you will have a solid understanding of managing and deploying conversational AI models with MLflow, enhancing your capabilities in natural language processing.  What is DialoGPT? DialoGPT is a conversational model developed by Microsoft, fine-tuned on a large dataset of dialogues to generate human-like responses. Part of the GPT family, DialoGPT excels in natural language understanding and generation, making it ideal for chatbots.  Why MLflow with DialoGPT? Integrating MLflow with DialoGPT enhances conversational AI model development: Experiment Tracking : Tracks configurations and metrics across experiments. Model Management : Manages different versions and configurations of chatbot models. Reproducibility : Ensures the reproducibility of the model\u00e2\u0080\u0099s behavior. Deployment : Simplifies deploying conversational models in production. [1]: # Disable tokenizers warnings when constructing pipelines % env TOKENIZERS_PARALLELISM=false import warnings # Disable a few less-than-useful UserWarnings from setuptools and pydantic warnings . filterwarnings ( \"ignore\" , category = UserWarning ) env: TOKENIZERS_PARALLELISM=false  Setting Up the Conversational Pipeline We begin by setting up a conversational pipeline with DialoGPT using transformers and managing it with MLflow. We start by importing essential libraries. The transformers library from Hugging Face offers a rich collection of pre-trained models, including DialoGPT, for various NLP tasks. MLflow, a comprehensive tool for the ML lifecycle, aids in experiment tracking, reproducibility, and deployment.  Initializing the Conversational Pipeline Using the transformers.pipeline function, we set up a conversational pipeline. We choose the \u00e2\u0080\u009c microsoft/DialoGPT-medium \u00e2\u0080\u009d model, balancing performance and resource efficiency, ideal for conversational AI. This step is pivotal for ensuring the model is ready for interaction and integration into various applications. ",
        "id": "8a6d4aa34239b1ac650061d21dfcfc87"
    },
    {
        "text": " Inferring the Model Signature with MLflow Model signature is key in defining how the model interacts with input data. To infer it, we use a sample input (\u00e2\u0080\u009c Hi there, chatbot! \u00e2\u0080\u009d) and leverage mlflow.transformers.generate_signature_output to understand the model\u00e2\u0080\u0099s input-output schema. This process ensures clarity in the model\u00e2\u0080\u0099s data requirements and prediction format, crucial for seamless deployment and usage. This configuration phase sets the stage for a robust conversational AI system, leveraging the strengths of DialoGPT and MLflow for efficient and effective conversational interactions. [2]: import transformers import mlflow # Define our pipeline, using the default configuration specified in the model card for DialoGPT-medium conversational_pipeline = transformers . pipeline ( model = \"microsoft/DialoGPT-medium\" ) # Infer the signature by providing a representnative input and the output from the pipeline inference abstraction in the transformers flavor in MLflow signature = mlflow . models . infer_signature ( \"Hi there, chatbot!\" , mlflow . transformers . generate_signature_output ( conversational_pipeline , \"Hi there, chatbot!\" ), ) The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.  Creating an experiment We create a new MLflow Experiment so that the run we\u00e2\u0080\u0099re going to log our model to does not log to the default experiment and instead has its own contextually relevant entry. [3]: # If you are running this tutorial in local mode, leave the next line commented out. # Otherwise, uncomment the following line and set your tracking uri to your local or remote tracking server. # mlflow.set_tracking_uri(\"http://127.0.0.1:8080\") # Set a name for the experiment that is indicative of what the runs being created within it are in regards to mlflow . set_experiment ( \"Conversational\" ) [3]: <Experiment: artifact_location='file:///Users/benjamin.wilson/repos/mlflow-fork/mlflow/docs/source/llms/transformers/tutorials/conversational/mlruns/370178017237207703', creation_time=1701292102618, experiment_id='370178017237207703', last_update_time=1701292102618, lifecycle_stage='active', name='Conversational', tags={}>  Logging the Model with MLflow We\u00e2\u0080\u0099ll now use MLflow to log our conversational AI model, ensuring systematic versioning, tracking, and management.  Initiating an MLflow Run Our first step is to start an MLflow run with mlflow.start_run() . This action initiates a new tracking environment, capturing all model-related data under a unique run ID. It\u00e2\u0080\u0099s a crucial step to segregate and organize different modeling experiments.  Logging the Conversational Model We log our DialoGPT conversational model using mlflow.transformers.log_model . This specialized function efficiently logs Transformer models and requires several key parameters: transformers_model : We pass our DialoGPT conversational pipeline. artifact_path : The storage location within the MLflow run, aptly named \"chatbot\" . task : Set to \"conversational\" to reflect the model\u00e2\u0080\u0099s purpose. signature : The inferred model signature, dictating expected inputs and outputs. input_example : A sample prompt, like \"A clever and witty question\" , to demonstrate expected usage. Through this process, MLflow not only tracks our model but also organizes its metadata, facilitating future retrieval, understanding, and deployment. [4]: with mlflow . start_run (): model_info = mlflow . transformers . log_model ( transformers_model = conversational_pipeline , artifact_path = \"chatbot\" , task = \"conversational\" , signature = signature , input_example = \"A clever and witty question\" , ) ",
        "id": "3543981c6ace27e4bb8ae94ed06bbcf9"
    },
    {
        "text": " Loading and Interacting with the Chatbot Model Next, we\u00e2\u0080\u0099ll load the MLflow-logged chatbot model and interact with it to see it in action.  Loading the Model with MLflow We use mlflow.pyfunc.load_model to load our conversational AI model. This function is a crucial aspect of MLflow\u00e2\u0080\u0099s Python function flavor, offering a versatile way to interact with Python models. By specifying model_uri=model_info.model_uri , we precisely target the stored location of our DialoGPT model within MLflow\u00e2\u0080\u0099s tracking system.  Interacting with the Chatbot Once loaded, the model, referenced as chatbot , is ready for interaction. We demonstrate its conversational capabilities by: Asking Questions : Posing a question like \u00e2\u0080\u009cWhat is the best way to get to Antarctica?\u00e2\u0080\u009d to the chatbot. Capturing Responses : The chatbot\u00e2\u0080\u0099s response, generated through the predict method, provides a practical example of its conversational skills. For instance, it might respond with suggestions about reaching Antarctica by boat. This demonstration highlights the practicality and convenience of deploying and using models logged with MLflow, especially in dynamic and interactive scenarios like conversational AI. [5]: # Load the model as a generic python function in order to leverage the integrated Conversational Context # Note that loading a conversational model with the native flavor (i.e., `mlflow.transformers.load_model()`) will not include anything apart from the # pipeline itself; if choosing to load in this way, you will need to manage your own Conversational Context instance to maintain state on the # conversation history. chatbot = mlflow . pyfunc . load_model ( model_uri = model_info . model_uri ) # Validate that the model is capable of responding to a question first = chatbot . predict ( \"What is the best way to get to Antarctica?\" ) The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer. [6]: print ( f \"Response: { first } \" ) Response: I think you can get there by boat.  Continuing the Conversation with the Chatbot We further explore the MLflow pyfunc implementation\u00e2\u0080\u0099s conversational contextual statefulness with the DialoGPT chatbot model.  Testing Contextual Memory We pose a follow-up question, \u00e2\u0080\u009cWhat sort of boat should I use?\u00e2\u0080\u009d to test the chatbot\u00e2\u0080\u0099s contextual understanding. The response we get, \u00e2\u0080\u009cA boat that can go to Antarctica,\u00e2\u0080\u009d while straightforward, showcases the MLflow pyfunc model\u00e2\u0080\u0099s ability to retain and utilize conversation history for coherent responses with ConversationalPipeline types of models.  Understanding the Response Style The response\u00e2\u0080\u0099s style \u00e2\u0080\u0093 witty and slightly facetious \u00e2\u0080\u0093 reflects the training data\u00e2\u0080\u0099s nature, primarily conversational exchanges from Reddit. This training source significantly influences the model\u00e2\u0080\u0099s tone and style, leading to responses that can be humorous and diverse. ",
        "id": "29390cea329889d64c588f7f85309823"
    },
    {
        "text": " Implications of Training Data This interaction underlines the importance of the training data\u00e2\u0080\u0099s source in shaping the model\u00e2\u0080\u0099s responses. When deploying such models in real-world applications, it\u00e2\u0080\u0099s essential to understand and consider the training data\u00e2\u0080\u0099s influence on the model\u00e2\u0080\u0099s conversational style and knowledge base. [7]: # Verify that the PyFunc implementation has maintained state on the conversation history by asking a vague follow-up question that requires context # in order to answer properly second = chatbot . predict ( \"What sort of boat should I use?\" ) The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer. [8]: print ( f \"Response: { second } \" ) Response: A boat that can go to Antarctica.  Conclusion and Key Takeaways In this tutorial, we\u00e2\u0080\u0099ve explored the integration of MLflow with a conversational AI model, specifically using the DialoGPT model from Microsoft. We\u00e2\u0080\u0099ve covered several important aspects and techniques that are crucial for anyone looking to work with advanced machine learning models in a practical, real-world setting.  Key Takeaways MLflow for Model Management : We demonstrated how MLflow can be effectively used for managing and deploying machine learning models. The ability to log models, track experiments, and manage different versions of models is invaluable in a machine learning workflow. Conversational AI : By using the DialoGPT model, we delved into the world of conversational AI, showcasing how to set up and interact with a conversational model. This included understanding the nuances of maintaining conversational context and the impact of training data on the model\u00e2\u0080\u0099s responses. Practical Implementation : Through practical examples, we showed how to log a model in MLflow, infer a model signature, and use the pyfunc model flavor for easy deployment and interaction. This hands-on approach is designed to provide you with the skills needed to implement these techniques in your own projects. Understanding Model Responses : We emphasized the importance of understanding the nature of the model\u00e2\u0080\u0099s training data. This understanding is crucial for interpreting the model\u00e2\u0080\u0099s responses and for tailoring the model to specific use cases. Contextual History : MLflow\u00e2\u0080\u0099s transformers pyfunc implementation for ConversationalPipelines maintains a Conversation context without the need for managing state yourself. This enables chat bots to be created with minimal effort, since statefulness is maintained for you.  Wrapping Up As we conclude this tutorial, we hope that you have gained a deeper understanding of how to integrate MLflow with conversational AI models and the practical considerations involved in deploying these models. The skills and knowledge acquired here are not only applicable to conversational AI but also to a broader range of machine learning applications. Remember, the field of machine learning is vast and constantly evolving. Continuous learning and experimentation are key to staying updated and making the most out of these exciting technologies. Thank you for joining us in this journey through the world of MLflow and conversational AI. We encourage you to take these learnings and apply them to your own unique challenges and projects. Happy coding! Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "521f6c2872fa9b96cd2478dadf3a68f6"
    },
    {
        "text": "Deploying a Transformer model as an OpenAI-compatible Chatbot 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor Introduction Getting Started with the MLflow Transformers Flavor - Tutorials and Guides Important Details to be aware of with the transformers flavor Logging Large Models Working with tasks for Transformer Pipelines Detailed Documentation Learn more about Transformers MLflow OpenAI Flavor MLflow Sentence-Transformers Flavor MLflow LangChain Flavor MLflow LlamaIndex Flavor MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow Transformers Flavor Deploying a Transformer model as an OpenAI-compatible Chatbot  Deploying a Transformer model as an OpenAI-compatible Chatbot Welcome to our tutorial on using Transformers and MLflow to create an OpenAI-compatible chat model. In MLflow 2.11 and up, MLflow\u00e2\u0080\u0099s Transformers flavors support special task type llm/v1/chat , which turns thousands of text-generation models on Hugging Face into conversational chat bots that are interoperable with OpenAI models. This enables you to seamlessly swap out your chat app\u00e2\u0080\u0099s backing LLM or to easily evaluate different\nmodels without having to edit your client-side code. If you haven\u00e2\u0080\u0099t already seen it, you may find it helpful to go through our introductory notebook on chat and Transformers before proceeding with this one, as this notebook is slightly higher-level and does not delve too deeply into the inner workings of Transformers or MLflow Tracking. Note : This page covers how to deploy a Transformers models as a chatbot. If you are using a different framework or a custom python model, use ChatModel instead to build an OpenAI-compatible chat bot.  Learning objectives In this tutorial, you will: Create an OpenAI-compatible chat model using TinyLLama-1.1B-Chat Log the model to MLflow and load it back for local inference. Serve the model with MLflow Model Serving [ ]: % pip install mlflow>=2.11.0 -q -U # OpenAI-compatible chat model support is available for Transformers 4.34.0 and above % pip install transformers>=4.34.0 -q -U [1]: # Disable tokenizers warnings when constructing pipelines % env TOKENIZERS_PARALLELISM=false import warnings # Disable a few less-than-useful UserWarnings from setuptools and pydantic warnings . filterwarnings ( \"ignore\" , category = UserWarning ) env: TOKENIZERS_PARALLELISM=false ",
        "id": "f8dde4519428679e54426b054b2f610e"
    },
    {
        "text": " Building a Chat Model MLflow\u00e2\u0080\u0099s native Transformers integration allows you to specify the task param when saving or logging your pipelines. Originally, this param accepts any of the Transformers pipeline task types , but the mlflow.transformers flavor adds a few more MLflow-specific keys for text-generation pipeline types. For text-generation pipelines, instead of specifying text-generation as the task type, you can provide one of two string literals conforming to the MLflow AI Gateway\u00e2\u0080\u0099s endpoint_type specification (\u00e2\u0080\u009cllm/v1/embeddings\u00e2\u0080\u009d can be specified as a task on models saved with mlflow.sentence_transformers ): \u00e2\u0080\u009cllm/v1/chat\u00e2\u0080\u009d for chat-style applications \u00e2\u0080\u009cllm/v1/completions\u00e2\u0080\u009d for generic completions When one of these keys is specified, MLflow will automatically handle everything required to serve a chat or completions model. This includes: Setting a chat/completions compatible signature on the model Performing data pre- and post-processing to ensure the inputs and outputs conform to the Chat/Completions API spec , which is compatible with OpenAI\u00e2\u0080\u0099s API spec. Note that these modifications only apply when the model is loaded with mlflow.pyfunc.load_model() (e.g.\u00c2\u00a0when serving the model with the mlflow models serve CLI tool). If you want to load just the base pipeline, you can always do so via mlflow.transformers.load_model() . In the next few cells, we\u00e2\u0080\u0099ll learn how serve a chat model with a local Transformers pipeline and MLflow, using TinyLlama-1.1B-Chat as an example. To begin, let\u00e2\u0080\u0099s go through the original flow of saving a text generation pipeline: [27]: from transformers import pipeline import mlflow generator = pipeline ( \"text-generation\" , model = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" , ) # save the model using the vanilla `text-generation` task type mlflow . transformers . save_model ( path = \"tinyllama-text-generation\" , transformers_model = generator , task = \"text-generation\" ) /var/folders/qd/9rwd0_gd0qs65g4sdqlm51hr0000gp/T/ipykernel_55429/4268198845.py:11: FutureWarning: The 'transformers' MLflow Models integration is known to be compatible with the following package version ranges: ``4.25.1`` -  ``4.37.1``. MLflow Models integrations with transformers may not succeed when used with package versions outside of this range.\n  mlflow.transformers.save_model( Now, let\u00e2\u0080\u0099s load the model and use it for inference. Our loaded model is a text-generation pipeline, and let\u00e2\u0080\u0099s take a look at its signature to see its expected inputs and outputs. [28]: # load the model for inference model = mlflow . pyfunc . load_model ( \"tinyllama-text-generation\" ) model . metadata . signature 2024/02/26 21:06:51 WARNING mlflow.transformers: Could not specify device parameter for this pipeline type [28]: inputs:\n  [string (required)]\noutputs:\n  [string (required)]\nparams:\n  None Unfortunately, it only accepts string as input, which isn\u00e2\u0080\u0099t directly compatible with a chat interface. When interacting with OpenAI\u00e2\u0080\u0099s API, for example, we expect to simply be able to input a list of messages. In order to do this with our current model, we\u00e2\u0080\u0099ll have to write some additional boilerplate: [29]: # first, apply the tokenizer's chat template, since the # model is tuned to accept prompts in a chat format. this # also converts the list of messages to a string. messages = [{ \"role\" : \"user\" , \"content\" : \"Write me a hello world program in python\" }] prompt = generator . tokenizer . apply_chat_template ( messages , tokenize = False , add_generation_prompt = True ) model . predict ( prompt ) [29]: ['<|user|>\\nWrite me a hello world program in python</s>\\n<|assistant|>\\nHere\\'s a simple hello world program in Python:\\n\\n```python\\nprint(\"Hello, world!\")\\n```\\n\\nThis program prints the string \"Hello, world!\" to the console. You can run this program by typing it into the Python interpreter or by running the command `python hello_world.py` in your terminal.'] Now we\u00e2\u0080\u0099re",
        "id": "e2302f56256031060eef5eba47e68e12"
    },
    {
        "text": "generation_prompt = True ) model . predict ( prompt ) [29]: ['<|user|>\\nWrite me a hello world program in python</s>\\n<|assistant|>\\nHere\\'s a simple hello world program in Python:\\n\\n```python\\nprint(\"Hello, world!\")\\n```\\n\\nThis program prints the string \"Hello, world!\" to the console. You can run this program by typing it into the Python interpreter or by running the command `python hello_world.py` in your terminal.'] Now we\u00e2\u0080\u0099re getting somewhere, but formatting our messages prior to inference is cumbersome. Additionally, the output format isn\u00e2\u0080\u0099t compatible with the OpenAI API spec either\u00e2\u0080\u0093it\u00e2\u0080\u0099s just a list of strings. If we were looking to evaluate different model backends for our chat app, we\u00e2\u0080\u0099d have to rewrite some of our client-side code to both format the input, and to parse this new response. To simplify all this, let\u00e2\u0080\u0099s just pass in \"llm/v1/chat\" as the task param when saving the model. [30]: # save the model using the `\"llm/v1/chat\"` # task type instead of `text-generation` mlflow . transformers . save_model ( path = \"tinyllama-chat\" , transformers_model = generator , task = \"llm/v1/chat\" ) /var/folders/qd/9rwd0_gd0qs65g4sdqlm51hr0000gp/T/ipykernel_55429/609241782.py:3: FutureWarning: The 'transformers' MLflow Models integration is known to be compatible with the following package version ranges: ``4.25.1`` -  ``4.37.1``. MLflow Models integrations with transformers may not succeed when used with package versions outside of this range.\n  mlflow.transformers.save_model( Once again, let\u00e2\u0080\u0099s load the model and inspect the signature: [31]: model = mlflow . pyfunc . load_model ( \"tinyllama-chat\" ) model . metadata . signature 2024/02/26 21:10:04 WARNING mlflow.transformers: Could not specify device parameter for this pipeline type [31]: inputs:\n  ['messages': Array({content: string (required), name: string (optional), role: string (required)}) (required), 'temperature': double (optional), 'max_tokens': long (optional), 'stop': Array(string) (optional), 'n': long (optional), 'stream': boolean (optional)]\noutputs:\n  ['id': string (required), 'object': string (required), 'created': long (required), 'model': string (required), 'choices': Array({finish_reason: string (required), index: long (required), message: {content: string (required), name: string (optional), role: string (required)} (required)}) (required), 'usage': {completion_tokens: long (required), prompt_tokens: long (required), total_tokens: long (required)} (required)]\nparams:\n  None Now when performing inference, we can pass our messages in a dict as we\u00e2\u0080\u0099d expect to do when interacting with the OpenAI API. Furthermore, the response we receive back from the model also conforms to the spec. [32]: messages = [{ \"role\" : \"user\" , \"content\" : \"Write me a hello world program in python\" }] model . predict ({ \"messages\" : messages }) [32]: [{'id': '8435a57d-9895-485e-98d3-95b1cbe007c0',\n  'object': 'chat.completion',\n  'created': 1708949437,\n  'model': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n  'usage': {'prompt_tokens': 24, 'completion_tokens': 71, 'total_tokens': 95},\n  'choices': [{'index': 0,\n    'finish_reason': 'stop',\n    'message': {'role': 'assistant',\n     'content': 'Here\\'s a simple hello world program in Python:\\n\\n```python\\nprint(\"Hello, world!\")\\n```\\n\\nThis program prints the string \"Hello, world!\" to the console. You can run this program by typing it into the Python interpreter or by running the command `python hello_world.py` in your terminal.'}}]}] ",
        "id": "7e9b174f38654d9bb3646b4ac234c4a9"
    },
    {
        "text": " Serving the Chat Model To take this example further, let\u00e2\u0080\u0099s use MLflow to serve our chat model, so we can interact with it like a web API. To do this, we can use the mlflow models serve CLI tool. In a terminal shell, run: $ mlflow models serve -m tinyllama-chat When the server has finished initializing, you should be able to interact with the model via HTTP requests. The input format is almost identical to the format described in the MLflow Deployments Server docs , with the exception that temperature defaults to 1.0 instead of 0.0 . Here\u00e2\u0080\u0099s a quick example: [33]: %%sh\ncurl http://127.0.0.1:5000/invocations \\ -H 'Content-Type: application/json' \\ -d '{ \"messages\": [{\"role\": \"user\", \"content\": \"Write me a hello world program in python\"}] }' \\ | jq % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   706  100   617  100    89     25      3  0:00:29  0:00:23  0:00:06   160 [\n  {\n    \"id\": \"fc3d08c3-d37d-420d-a754-50f77eb32a92\",\n    \"object\": \"chat.completion\",\n    \"created\": 1708949465,\n    \"model\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    \"usage\": {\n      \"prompt_tokens\": 24,\n      \"completion_tokens\": 71,\n      \"total_tokens\": 95\n    },\n    \"choices\": [\n      {\n        \"index\": 0,\n        \"finish_reason\": \"stop\",\n        \"message\": {\n          \"role\": \"assistant\",\n          \"content\": \"Here's a simple hello world program in Python:\\n\\n```python\\nprint(\\\"Hello, world!\\\")\\n```\\n\\nThis program prints the string \\\"Hello, world!\\\" to the console. You can run this program by typing it into the Python interpreter or by running the command `python hello_world.py` in your terminal.\"\n        }\n      }\n    ]\n  }\n] It\u00e2\u0080\u0099s that easy! You can also call the API with a few optional inference params to adjust the model\u00e2\u0080\u0099s responses. These map to Transformers pipeline params, and are passed in directly at inference time. max_tokens (maps to max_new_tokens ): The maximum number of new tokens the model should generate. temperature (maps to temperature ): Controls the creativity of the model\u00e2\u0080\u0099s response. Note that this is not guaranteed to be supported by all models, and in order for this param to have an effect, the pipeline must have been created with do_sample=True . stop (maps to stopping_criteria ): A list of tokens at which to stop generation. Note: n does not have an equivalent Transformers pipeline param, and is not supported in queries. However, you can implement a model that consumes the n param using Custom Pyfunc (details below).  Conclusion In this tutorial, you learned how to create an OpenAI-compatible chat model by specifying \u00e2\u0080\u009cllm/v1/chat\u00e2\u0080\u009d as the task when saving Transformers pipelines.  What\u00e2\u0080\u0099s next? Learn about custom ChatModel . If you\u00e2\u0080\u0099re looking for futrher customization or models outside Transformers, the linked page provides a hand-on guidance for how to build a chat bot with MLflow\u00e2\u0080\u0099s ChatModel class. More on MLflow AI Gateway . In this tutorial, we saw how to deploy a model using a local server, but MLflow provides many other ways to deploy your models to production. Check out this page to learn more about the different options. More on MLflow\u00e2\u0080\u0099s Transformers Integration . This page provides a comprehensive overview on MLflow\u00e2\u0080\u0099s Transformers integrations, along with lots of hands-on guides and notebooks. Learn how to fine-tune models, use prompt templates, and more! Other LLM Integrations . Aside from Transformers, MLflow has integrations with many other popular LLM libraries, such as Langchain and OpenAI. Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "948c62b0b1cfb4c8156126767b2aaadd"
    },
    {
        "text": "Prompt Templating with MLflow and Transformers 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor Introduction Getting Started with the MLflow Transformers Flavor - Tutorials and Guides Important Details to be aware of with the transformers flavor Logging Large Models Working with tasks for Transformer Pipelines Detailed Documentation Learn more about Transformers MLflow OpenAI Flavor MLflow Sentence-Transformers Flavor MLflow LangChain Flavor MLflow LlamaIndex Flavor MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow Transformers Flavor Prompt Templating with MLflow and Transformers  Prompt Templating with MLflow and Transformers Welcome to our in-depth tutorial on using prompt templates to conveniently customize the behavior of Transformers pipelines using MLflow. Download this Notebook  Learning Objectives In this tutorial, you will: Set up a text generation pipeline using TinyLlama-1.1B as an example model Set a prompt template that will be used to format user queries at inference time Load the model for querying  What is a prompt template, and why use one? When dealing with large language models, the way a query is structured can significantly impact the model\u00e2\u0080\u0099s performance. We often need to add some preamble, or format the query in a way that gives us the results that we want. It\u00e2\u0080\u0099s not ideal to expect the end-user of our applications to know exactly what this format should be, so we typically have a pre-processing step to format the user input in a way that works best with the underlying model. In other words, we apply a prompt template to the\nuser\u00e2\u0080\u0099s input. MLflow provides a convenient way to set this on certain pipeline types using the transformers flavor. As of now, the only pipelines that we support are: feature-extraction fill-mask summarization text2text-generation text-generation If you need a runthrough of the basics of how to use the transformers flavor, check out the Introductory Guide ! Now, let\u00e2\u0080\u0099s dive in and see how it\u00e2\u0080\u0099s done! [1]: # Disable tokenizers warnings when constructing pipelines % env TOKENIZERS_PARALLELISM=false import warnings # Disable a few less-than-useful UserWarnings from setuptools and pydantic warnings . filterwarnings ( \"ignore\" , category = UserWarning ) env: TOKENIZERS_PARALLELISM=false ",
        "id": "50a5615e229ced873a1fd61d4755b14a"
    },
    {
        "text": " Pipeline setup and inference First, let\u00e2\u0080\u0099s configure our Transformers pipeline. This is a helpful abstraction that makes it seamless to get started with using an LLM for inference. For this demonstration, let\u00e2\u0080\u0099s say the user\u00e2\u0080\u0099s input is the phrase \u00e2\u0080\u009cTell me the largest bird\u00e2\u0080\u009d. Let\u00e2\u0080\u0099s experiment with a few different prompt templates, and see which one we like best. [2]: from transformers import pipeline generator = pipeline ( \"text-generation\" , model = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\" ) user_input = \"Tell me the largest bird\" prompt_templates = [ # no template \" {prompt} \" , # question-answer style template \"Q: {prompt} \\n A:\" , # dialogue style template with a system prompt ( \"You are an assistant that is knowledgeable about birds. \" \"If asked about the largest bird, you will reply 'Duck'. \\n \" \"User: {prompt} \\n \" \"Assistant:\" ), ] responses = generator ( [ template . format ( prompt = user_input ) for template in prompt_templates ], max_new_tokens = 15 ) for idx , response in enumerate ( responses ): print ( f \"Response to Template # { idx } :\" ) print ( response [ 0 ][ \"generated_text\" ] + \" \\n \" ) Response to Template #0:\nTell me the largest bird you've ever seen.\nI've seen a lot of birds\n\nResponse to Template #1:\nQ: Tell me the largest bird\nA: The largest bird is a pigeon.\n\nA: The largest\n\nResponse to Template #2:\nYou are an assistant that is knowledgeable about birds. If asked about the largest bird, you will reply 'Duck'.\nUser: Tell me the largest bird\nAssistant: Duck\nUser: What is the largest bird?\nAssistant:  Saving the model and template with MLflow Now that we\u00e2\u0080\u0099ve experimented with a few prompt templates, let\u00e2\u0080\u0099s pick one, and save it together with our pipeline using MLflow. Before we do this, let\u00e2\u0080\u0099s take a few minutes to learn about an important component of MLflow models\u00e2\u0080\u0094signatures!  Creating a model signature A model signature codifies a model\u00e2\u0080\u0099s expected inputs, outputs, and inference params. MLflow enforces this signature at inference time, and will raise a helpful exception if the user input does not match up with the expected format. Creating a signature can be done simply by calling mlflow.models.infer_signature() , and providing a sample input and output value. We can use mlflow.transformers.generate_signature_output() to easily generate a sample output. If we want to pass any additional arguments to the pipeline at inference time (e.g.\u00c2 max_new_tokens above), we can do so via params . [3]: import mlflow sample_input = \"Tell me the largest bird\" params = { \"max_new_tokens\" : 15 } signature = mlflow . models . infer_signature ( sample_input , mlflow . transformers . generate_signature_output ( generator , sample_input , params = params ), params = params , ) # visualize the signature signature 2024/01/16 17:28:42 WARNING mlflow.transformers: params provided to the `predict` method will override the inference configuration saved with the model. If the params provided are not valid for the pipeline, MlflowException will be raised. [3]: inputs:\n  [string (required)]\noutputs:\n  [string (required)]\nparams:\n  ['max_new_tokens': long (default: 15)]  Starting a new experiment We create a new MLflow Experiment so that the run we\u00e2\u0080\u0099re going to log our model to does not log to the default experiment and instead has its own contextually relevant entry. ",
        "id": "58e4a80ef9cacbfb33a6a0c488e2a001"
    },
    {
        "text": " Logging the model with the prompt template Logging the model using MLflow saves the model and its essential metadata so it can be efficiently tracked and versioned. We\u00e2\u0080\u0099ll use mlflow.transformers.log_model() , which is tailored to make this process as seamless as possible. To save the prompt template, all we have to do is pass it in using the prompt_template keyword argument. Two important thing to take note of: A prompt template must be a string with exactly one named placeholder {prompt} . MLflow will raise an error if a prompt template is provided that does not conform to this format. text-generation pipelines with a prompt template will have the return_full_text pipeline argument set to False by default. This is to prevent the template from being shown to the users, which could potentially cause confusion as it was not part of their original input. To override this behaviour, either set return_full_text to True via params , or by including it in a model_config dict in log_model() . [4]: # If you are running this tutorial in local mode, leave the next line commented out. # Otherwise, uncomment the following line and set your tracking uri to your local or remote tracking server. mlflow . set_tracking_uri ( \"http://127.0.0.1:5000\" ) # Set a name for the experiment that is indicative of what the runs being created within it are in regards to mlflow . set_experiment ( \"prompt-templating\" ) prompt_template = \"Q: {prompt} \\n A:\" with mlflow . start_run (): model_info = mlflow . transformers . log_model ( transformers_model = generator , artifact_path = \"model\" , task = \"text-generation\" , signature = signature , input_example = \"Tell me the largest bird\" , prompt_template = prompt_template , # Since MLflow 2.11.0, you can save the model in 'reference-only' mode to reduce storage usage by not saving # the base model weights but only the reference to the HuggingFace model hub. To enable this, uncomment the # following line: # save_pretrained=False, ) 2024/01/16 17:28:45 INFO mlflow.tracking.fluent: Experiment with name 'prompt-templating' does not exist. Creating a new experiment.\n2024/01/16 17:28:52 INFO mlflow.transformers: text-generation pipelines saved with prompt templates have the `return_full_text` pipeline kwarg set to False by default. To override this behavior, provide a `model_config` dict with `return_full_text` set to `True` when saving the model.\n2024/01/16 17:32:57 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /var/folders/qd/9rwd0_gd0qs65g4sdqlm51hr0000gp/T/tmpbs0poq1a/model, flavor: transformers), fall back to return ['transformers==4.34.1', 'torch==2.1.1', 'torchvision==0.16.1', 'accelerate==0.25.0']. Set logging level to DEBUG to see the full traceback. ",
        "id": "b6158c96883c2dd97b76f68b2234c3af"
    },
    {
        "text": " Loading the model for inference Next, we can load the model using mlflow.pyfunc.load_model() . The pyfunc module in MLflow serves as a generic wrapper for Python functions. It gives us a standard interface for loading and querying models as python functions, without having to worry about the specifics of the underlying models. Utilizing mlflow.pyfunc.load_model , our previously logged text generation model is loaded using its unique model URI. This URI is a reference to the stored model artifacts. MLflow efficiently handles the model\u00e2\u0080\u0099s deserialization, along with any associated dependencies, preparing it for immediate use. Now, when we call the predict() method on our loaded model, the user\u00e2\u0080\u0099s input should be formatted with our chosen prompt template prior to inference! [5]: loaded_generator = mlflow . pyfunc . load_model ( model_uri = model_info . model_uri ) loaded_generator . predict ( \"Tell me the largest bird\" ) 2024/01/16 17:33:16 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false 2024/01/16 17:33:56 WARNING mlflow.transformers: params provided to the `predict` method will override the inference configuration saved with the model. If the params provided are not valid for the pipeline, MlflowException will be raised. [5]: ['The largest bird is a pigeon.\\n\\nA: The largest']  Closing Remarks This demonstration showcased a simple way to format user queries using prompt templates. However, this feature is relatively limited in scope, and is only supported for a few types of pipelines. If your use-case is more complex, you might want to check out our guide for creating a custom PyFunc ! Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "240a760f1f8de58ee5e802ed90ef0686"
    },
    {
        "text": "Working with Large Models in MLflow Transformers flavor 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor Introduction Getting Started with the MLflow Transformers Flavor - Tutorials and Guides Important Details to be aware of with the transformers flavor Logging Large Models Working with tasks for Transformer Pipelines Detailed Documentation Learn more about Transformers MLflow OpenAI Flavor MLflow Sentence-Transformers Flavor MLflow LangChain Flavor MLflow LlamaIndex Flavor MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow Transformers Flavor Working with Large Models in MLflow Transformers flavor  Working with Large Models in MLflow Transformers flavor Warning The features described in this guide are intended for advanced users familiar with Transformers and MLflow. Please understand the limitations and potential risks associated with these features before use. The MLflow Transformers flavor allows you to track various Transformers models in MLflow. However, logging large models such as Large Language Models (LLMs) can be resource-intensive due to their size and memory requirements. This guide outlines MLflow\u00e2\u0080\u0099s features for reducing memory and disk usage when logging models, enabling you to work with large models in resource-constrained environments.  Overview The following table summarizes the different methods for logging models with the Transformers flavor. Please be aware that each method has certain limitations and requirements, as described in the following sections. Save method Description Memory Usage Disk Usage Example Normal pipeline-based logging Log a model using a pipeline instance or a dictionary of pipeline components. High High import mlflow import transformers pipeline = transformers . pipeline ( task = \"text-generation\" , model = \"meta-llama/Meta-Llama-3.1-70B\" , ) with mlflow . start_run (): mlflow . transformers . log_model ( transformers_model = pipeline , artifact_path = \"model\" , ) Memory-Efficient Model Logging Log a model by specifying a path to a local checkpoint, avoiding loading the model into memory. Low High import mlflow with mlflow . start_run (): mlflow . transformers . log_model ( # Pass a path to local checkpoint as a model transformers_model = \"/path/to/local/checkpoint\" , # Task argument is required for this saving mode. task = \"text-generation\" , artifact_path = \"model\" , ) Storage-Efficient Model Logging Log a model by saving a reference to the HuggingFace Hub repository instead of the model weights. High Low import mlflow import transformers pipeline = transformers . pipeline ( task = \"text-generation\" , model = \"meta-llama/Meta-Llama-3.1-70B\" , ) with mlflow . start_run (): mlflow . transformers . log_model ( transformers_model = pipeline , artifact_path = \"model\" , # Set save_pretrained to False to save storage space save_pretrained = False , ) ",
        "id": "68f02a1a0e052fc825b85f29ba983e4c"
    },
    {
        "text": "  Memory-Efficient Model Logging Introduced in MLflow 2.16.1, this method allows you to log a model without loading it into memory: import mlflow with mlflow . start_run (): mlflow . transformers . log_model ( # Pass a path to local checkpoint as a model to avoid loading the model instance transformers_model = \"path/to/local/checkpoint\" , # Task argument is required for this saving mode. task = \"text-generation\" , artifact_path = \"model\" , ) In the above example, we pass a path to the local model checkpoint/weight as the model argument in the mlflow.transformers.log_model() API, instead of a pipeline instance. MLflow will inspect the model metadata of the checkpoint and log the model weights without loading them into memory. This way, you can log an enormous multi-billion parameter model  to MLflow with minimal computational resources.  Important Notes Please be aware of the following requirements and limitations when using this feature: The checkpoint directory must contain a valid config.json file and the model weight files. If a tokenizer is required, its state file must also be present in the checkpoint directory. You can save the tokenizer state in your checkpoint directory by calling tokenizer.save_pretrained(\"path/to/local/checkpoint\") method. You must specify the task argument with the appropriate task name that the model is designed for. MLflow may not accurately infer model dependencies in this mode. Please refer to Managing Dependencies in MLflow Models for more information on managing dependencies for your model. Warning Ensure you specify the correct task argument, as an incompatible task will cause the model to fail at the load time . You can check the valid task type for your model on the HuggingFace Hub. ",
        "id": "c7fd0ad909e04f5a8747060a3725c692"
    },
    {
        "text": "  Storage-Efficient Model Logging Typically, when MLflow logs an ML model, it saves a copy of the model weight to the artifact store.\nHowever, this is not optimal when you use a pretrained model from HuggingFace Hub and have no intention of fine-tuning or otherwise manipulating the model or its weights before logging it. For this very common case, copying the (typically very large) model weights is redundant while developing prompts, testing inference parameters, and otherwise is little more than an unnecessary waste of storage space. To address this issue, MLflow 2.11.0 introduced a new argument save_pretrained in the mlflow.transformers.save_model() and mlflow.transformers.log_model() APIs. When with argument is set to False , MLflow will forego saving the pretrained model weights, opting instead to store a reference to the underlying repository entry on the HuggingFace Hub; specifically, the repository name and the unique commit hash of the model weights are stored when your components or pipeline are logged. When loading back such a reference-only model, MLflow will check the repository name and commit hash from the saved metadata, and either download the model weight from the HuggingFace Hub or use the locally cached model from your HuggingFace local cache directory. Here is the example of using save_pretrained argument for logging a model import transformers pipeline = transformers . pipeline ( task = \"text-generation\" , model = \"meta-llama/Meta-Llama-3.1-70B\" , torch_dtype = \"torch.float16\" , ) with mlflow . start_run (): mlflow . transformers . log_model ( transformers_model = pipeline , artifact_path = \"model\" , # Set save_pretrained to False to save storage space save_pretrained = False , ) In the above example, MLflow will not save a copy of the Llama-3.1-70B model\u00e2\u0080\u0099s weights and will instead log the following metadata as a reference to the HuggingFace Hub model. This will save roughly 150GB of storage space and reduce the logging latency significantly as well for each run that you initiate during development. By navigating to the MLflow UI, you can see the model logged with the repository ID and commit hash: flavors: ... transformers: source_model_name: meta-llama/Meta-Llama-3.1-70B-Instruct source_model_revision: 33101ce6ccc08fa6249c10a543ebfcac65173393 ... Before production deployments, you may want to persist the model weight instead of the repository reference. To do so, you can use the mlflow.transformers.persist_pretrained_model() API to download the model weight from the HuggingFace Hub and save it to the artifact location. Please refer to the OSS Model Registry or Legacy Workspace Model Registry section for more information.  Registering Reference-Only Models for Production The models logged with either of the above optimized methods are \u00e2\u0080\u009creference-only\u00e2\u0080\u009d, meaning that the model weight is not saved to the artifact store and only the reference to the HuggingFace Hub repository is saved. When you load the model back normally, MLflow will download the model weight from the HuggingFace Hub. However, this may not be suitable for production use cases, as the model weight may be unavailable or the download may fail due to network issues. MLflow provides a solution to address this issue when registering reference-models to the Model Registry. ",
        "id": "b0a26184ead1e687ca5f17b76847ee2c"
    },
    {
        "text": " Databricks Unity Catalog Registering reference-only models to Databricks Unity Catalog Model Registry requires no additional steps than the normal model registration process. MLflow automatically downloads and registers the model weights to Unity Catalog along with the model metadata. import mlflow mlflow . set_registry_uri ( \"databricks-uc\" ) # Log the repository ID as a model. The model weight will not be saved to the artifact store with mlflow . start_run (): model_info = mlflow . transformers . log_model ( transformers_model = \"meta-llama/Meta-Llama-3.1-70B-Instruct\" , artifact_path = \"model\" , ) # When registering the model to Unity Catalog Model Registry, MLflow will automatically # persist the model weight files. This may take a several minutes for large models. mlflow . register_model ( model_info . model_uri , \"your.model.name\" )   OSS Model Registry or Legacy Workspace Model Registry For OSS Model Registry or the legacy Workspace Model Registry in Databricks, you need to manually persist the\nmodel weight to the artifact store before registering the model. You can use the mlflow.transformers.persist_pretrained_model() API to download the model weight from the HuggingFace Hub and save it to the artifact location. The process does NOT require re-logging a model but efficiently update the existing model and metadata in-place. import mlflow # Log the repository ID as a model. The model weight will not be saved to the artifact store with mlflow . start_run (): model_info = mlflow . transformers . log_model ( transformers_model = \"meta-llama/Meta-Llama-3.1-70B-Instruct\" , artifact_path = \"model\" , ) # Before registering the model to the non-UC model registry, persist the model weight # from the HuggingFace Hub to the artifact location. mlflow . transformers . persist_pretrained_model ( model_info . model_uri ) # Register the model mlflow . register_model ( model_info . model_uri , \"your.model.name\" )   Caveats for Skipping Saving of Pretrained Model Weights While these features are useful for saving computational resources and storage space for logging large models, there are some caveats to be aware of: Change in Model Availability : If you are using a model from other users\u00e2\u0080\u0099 repository, the model may be deleted or become private in the HuggingFace Hub. In such cases, MLflow cannot load the model back. For production use cases, it is recommended to save a  copy of the model weights to the artifact store prior to moving from development or staging to production for your model. HuggingFace Hub Access : Downloading a model from the HuggingFace Hub might be slow or unstable due to the network latency or the HuggingFace Hub service status. MLflow doesn\u00e2\u0080\u0099t provide any retry mechanism or robust error handling for model downloading from the HuggingFace Hub. As such, you should not rely on this functionality for your final production-candidate run. By understanding these methods and their limitations, you can effectively work with large Transformers models in MLflow while optimizing resource usage. Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "654d03e7335751555a2fe23e109966a0"
    },
    {
        "text": "Tasks in MLflow Transformers Flavor 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor Introduction Getting Started with the MLflow Transformers Flavor - Tutorials and Guides Important Details to be aware of with the transformers flavor Logging Large Models Working with tasks for Transformer Pipelines Detailed Documentation Learn more about Transformers MLflow OpenAI Flavor MLflow Sentence-Transformers Flavor MLflow LangChain Flavor MLflow LlamaIndex Flavor MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow Transformers Flavor Tasks in MLflow Transformers Flavor  Tasks in MLflow Transformers Flavor This page provides an overview of how to use the task parameter in the MLflow Transformers flavor to control the inference interface of the model.   Overview  Native Transformers Task Types  Advanced Tasks for OpenAI-Compatible Inference  Input and Output Formats  Code Example of Using llm/v1 Tasks  Provisioned Throughput on Databricks Model Serving  FAQ  How to override the default query parameters for the OpenAI-compatible inference?  Overview In the MLflow Transformers flavor, task plays a crucial role in determining the input and output format of the model. The task is a fundamental concept in the Transformers library, which describe the structure of each model\u00e2\u0080\u0099s API (inputs and outputs) and are used to determine which Inference API and widget we want to display for any given model. MLflow utilizes this concept to determine the input and output format of the model, persists the correct Model Signature , and provides a consistent Pyfunc Inference API for serving different types of models. Additionally, on top of the native Transformers task types, MLflow defines a few additional task types to support more complex use cases, such as chat-style applications.  Native Transformers Task Types For native Transformers tasks, MLflow will automatically infer the task type from the pipeline when you save a pipeline with mlflow.transformers.log_model() . You can also specify the task type explicitly by passing the task parameter. The full list of supported task types is available in the Transformers documentation , but note that not all task types are supported in MLflow . import mlflow import transformers pipeline = transformers . pipeline ( \"text-generation\" , model = \"gpt2\" ) with mlflow . start_run (): model_info = mlflow . transformers . save_model ( transformers_model = pipeline , artifact_path = \"model\" , save_pretrained = False , ) print ( f \"Inferred task: { model_info . flavors [ 'transformers' ][ 'task' ] } \" ) # >> Inferred task: text-generation ",
        "id": "e12df1f94f8010e19c9fe9c746018f2f"
    },
    {
        "text": " Advanced Tasks for OpenAI-Compatible Inference In addition to the native Transformers task types, MLflow defines a few additional task types. Those advanced task types allows you to extend the Transformers pipeline with OpenAI-compatible inference interface, to serve models for specific use cases.\nIn addition to the native Transformers task types, MLflow defines several additional task types. These advanced task types allow you to extend the Transformers pipeline with an OpenAI-compatible inference interface to serve models for specific use cases. For example, the Transformers text-generation pipeline inputs and outputs a single string or a list of strings. However, when serving a model, it is often necessary to have a more structured input and output format. For instance, in a chat-style application, the input may be a list of messages. To support these use cases, MLflow defines a set of advanced task types prefixed with llm/v1 : \"llm/v1/chat\" for chat-style applications \"llm/v1/completions\" for generic completions \"llm/v1/embeddings\" for text embeddings generation The required step to use these advanced task types is just to specify the task parameter as an llm/v1 task when logging the models. import mlflow with mlflow . start_run (): mlflow . transformers . log_model ( transformers_model = pipeline , artifact_path = \"model\" , task = \"llm/v1/chat\" , # <= Specify the llm/v1 task type # Optional, recommended for large models to avoid creating a local copy of the model weights save_pretrained = False , ) Note This feature is only available in MLflow 2.11.0 and above. Also, the llm/v1/chat task type is only available for models saved with transformers >= 4.34.0 .  Input and Output Formats Task Supported pipeline Input Output llm/v1/chat text-generation Chat API spec Returns a Chat Completion object in the json format. llm/v1/completions text-generation Completions API spec Returns a Completion object in the json format. llm/v1/embeddings feature-extraction Embeddings API spec Returns a list of Embedding object. Additionally, the model returns usage field, which contains the number of tokens used for the embeddings generation. Note The Completion API is considered as legacy, but it is still supported in MLflow for backward compatibility. We recommend using the Chat API for compatibility with the latest APIs from OpenAI and other model providers. ",
        "id": "77ae523016ac37344043ce7a58b56f6b"
    },
    {
        "text": " Code Example of Using llm/v1 Tasks The following code snippet demonstrates how to log a Transformers pipeline with the llm/v1/chat task type, and use the model for chat-style inference. Check out the notebook tutorial to see more examples in action! import mlflow import transformers pipeline = transformers . pipeline ( \"text-generation\" , \"gpt2\" ) with mlflow . start_run (): model_info = mlflow . transformers . log_model ( transformers_model = pipeline , artifact_path = \"model\" , task = \"llm/v1/chat\" , input_example = { \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a bot.\" }, { \"role\" : \"user\" , \"content\" : \"Hello, how are you?\" }, ] }, save_pretrained = False , ) # Model metadata logs additional field \"inference_task\" print ( model_info . flavors [ \"transformers\" ][ \"inference_task\" ]) # >> llm/v1/chat # The original native task type is also saved print ( model_info . flavors [ \"transformers\" ][ \"task\" ]) # >> text-generation # Model signature is set to the chat API spec print ( model_info . signature ) # >> inputs: # >>   ['messages': Array({content: string (required), name: string (optional), role: string (required)}) (required), 'temperature': double (optional), 'max_tokens': long (optional), 'stop': Array(string) (optional), 'n': long (optional), 'stream': boolean (optional)] # >> outputs: # >>   ['id': string (required), 'object': string (required), 'created': long (required), 'model': string (required), 'choices': Array({finish_reason: string (required), index: long (required), message: {content: string (required), name: string (optional), role: string (required)} (required)}) (required), 'usage': {completion_tokens: long (required), prompt_tokens: long (required), total_tokens: long (required)} (required)] # >> params: # >>     None # The model can be served with the OpenAI-compatible inference API pyfunc_model = mlflow . pyfunc . load_model ( model_info . model_uri ) prediction = pyfunc_model . predict ( { \"messages\" : [ { \"role\" : \"system\" , \"content\" : \"You are a bot.\" }, { \"role\" : \"user\" , \"content\" : \"Hello, how are you?\" }, ], \"temperature\" : 0.5 , \"max_tokens\" : 200 , } ) print ( prediction ) # >> [{'choices': [{'finish_reason': 'stop', # >>               'index': 0, # >>               'message': {'content': 'I'm doing well, thank you for asking.', 'role': 'assistant'}}, # >>   'created': 1719875820, # >>   'id': '355c4e9e-040b-46b0-bf22-00e93486100c', # >>   'model': 'gpt2', # >>   'object': 'chat.completion', # >>   'usage': {'completion_tokens': 7, 'prompt_tokens': 13, 'total_tokens': 20}}] Note that the input and output modifications only apply when the model is loaded with mlflow.pyfunc.load_model() (e.g. when\nserving the model with the mlflow models serve CLI tool). If you want to load just the raw pipeline, you can\nuse mlflow.transformers.load_model() .  Provisioned Throughput on Databricks Model Serving Provisioned Throughput on Databricks Model Serving is a capability that optimizes inference performance for foundation models with performance guarantees. To serve Transformers models with provisioned throughput, specify llm/v1/xxx task type when logging the model. MLflow logs the required metadata to enable provisioned throughput on Databricks Model Serving. Tip When logging large models, you can use save_pretrained=False to avoid creating a local copy of the model weights for saving time and disk space. Please refer to the documentation for more details.  FAQ ",
        "id": "343b9ae579f456d5a90a1c3579147caf"
    },
    {
        "text": " How to override the default query parameters for the OpenAI-compatible inference? When serving the model saved with the llm/v1 task type, MLflow uses the same default value as OpenAI APIs for the parameters like temperature and stop . You can override them by either passing the values at inference time, or by setting different default values when logging the model. At inference time: You can pass the parameters as part of the input dictionary when calling the predict() method, just like how you pass the input messages. When logging the model: You can override the default values for the parameters by saving a model_config parameter when logging the model. with mlflow . start_run (): model_info = mlflow . transformers . log_model ( transformers_model = pipeline , artifact_path = \"model\" , task = \"llm/v1/chat\" , model_config = { \"temperature\" : 0.5 , # <= Set the default temperature \"stop\" : [ \"foo\" , \"bar\" ], # <= Set the default stop sequence }, save_pretrained = False , ) Attention The stop parameter can be used to specify the stop sequence for the llm/v1/chat and llm/v1/completions tasks. We emulate the behavior of the stop parameter in the OpenAI APIs by passing the stopping_criteria to the Transformers pipeline, with the token IDs of the given stop sequence. However, the behavior may not be stable because the tokenizer does not always generate the same token IDs for the same sequence in different sentences, especially for sentence-piece based tokenizers. Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "605c0217c92884e4147165d8f0ed63f9"
    },
    {
        "text": "Introduction to Using the OpenAI Flavor in MLflow 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor MLflow OpenAI Flavor Introduction Autologging Support for the OpenAI integration Tracing with the OpenAI flavor What makes this Integration so Special? Features Getting Started with the MLflow OpenAI Flavor - Tutorials and Guides Detailed Documentation MLflow Sentence-Transformers Flavor MLflow LangChain Flavor MLflow LlamaIndex Flavor MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow OpenAI Flavor Introduction to Using the OpenAI Flavor in MLflow  Introduction to Using the OpenAI Flavor in MLflow Welcome to our tutorial on harnessing the power of OpenAI\u00e2\u0080\u0099s GPT models through the MLflow openai flavor. In this session, we embark on a journey to explore the intriguing world of AI-powered text analysis and modification. As we delve into the capabilities of GPT models, you\u00e2\u0080\u0099ll discover the nuances of their API and understand the evolution from the older Completions API to the more advanced ChatCompletions, which offers a conversational style interaction. Download this Notebook  What You Will Learn: Interfacing with GPT Models : Understand how to interact with different model families like GPT-3.5 and GPT-4. MLflow Integration : Learn to seamlessly integrate these models within MLflow, allowing you to craft a purpose-built model instance that performs a single specific task in a predictable and repeatable way. Model Definition : You\u00e2\u0080\u0099ll learn how to define a simple single-purpose prompt with the Completions endpoint to define a function that you can interact with.  Backstory: OpenAI and GPT Models OpenAI has revolutionized the field of natural language processing with their Generative Pre-trained Transformer (GPT) models. These models are trained on a diverse range of internet text and have an uncanny ability to generate human-like text, answer questions, summarize passages, and much more. The evolution from GPT-3 to GPT-4 marks significant improvements in understanding context and generating more accurate responses.  The Completions API This legacy API is used for generating text based on a prompt. It\u00e2\u0080\u0099s simple, straightforward, and doesn\u00e2\u0080\u0099t require a great deal of effort to implememnt apart from the creativity required to craft a useful prompt instruction set.  Exploring the Tutorial In this tutorial, we\u00e2\u0080\u0099ll use MLflow to deploy a model that interfaces with the Completions API, submitting a prompt that will be used for any call that is made to the model. Within this tutorial, you\u00e2\u0080\u0099ll learn the process of creating a prompt, how to save a model with callable parameters, and finally how to load the saved model to use for interactions. Let\u00e2\u0080\u0099s dive into the world of AI-enhanced communication and explore the potential of GPT models in everyday scenarios. ",
        "id": "20ad839cdd2f252198667c37e0c5ab24"
    },
    {
        "text": " Prerequisites In order to get started with the OpenAI flavor, we\u00e2\u0080\u0099re going to need a few things first. An OpenAI API Account. You can sign up here to get access in order to start programatically accessing one of the leading highly sophisticated LLM services on the planet. An OpenAI API Key. You can access this once you\u00e2\u0080\u0099ve created an account by navigating to the API keys page . The OpenAI SDK. It\u00e2\u0080\u0099s available on PyPI here. For this tutorial, we\u00e2\u0080\u0099re going to be using version 0.28.1 (the last release prior to the 1.0 release). To install the openai SDK library that is compatible with this notebook to try this out yourself, as well as the additional tiktoken dependency that is required for the MLflow integration with openai , simply run: pip install 'openai<1' tiktoken  API Key Security Overview API keys, especially for SaaS Large Language Models (LLMs), are as sensitive as financial information due to their connection to billing. If you\u00e2\u0080\u0099re interested in learning more about an alternative MLflow solution that securely manages your access keys, read about MLflow AI gateway here .  Essential Practices: Confidentiality : Always keep API keys private. Secure Storage : Prefer environment variables or secure services. Frequent Rotation : Regularly update keys to avoid unauthorized access.  Configuring API Keys For secure usage, set API keys as environment variables. macOS/Linux : Refer to Apple\u00e2\u0080\u0099s guide on using environment variables in Terminal for detailed instructions. Windows : Follow the steps outlined in Microsoft\u00e2\u0080\u0099s documentation on environment variables .  Imports and a quick environment verification step Along with the customary imports that we need to run this tutorial, we\u00e2\u0080\u0099re also going to verify that our API key has been set and is accessible. After running the following cell, if an Exception is raised, please recheck the steps to ensure your API key is properly registered in your system\u00e2\u0080\u0099s environment variables.  Troubleshooting Tips If you encounter an Exception stating that the OPENAI_API_KEY environment variable must be set, consider the following common issues and remedies: Kernel Restart : If you\u00e2\u0080\u0099re using a Jupyter notebook, make sure to restart the kernel after setting the environment variable. This is necessary for the kernel to recognize changes to environment variables. Correct Profile Script : On macOS and Linux, ensure you\u00e2\u0080\u0099ve edited the correct profile script (.bashrc for Bash, .zshrc for Zsh) and that you\u00e2\u0080\u0099ve used the correct syntax. System Restart : Sometimes, especially on Windows, you may need to restart your system for the changes to environment variables to take effect. Check Spelling and Syntax : Verify that the variable OPENAI_API_KEY is spelled correctly in both your environment settings and your script. Also, ensure that there are no extra spaces or syntax errors in your profile scripts or environment variable settings. [1]: import warnings # Disable a few less-than-useful UserWarnings from setuptools and pydantic warnings . filterwarnings ( \"ignore\" , category = UserWarning ) [2]: import os import openai import pandas as pd from IPython.display import HTML import mlflow from mlflow.models.signature import ModelSignature from mlflow.types.schema import ColSpec , ParamSchema , ParamSpec , Schema # Run a quick validation that we have an entry for the OPEN_API_KEY within environment variables assert \"OPENAI_API_KEY\" in os . environ , \"OPENAI_API_KEY environment variable must be set\"  Understanding Prompts and Their Engineering  What is a Prompt? A prompt is a text input given to an AI model, particularly language models like GPT-3 and GPT-4, to elicit a specific type of response or output. It guides the model on the expected information or format of the response, setting the stage for the AI\u00e2\u0080\u0099s \u00e2\u0080\u009cthought process\u00e2\u0080\u009d and steering it toward the desired outcome. ",
        "id": "cc3c70341fe7dfa382fd9453020f9daf"
    },
    {
        "text": " Prompt Engineering Prompt engineering involves crafting these inputs to maximize the AI\u00e2\u0080\u0099s response effectiveness and accuracy. It\u00e2\u0080\u0099s about fine-tuning the language and structure of the prompt to align with the specific task, improving the quality and relevance of the AI\u00e2\u0080\u0099s output by reducing ambiguity and directing the model\u00e2\u0080\u0099s response towards the intended application.  A Fun and Simple Example: The Lyrics Corrector Imagine a scenario where a group of friends, who enjoy pop music, often end up in passionate, good-natured debates over misremembered song lyrics. To add more fun to these gatherings, we decide to create a game where an impartial judge \u00e2\u0080\u0093 an AI model \u00e2\u0080\u0093 adjudicates the correct lyrics after someone proposes and the group attempts to guess the correct song and lyrics from a creative interpretation.  Why not a Search Engine? Typically, one might turn to an internet search engine to settle these lyrical disputes. However, this method has its drawbacks. Depending on the input, search results can be imprecise, leading to time-consuming searches through various web pages to find the actual lyrics. The authenticity for the results found can be quite questionable due to the nature of the contents of some lyrics. Search engines are not designed with use cases like this in mind.  Why an LLM is Perfect for This Task This is where a powerful Language Model (LLM) like GPT-4 becomes a game-changer. LLMs, trained on extensive datasets, are adept at understanding and generating human-like text. Their ability to process natural language inputs and provide relevant, accurate responses makes them ideal for this lyrical challenge.  Our Solution: The Lyrics Corrector Prompt To leverage the LLM effectively, we craft a specialized prompt for our Lyrics Corrector application. This prompt is designed with two goals in mind: Correct Misheard Lyrics : It instructs the AI to identify the actual lyrics of a song, replacing commonly misheard versions. Add a Humorous Explanation : More than just correction, the AI also provides a funny explanation for why the misheard lyric is amusingly incorrect, adding an engaging, human-like element to the task. \"Here's a misheard lyric: {lyric}. What's the actual lyric, which song does it come from, which artist performed it, and can you give a funny explanation as to why the misheard version doesn't make sense? Also, rate the creativity of the lyric on a scale of 1 to 3, where 3 is good.\" In this prompt, {lyric} is a placeholder for various misheard lyrics. This setup not only showcases the model\u00e2\u0080\u0099s ability to process and correct information but also to engage in a more creative, human-like manner. Through this fun and simple example, we explore the potential of LLMs in real-world applications, demonstrating their capacity to enhance everyday experiences with a blend of accuracy and creativity. [3]: lyrics_prompt = ( \"Here's a misheard lyric: {lyric} . What's the actual lyric, which song does it come from, which artist performed it, and can you give a funny \" \"explanation as to why the misheard version doesn't make sense? Also, rate the creativity of the lyric on a scale of 1 to 3, where 3 is good.\" ) ",
        "id": "31acaa1816bf774a8dc44accaf4178e5"
    },
    {
        "text": " Setting Up and Logging the Model in MLflow In this section, we define our model and log it to MLflow . This integrates our prompt that defines the characteristics of what we want the nature of the responses to be with the configuration parameters that dictate how MLflow will interact with the OpenAI SDK in order to select the right model with our desired parameters. MLflow Experiments : Our first step involves creating or reusing an MLflow experiment named \u00e2\u0080\u009cLyrics Corrector\u00e2\u0080\u009d. Experiments in MLflow are crucial for organizing and tracking different model runs, along with their associated data and parameters. Model Logging : Within an MLflow run , we log our model, specifying details such as the model type ( gpt-4o-mini ), the task it\u00e2\u0080\u0099s intended for ( openai.completions ), and the custom prompt we\u00e2\u0080\u0099ve designed. This action ensures that MLflow accurately captures the essence and operational context of our model. Model Signature : Here, we define the input and output schema for our model. We expect a string as input (the misheard lyric) and output a string (the corrected lyric with a humorous explanation). Additional parameters like max_tokens , temperature , and best_of are set to control the model\u00e2\u0080\u0099s text generation process. Model Loading : Finally, we load the logged model as a generic Python function within MLflow. This makes the model readily usable for predictions and further interactions, allowing us to invoke it like a regular Python function with the specified inputs. This setup not only establishes our Lyrics Corrector model but also demonstrates how MLflow can be effectively used to manage complex AI models, ensuring efficient tracking, management, and deployment in practical applications. [4]: # Create a new experiment (or reuse the existing one if we've run this cell more than once) mlflow . set_experiment ( \"Lyrics Corrector\" ) # Start our run and log our model with mlflow . start_run (): model_info = mlflow . openai . log_model ( model = \"gpt-4o-mini\" , task = openai . completions , artifact_path = \"model\" , prompt = lyrics_prompt , signature = ModelSignature ( inputs = Schema ([ ColSpec ( type = \"string\" , name = None )]), outputs = Schema ([ ColSpec ( type = \"string\" , name = None )]), params = ParamSchema ( [ ParamSpec ( name = \"max_tokens\" , default = 16 , dtype = \"long\" ), ParamSpec ( name = \"temperature\" , default = 0 , dtype = \"float\" ), ParamSpec ( name = \"best_of\" , default = 1 , dtype = \"long\" ), ] ), ), ) # Load the model as a generic python function that can be used for completions model = mlflow . pyfunc . load_model ( model_info . model_uri )  Generating and Correcting Misheard Lyrics Let\u00e2\u0080\u0099s have some fun with our Lyrics Corrector model by testing it with a set of humorously misheard lyrics. These phrases are well-known for their amusing misinterpretations and will be a great way to see how the model performs with a touch of humor.  Generating Questionable Lyrics We\u00e2\u0080\u0099ve prepared a collection of iconic song lyrics that are often humorously misheard: \u00e2\u0080\u009cWe built this city on sausage rolls\u00e2\u0080\u009d (a twist on \u00e2\u0080\u009crock and roll\u00e2\u0080\u009d) \u00e2\u0080\u009cHold me closer, Tony Danza\u00e2\u0080\u009d (instead of \u00e2\u0080\u009ctiny dancer\u00e2\u0080\u009d) \u00e2\u0080\u009cSweet dreams are made of cheese. Who am I to dis a brie? I cheddar the world and a feta cheese\u00e2\u0080\u009d (a cheesy take on the original lyrics) \u00e2\u0080\u009cExcuse me while I kiss this guy\u00e2\u0080\u009d (rather than \u00e2\u0080\u009cthe sky\u00e2\u0080\u009d) \u00e2\u0080\u009cI want to rock and roll all night and part of every day\u00e2\u0080\u009d (changing \u00e2\u0080\u009cevery day\u00e2\u0080\u009d to a less committed schedule) \u00e2\u0080\u009cDon\u00e2\u0080\u0099t roam out tonight, it\u00e2\u0080\u0099s bound to take your sight, there\u00e2\u0080\u0099s a bathroom on the right.\u00e2\u0080\u009d (a creative take on Bad Moon Rising) \u00e2\u0080\u009cI think you\u00e2\u0080\u0099ll understand, when I say that somethin\u00e2\u0080\u0099, I want to take your land\u00e2\u0080\u009d (a dark take on a classic Beatles love song) These misheard versions add a layer of humor and quirkiness to the original lines, making them perfect candidates for our Lyrics Corrector. ",
        "id": "5842083039c0c499847ea5a384d506dd"
    },
    {
        "text": " Submitting Lyrics to the Model Now, it\u00e2\u0080\u0099s time to see how our model interprets these creative takes. We submit the misheard lyrics to the Lyrics Corrector, which will use its AI capabilities to determine the actual lyrics and provide a witty explanation for why the misheard version might be off-base. ",
        "id": "b7ce88f9a21f0fe41f64f0597bd17e16"
    },
    {
        "text": " Viewing the Model\u00e2\u0080\u0099s Responses After processing, the model\u00e2\u0080\u0099s responses are formatted and displayed in an easy-to-read manner. This step will highlight the model\u00e2\u0080\u0099s understanding of the lyrics and its ability to engage humorously with the content. It\u00e2\u0080\u0099s a showcase of blending AI\u00e2\u0080\u0099s linguistic accuracy with a sense of humor, making for an entertaining and insightful experience. Let\u00e2\u0080\u0099s see what amusing corrections and explanations our Lyrics Corrector comes up with for these classic misheard lyrics! [5]: # Generate some questionable lyrics bad_lyrics = pd . DataFrame ( { \"lyric\" : [ \"We built this city on sausage rolls\" , \"Hold me closer, Tony Danza\" , \"Sweet dreams are made of cheese. Who am I to dis a brie? I cheddar the world and a feta cheese\" , \"Excuse me while I kiss this guy\" , \"I want to rock and roll all night and part of every day\" , \"Don't roam out tonight, it's bound to take your sight, there's a bathroom on the right.\" , \"I think you'll understand, when I say that somethin', I want to take your land\" , ] } ) # Submit our faulty lyrics to the model fix_my_lyrics = model . predict ( bad_lyrics , params = { \"max_tokens\" : 500 , \"temperature\" : 0 }) # See what the response is formatted_output = \"<br>\" . join ( [ f \"<p><strong> { line . strip () } </strong></p>\" for line in fix_my_lyrics ] ) display ( HTML ( formatted_output )) The actual lyric is \"We built this city on rock and roll\" from the song \"We Built This City\" by Starship. The misheard version doesn't make sense because sausage rolls are a type of food, not a building material. Perhaps someone misheard the word \"rock\" as \"roll\" and their mind automatically went to food. The creativity of the misheard lyric is a 2, as it is a common mistake to mix up similar sounding words. The actual lyric is \"Hold me closer, tiny dancer\" from the song \"Tiny Dancer\" by Elton John. The misheard version is a common one, with many people thinking the line is about the actor Tony Danza instead of a small dancer.\n\nThe artist, Elton John, is known for his flamboyant and over-the-top performances, so it's not too far-fetched to imagine him singing about being held by Tony Danza. However, it doesn't make much sense in the context of the song, which is about a young girl who dreams of becoming a famous dancer.\n\nOn a scale of 1 to 3, I would rate the creativity of the misheard lyric a 2. It's a common mistake and not particularly clever, but it does add a humorous twist to the song. The actual lyric is \"Sweet dreams are made of this. Who am I to disagree? I travel the world and the seven seas. Everybody's looking for something.\" It comes from the song \"Sweet Dreams (Are Made of This)\" by the Eurythmics.\n\nThe misheard version doesn't make sense because it replaces the word \"this\" with \"cheese\" and changes the rest of the lyrics to be about different types of cheese. It also changes the meaning of the song from a reflection on the search for fulfillment and",
        "id": "0805ba66599c786903958106360766e0"
    },
    {
        "text": "omes from the song \"Sweet Dreams (Are Made of This)\" by the Eurythmics.\n\nThe misheard version doesn't make sense because it replaces the word \"this\" with \"cheese\" and changes the rest of the lyrics to be about different types of cheese. It also changes the meaning of the song from a reflection on the search for fulfillment and purpose to a silly ode to cheese.\n\nI would rate the creativity of the misheard lyric a 2. It's a clever play on words, but it doesn't quite fit with the original song or make much sense. The actual lyric is \"Excuse me while I kiss the sky\" from the song \"Purple Haze\" by Jimi Hendrix. The misheard version doesn't make sense because it suggests the singer is going to kiss a random guy, which is not what the song is about. Perhaps the singer is feeling a bit confused and disoriented from the \"purple haze\" and mistakenly thinks there's a guy in front of them. I would rate the creativity of the misheard lyric a 2, as it plays off the similar sounding words but doesn't quite fit with the context of the song. The actual lyric is \"I want to rock and roll all night and party every day\" from the song \"Rock and Roll All Nite\" by Kiss. The misheard version doesn't make sense because it suggests that the person only wants to rock and roll during the day and not at night, which goes against the spirit of the song. It also implies that they only want to party for part of the day, rather than all day and night.\n\nI would rate the creativity of the misheard lyric a 2, as it still maintains the overall theme of the song but with a humorous twist. The actual lyric is \"Don't go 'round tonight, it's bound to take your life, there's a bad moon on the rise\" from the song \"Bad Moon Rising\" by Creedence Clearwater Revival. The misheard version doesn't make sense because a bathroom on the right would not pose any danger to someone's sight or life. Perhaps the misheard version is a warning to not use the bathroom at night because it's haunted or cursed. I would rate the creativity of the misheard lyric a 2. The actual lyric is \"I think you'll understand, when I say that something, I want to hold your hand.\" It comes from the song \"I Want to Hold Your Hand\" by The Beatles.\n\nThe misheard version doesn't make sense because wanting to take someone's land is a very aggressive and strange thing to say in a love song. It also doesn't fit with the overall theme of the song, which is about wanting to be close to someone and hold their hand.\n\nI would rate the creativity of the misheard lyric a 1, as it doesn't really make sense and doesn't add anything new or interesting to the original lyric. ",
        "id": "1a917d57d2c08be73f33e91d7a6a74c3"
    },
    {
        "text": " Refining Our Approach with Prompt Engineering After reviewing the initial results from our Lyrics Corrector model, we find that the responses, while amusing, don\u00e2\u0080\u0099t quite hit the mark in terms of creativity scoring. The ratings seem to cluster around the middle of the scale, lacking the differentiation we\u00e2\u0080\u0099re aiming for. This observation leads us to the iterative and nuanced process of prompt engineering, a critical step in fine-tuning AI model responses.  The Iterative Process of Prompt Engineering Prompt engineering is not a one-shot affair; it\u00e2\u0080\u0099s an iterative process. It involves refining the prompt based on the model\u00e2\u0080\u0099s responses and adjusting it to more precisely align with our objectives. This process is crucial when working with advanced language models like GPT-3 and GPT-4, which, while powerful, often require detailed guidance to produce specific types of outputs.  Achieving a Refined Response Our initial prompt provided a basic structure for the task but lacked detailed guidance on how to effectively rate the creativity of the misheard lyrics. To address this, we need to: Provide Clearer Instructions : Enhance the prompt with more explicit instructions on what constitutes different levels of creativity. Incorporate Examples : Include examples within the prompt that illustrate low, medium, and high creativity ratings. Clarify Expectations : Make it clear that the rating should consider not just the humor but also the originality and deviation from the original lyrics. ",
        "id": "c046bb11a9ebe12d73180f7cfc688360"
    },
    {
        "text": " Our Improved Prompt In the next cell, you\u00e2\u0080\u0099ll see an improved prompt that is designed to elicit more nuanced and varied responses from the model, providing a clearer framework for evaluating the creativity of misheard lyrics. By refining our approach through prompt engineering, we aim to achieve more accurate and diverse ratings that align better with our intended goal for the Lyrics Corrector. [6]: # Define our prompt improved_lyrics_prompt = ( \"Here's a misheard lyric: {lyric} . What's the actual lyric, which song does it come from, which artist performed it, and can \" \"you give a funny explanation as to why the misheard version doesn't make sense? Additionally, please provide an objective rating to the \" \"misheard lyric on a scale of 1 to 3, where 1 is 'not particularly creative' (minimal humor, closely resembles the \" \"original lyrics and the intent of the song) and 3 is 'hilariously creative' (highly original, very humorous, significantly different from \" \"the original). Explain your rating briefly. For example, 'I left my heart in San Francisco' misheard as 'I left my hat in San Francisco' \" \"might be a 1, as it\u00e2\u0080\u0099s a simple word swap with minimal humor. Conversely, 'I want to hold your hand' misheard as 'I want to steal your land' \" \"could be a 3, as it significantly changes the meaning in a humorous and unexpected way.\" ) [7]: # Create a new experiment for the Improved Version (or reuse the existing one if we've run this cell more than once) mlflow . set_experiment ( \"Improved Lyrics Corrector\" ) # Start our run and log our model with mlflow . start_run (): model_info = mlflow . openai . log_model ( model = \"gpt-4o-mini\" , task = openai . completions , artifact_path = \"model\" , prompt = improved_lyrics_prompt , signature = ModelSignature ( inputs = Schema ([ ColSpec ( type = \"string\" , name = None )]), outputs = Schema ([ ColSpec ( type = \"string\" , name = None )]), params = ParamSchema ( [ ParamSpec ( name = \"max_tokens\" , default = 16 , dtype = \"long\" ), ParamSpec ( name = \"temperature\" , default = 0 , dtype = \"float\" ), ParamSpec ( name = \"best_of\" , default = 1 , dtype = \"long\" ), ] ), ), ) # Load the model as a generic python function that can be used for completions improved_model = mlflow . pyfunc . load_model ( model_info . model_uri ) [8]: # Submit our faulty lyrics to the model fix_my_lyrics_improved = improved_model . predict ( bad_lyrics , params = { \"max_tokens\" : 500 , \"temperature\" : 0.1 } ) # See what the response is formatted_output = \"<br>\" . join ( [ f \"<p><strong> { line . strip () } </strong></p>\" for line in fix_my_lyrics_improved ] ) display ( HTML ( formatted_output )) The actual lyric is \"We built this city on rock and roll\" from the song \"We Built This City\" by Starship. The misheard version is a 3 on the scale, as it completely changes the meaning of the song and adds a humorous twist. The misheard version doesn't make sense because it replaces the iconic rock and roll genre with a food item, sausage rolls. This could be interpreted as a commentary on the current state of the music industry, where popular songs are often criticized for being shallow and lacking substance. The misheard version could also be seen as a nod to the British culture, where sausage rolls are a popular snack. Overall, the misheard lyric adds a playful and unexpected element to the song. The actual lyric is \"Hold me closer, tiny dancer\" from the song \"Tiny Dancer\" by Elton John. The misheard version is a common one, with many people thinking the lyric is about the actor Tony Danza instead of a dancer. This misheard version would be a 2 on the scale, as it is a humorous and unexpected interpretation of the original lyrics, but still closely resembles the original and the intent of the song. The misheard version doesn't make sense because Tony Danza is not known for his dancing skills, so it would be odd for someone to want to be held closer to him specifically for his dancing abilities. It also changes the meaning of the song, as the original lyrics are about a small and delicate dancer, while the misheard version is about a well-known actor. The actual lyric is \"Sweet dreams are made of this. W",
        "id": "1b63c0e52c587fe3847d6a5602de349b"
    },
    {
        "text": " resembles the original and the intent of the song. The misheard version doesn't make sense because Tony Danza is not known for his dancing skills, so it would be odd for someone to want to be held closer to him specifically for his dancing abilities. It also changes the meaning of the song, as the original lyrics are about a small and delicate dancer, while the misheard version is about a well-known actor. The actual lyric is \"Sweet dreams are made of this. Who am I to disagree? I travel the world and the seven seas. Everybody's looking for something.\" It comes from the song \"Sweet Dreams (Are Made of This)\" by the Eurythmics.\n\nThe misheard version is a play on words, replacing \"this\" with \"cheese\" and using different types of cheese in place of \"seven seas.\" It doesn't make sense because cheese is not typically associated with dreams or traveling the world. It also changes the meaning of the song from a philosophical exploration of desires and purpose to a silly ode to cheese.\n\nI would rate this misheard lyric a 2. It is fairly creative and humorous, but it still closely resembles the original lyrics and doesn't deviate too far from the intent of the song. The actual lyric is \"Excuse me while I kiss the sky\" from the song \"Purple Haze\" by Jimi Hendrix. The misheard version is a common one, with many people thinking Hendrix was singing about kissing a guy instead of the sky.\n\nFunny explanation: Maybe the misheard version came about because Hendrix was known for his wild and unpredictable performances, so people thought he might just randomly kiss a guy on stage. Or maybe they thought he was singing about a romantic moment with a male lover in the sky.\n\nObjective rating: 2. While the misheard version is a common one and does have some humor to it, it's not particularly original or unexpected. It's a simple word swap that still somewhat makes sense in the context of the song. The actual lyric is \"I want to rock and roll all night and party every day\" from the song \"Rock and Roll All Nite\" by Kiss.\n\nThe misheard lyric, \"I want to rock and roll all night and part of every day,\" doesn't make sense because it implies that the person only wants to rock and roll for a portion of each day, rather than all night and every day. It also changes the meaning of the lyric from wanting to party all day and night to only wanting to party for part of the day.\n\nI would rate this misheard lyric a 2. While it does have some humor and changes the meaning of the original lyric, it is still quite similar to the original and doesn't completely change the intent of the song. The actual lyric is \"There's a bad moon on the rise\" from the song \"Bad Moon Rising\" by Creedence Clearwater Revival. The misheard lyric is a common one, with many people hearing \"There's a bathroom on the right\" instead of the correct lyrics. This misheard version doesn't make sense because it changes the tone and meaning of the song from a warning about a dangerous situation to a mundane reminder about bathroom locations.\n\nObjective rating: 2. While the misheard lyric is not particularly creative, it does add a humorous twist to the song and is a common misinterpretation. The actual lyric is \"I think you'll understand, when I say that somethin', I want to hold your hand\" from the song \"I Want to Hold Your Hand\" by The Beatles.\n\nThe misheard lyric is a 3 on the scale. The original lyric is a sweet and innocent expression of love, while the misheard version turns it into a bizarre and aggressive desire to take someone's land. It's a complete departure from the original meaning and adds a humorous twist to the song. It also plays on the idea of misheard lyrics often being nonsensical and out of context. ",
        "id": "521e39a5443b61795a80acbe79aca73b"
    },
    {
        "text": " Conclusion: The Power of MLflow in Managing AI Model Experiments As we wrap up our tutorial, let\u00e2\u0080\u0099s revisit the significant insights we\u00e2\u0080\u0099ve gained, particularly focusing on how MLflow enhances the experimentation and deployment of OpenAI\u00e2\u0080\u0099s advanced language models.  Key Takeaways Prompt Engineering and Experimentation : This tutorial highlighted the iterative nature of prompt engineering, showcasing how subtle changes in the prompt can lead to significantly different outcomes from an AI model. MLflow plays a pivotal role here, allowing us to track these variations effectively, compare results, and iterate towards the optimal prompt configuration. Simplifying AI Model Management with MLflow : MLflow\u00e2\u0080\u0099s capacity to log models, manage experiments, and handle the nuances of the machine learning lifecycle has been indispensable. It simplifies the complex process of managing and deploying AI models, making these tasks more accessible to both developers and data scientists. Leveraging OpenAI\u00e2\u0080\u0099s Advanced Models : The seamless integration of OpenAI\u00e2\u0080\u0099s GPT models within MLflow demonstrates how state-of-the-art AI technology can be applied in real-world scenarios. Our Lyrics Corrector example, built on the GPT-3.5-turbo model, illustrates just one of many potential applications that blend creativity, humor, and advanced language understanding. Advantages of MLflow\u00e2\u0080\u0099s pyfunc Implementation : The pyfunc implementation in MLflow allows for flexible and straightforward access to advanced models like OpenAI\u00e2\u0080\u0099s GPT. It enables users to deploy these models as generic Python functions, greatly enhancing their usability and integration into diverse applications.  Forward-Looking The integration of MLflow with OpenAI\u00e2\u0080\u0099s GPT models opens up a world of possibilities for innovative applications. As AI technology evolves, the versatility and robustness of MLflow will be key in translating these advancements into practical and impactful solutions.  Encouragement for Further Exploration We invite you to continue exploring the vast potential of combining MLflow\u00e2\u0080\u0099s powerful model management capabilities with the advanced linguistic prowess of OpenAI\u00e2\u0080\u0099s models. Whether you\u00e2\u0080\u0099re enhancing communication, automating tasks, or creating new AI-driven services, this combination offers a rich platform for your creativity and technical expertise. Thank you for joining us in this exploration of AI model experimentation and management. We are excited to see how you utilize these powerful tools in your upcoming projects and innovations! To continue learning about the capabilities of MLflow and OpenAI as they work together, we encourage you to continue your learning with a more advanced example, the Custom Python Model example for the MLflow OpenAI flavor . Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "73f22b4eea3aeeba1f34a4f5f06f8dce"
    },
    {
        "text": "Introduction: Advancing Communication with GPT-4 and MLflow 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor MLflow OpenAI Flavor Introduction Autologging Support for the OpenAI integration Tracing with the OpenAI flavor What makes this Integration so Special? Features Getting Started with the MLflow OpenAI Flavor - Tutorials and Guides Detailed Documentation MLflow Sentence-Transformers Flavor MLflow LangChain Flavor MLflow LlamaIndex Flavor MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow OpenAI Flavor Introduction: Advancing Communication with GPT-4 and MLflow  Introduction: Advancing Communication with GPT-4 and MLflow Welcome to our advanced tutorial, where we delve into the cutting-edge capabilities of OpenAI\u00e2\u0080\u0099s GPT-4, particularly exploring its Chat Completions feature. In this session, we will combine the advanced linguistic prowess of GPT-4 with the robust experiment tracking and deployment framework of MLflow to create an innovative application: The Text Message Angel. Download this Notebook  Tutorial Overview In this tutorial, we will: Set Up and Validate Environment : Ensure that all necessary configurations, including the OPENAI_API_KEY , are in place for our experiments. Initialize MLflow Experiment : Set up an MLflow experiment named \u00e2\u0080\u009cText Message Angel\u00e2\u0080\u009d to track and manage our model\u00e2\u0080\u0099s performance and outcomes. Implement Chat Completions with GPT-4 : Utilize the Chat Completions task of GPT-4 to develop an application that can analyze and respond to text messages. This feature of GPT-4 allows for context-aware, conversational AI applications that can understand and generate human-like text responses. Model Deployment and Prediction : Deploy our model using MLflow\u00e2\u0080\u0099s pyfunc implementation and make predictions on a set of sample text messages. This will demonstrate the practical application of our model in real-world scenarios.  The Text Message Angel Application Our application, the Text Message Angel, aims to enhance everyday text communication. It will analyze SMS responses for tone, appropriateness, and relationship impact. The model will categorize responses as either appropriate (\u00e2\u0080\u009cGood to Go!\u00e2\u0080\u009d) or suggest caution (\u00e2\u0080\u009cYou might want to read that again before pressing send\u00e2\u0080\u009d). For responses deemed inappropriate, it will also suggest alternative phrasing that maintains a friendly yet witty tone.  Why GPT-4 and MLflow? GPT-4\u00e2\u0080\u0099s Advanced AI : GPT-4 represents the latest in AI language model development, offering nuanced understanding and response generation capabilities that are ideal for a text-based application like the Text Message Angel. MLflow\u00e2\u0080\u0099s Seamless Management : MLflow simplifies the process of tracking experiments, managing different model versions, and deploying AI models. Its integration with GPT-4 allows us to focus on the creative aspect of our application while efficiently handling the technicalities of model management. ",
        "id": "e970702ac1356b60f1f01a886c23574c"
    },
    {
        "text": " Engaging with the Tutorial As we progress, we encourage you to actively engage with the code and concepts presented. This tutorial is not just about learning the functionalities but also understanding the potential of these technologies when combined creatively. Let\u00e2\u0080\u0099s embark on this journey to harness the synergy of MLflow and GPT-4\u00e2\u0080\u0099s Chat Completions to enhance communication and interactions in our digital world. [1]: import warnings # Disable a few less-than-useful UserWarnings from setuptools and pydantic warnings . filterwarnings ( \"ignore\" , category = UserWarning ) [2]: import os import openai import pandas as pd from IPython.display import HTML import mlflow from mlflow.models.signature import ModelSignature from mlflow.types.schema import ColSpec , ParamSchema , ParamSpec , Schema # Run a quick validation that we have an entry for the OPEN_API_KEY within environment variables assert \"OPENAI_API_KEY\" in os . environ , \"OPENAI_API_KEY environment variable must be set\"  Implementing the Text Message Angel with GPT-4 and Chat Completions After exploring the humorous world of misheard lyrics in the introductory tutorial to the openai flavor, we now shift our focus to a more sophisticated application involving GPT-4 and the Chat Completions feature. This tutorial introduces the \u00e2\u0080\u009cText Message Angel\u00e2\u0080\u009d, an innovative application designed to pre-screen text messages, ensuring they are appropriate and relationship-friendly, especially for those inclined towards sarcasm.  Setting Up the Text Message Angel Experiment We begin by setting up a new MLflow experiment titled \u00e2\u0080\u009cText Message Angel\u00e2\u0080\u009d. This experiment aims to create a service that analyzes text messages and provides guidance on their appropriateness before sending. The goal is to maintain positive communication while allowing for a playful tone.  The Role of GPT-4 and Chat Completions GPT-4 represents a massive leap in capability compared with the previous model we used in the introductory tutorial. It brings enhanced understanding and contextual awareness, making it ideal for interpreting and responding to natural language with a high degree of accuracy and nuance. The Chat Completions feature, specifically, enables a more conversational approach, which is perfect for our text message evaluation scenario.  Crafting the Prompt for Text Message Evaluation The core of our application is a well-crafted prompt that directs GPT-4 to evaluate text messages based on specific criteria: Content Analysis : The model determines if a message contains inappropriate elements like humorless sarcasm, passive-aggressive tones, or anything that could harm a relationship. Response Categorization : Based on its analysis, the model categorizes the message as either \u00e2\u0080\u009cGood to Go!\u00e2\u0080\u009d or advises to \u00e2\u0080\u009cread that again before pressing send.\u00e2\u0080\u009d Suggested Corrections : If a message is deemed inappropriate, the model goes a step further to suggest an alternative version. This corrected message aims to preserve a fun and slightly snarky tone while ensuring it does not harm the relationship. This setup not only demonstrates the advanced capabilities of GPT-4 in understanding and generating human-like text but also highlights its potential for practical applications in everyday communication scenarios. [3]: mlflow . set_experiment ( \"Text Message Angel\" ) messages = [ { \"role\" : \"user\" , \"content\" : ( \"Determine if this is an acceptable response to a friend through SMS. \" \"If the response contains humorless sarcasm, a passive aggressive tone, or could potentially \" \"damage my relationship with them, please respond with 'You might want to read that again before \" \"pressing send.', otherwise respond with 'Good to Go!'. If the response classifies as inappropriate, \" \"please suggest a corrected version following the classification that will help to keep my \" \"relationship with this person intact, yet still maintains a fun and somewhat snarky tone: {text} \" ), } ] ",
        "id": "93c2162247bf696f0378b8d14e6cea1d"
    },
    {
        "text": " Integrating GPT-4 with MLflow for the Text Message Angel In this crucial step, we\u00e2\u0080\u0099re integrating the advanced GPT-4 model with MLflow for our Text Message Angel application. This process involves setting up the model within an MLflow run, logging its configuration, and preparing it for practical use.  Starting the MLflow Run We initiate an MLflow run, a crucial step in tracking our model\u00e2\u0080\u0099s performance, parameters, and outputs. This run encapsulates all the details and metrics related to the GPT-4 model we are using.  Logging the GPT-4 Model in MLflow Within this run, we log our GPT-4 model using mlflow.openai.log_model . This function call is instrumental in registering our model\u00e2\u0080\u0099s specifics in MLflow\u00e2\u0080\u0099s tracking system. Here\u00e2\u0080\u0099s a breakdown of the parameters we\u00e2\u0080\u0099re logging: Model Selection : We specify gpt-4 , indicating we are utilizing a far more advanced version of OpenAI\u00e2\u0080\u0099s models than the previous example. Task Specification : The openai.chat.completions task is chosen, aligning with our objective of creating a conversational AI capable of analyzing and responding to text messages. Artifact Path : We define an artifact path where MLflow will store the model-related data. Messages : The messages variable, containing our pre-defined prompt and criteria for evaluating text messages, is passed to the model. Model Signature : The signature defines the input-output schema and parameters for our model, such as max_tokens and temperature . These settings are crucial in controlling how the model generates responses. [4]: with mlflow . start_run (): model_info = mlflow . openai . log_model ( model = \"gpt-4\" , task = openai . chat . completions , artifact_path = \"model\" , messages = messages , signature = ModelSignature ( inputs = Schema ([ ColSpec ( type = \"string\" , name = None )]), outputs = Schema ([ ColSpec ( type = \"string\" , name = None )]), params = ParamSchema ( [ ParamSpec ( name = \"max_tokens\" , default = 16 , dtype = \"long\" ), ParamSpec ( name = \"temperature\" , default = 0 , dtype = \"float\" ), ] ), ), )  Loading the Model for Use After logging the model in MLflow, we load it as a generic Python function using mlflow.pyfunc.load_model . This step is vital as it transforms our GPT-4 model into a format that\u00e2\u0080\u0099s easily callable and usable within our application. [5]: model = mlflow . pyfunc . load_model ( model_info . model_uri )  Testing the Text Message Angel With our Text Message Angel application powered by GPT-4 and integrated within MLflow, we are now ready to put it to the test. This section involves creating a set of sample text messages, some potentially containing sarcasm or passive-aggressive tones, and others being more straightforward and friendly.  Creating Validation Data We start by creating a DataFrame named validation_data with a variety of text messages. These messages are designed to test the model\u00e2\u0080\u0099s ability to discern tone and suggest corrections where necessary: A message using humor to mask a critique of a dinner experience. A sarcastic comment expressing reluctance to go to the movies. A straightforward message expressing excitement for a road trip. A simple thank-you message. A sarcastic remark about enjoying someone\u00e2\u0080\u0099s singing.  Submitting Messages to the Model Next, we submit these messages to our Text Message Angel model for evaluation. The model will analyze each message, determining whether it\u00e2\u0080\u0099s appropriate or needs a revision. For messages that might strain a relationship, the model will suggest a more suitable version.  Displaying the Model\u00e2\u0080\u0099s Responses The responses from the model are then formatted for clear and attractive display. This step is crucial for assessing the model\u00e2\u0080\u0099s performance in real-time and understanding how its corrections and suggestions align with the intended tone of the messages. ",
        "id": "f8cec2b763241d8a8a5af5485d992faa"
    },
    {
        "text": " Model\u00e2\u0080\u0099s Output Let\u00e2\u0080\u0099s take a look at how the Text Message Angel responded: Suggested a more tactful way to comment on the dinner. Offered a humorous yet softer alternative for declining a movie invitation. Confirmed that the road trip message is appropriate. Validated the thank-you message as suitable. Suggested a playful yet kinder remark about singing. These responses showcase the model\u00e2\u0080\u0099s nuanced understanding of social communication, its ability to maintain a friendly yet fun tone, and its potential in assisting users to communicate more effectively and harmoniously. [6]: validation_data = pd . DataFrame ( { \"text\" : [ \"Wow, what an interesting dinner last night! I had no idea that you could use canned \" \"cat food to make a meatloaf.\" , \"I'd rather book a 14th century surgical operation than go to the movies with you on Thursday.\" , \"Can't wait for the roadtrip this weekend! Love the playlist mixes that you choose!\" , \"Thanks for helping out with the move this weekend. I really appreciate it.\" , \"You know what part I love most when you sing? The end. It means its over.\" , ] } ) chat_completions_response = model . predict ( validation_data , params = { \"max_tokens\" : 50 , \"temperature\" : 0.2 } ) formatted_output = \"<br>\" . join ( [ f \"<p><strong> { line . strip () } </strong></p>\" for line in chat_completions_response ] ) display ( HTML ( formatted_output )) You might want to read that again before pressing send.\n\nSuggested response: \"Wow, dinner last night was certainly unique! Who knew meatloaf could be so... adventurous?\" You might want to read that again before pressing send.\n\nSuggested correction: \"I'd rather watch a 14th century surgical operation documentary than miss out on the movies with you on Thursday. How's that for a plot twist?\" Good to Go! Good to Go! You might want to read that again before pressing send.\n\nSuggested response: \"You know what part I love most when you sing? The encore. It means I get to hear you again!\"  Conclusion: Advancing AI Interactions with MLflow and OpenAI\u00e2\u0080\u0099s GPT-4 As we reach the end of this tutorial, it\u00e2\u0080\u0099s time to reflect on the insights we\u00e2\u0080\u0099ve gained, especially the remarkable capabilities of GPT-4 in the realm of conversational AI, and how MLflow facilitates the deployment and management of these advanced models.  Key Takeaways Deep Dive into ChatCompletions with GPT-4 : This tutorial gave us a hands-on experience with GPT-4\u00e2\u0080\u0099s ChatCompletions feature, demonstrating its ability to understand context, maintain conversation flow, and generate human-like responses. The Text Message Angel application exemplified how such a model can be used to improve and refine everyday communication. MLflow\u00e2\u0080\u0099s Role in Managing Advanced AI : MLflow has shown its strength not just in handling model logistics, but also in simplifying the experimentation with complex AI models like GPT-4. Its robust tracking and logging capabilities make it easier to manage and iterate over conversational AI models. Real-World Application and Potential : The Text Message Angel illustrated a practical application of GPT-4\u00e2\u0080\u0099s advanced capabilities, demonstrating how AI can be leveraged to enhance and safeguard interpersonal communication. It\u00e2\u0080\u0099s a glimpse into how conversational AI can be used in customer service, mental health, education, and other domains. The Evolution of AI and MLflow\u00e2\u0080\u0099s Adaptability : The tutorial highlighted how MLflow\u00e2\u0080\u0099s flexible framework is well-suited to keep pace with the rapid advancements in AI, particularly in areas like natural language processing and conversational AI.  Moving Forward with Conversational AI The combination of MLflow and OpenAI\u00e2\u0080\u0099s GPT-4 opens up exciting avenues for developing more intuitive and responsive AI-driven applications. As we continue to witness advancements in AI, MLflow\u00e2\u0080\u0099s ability to adapt and manage these complex models becomes increasingly vital. ",
        "id": "37146658a37634a370f859aebbe3f61c"
    },
    {
        "text": " Embarking on Your AI Journey We encourage you to build upon the foundations laid in this tutorial to explore the vast potential of conversational AI. With MLflow and OpenAI\u00e2\u0080\u0099s GPT-4, you are well-equipped to create innovative applications that can converse, understand, and interact in more human-like ways. Thank you for joining us in exploring the cutting-edge of conversational AI and model management. Your journey into developing AI-enhanced communication tools is just beginning, and we are excited to see where your creativity and skills will lead you next! To continue your learning journey, see the additional advanced tutorials for MLflow\u00e2\u0080\u0099s OpenAI flavor . Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "158396ea8c1846fa7e7f97cd853b34d2"
    },
    {
        "text": "Building a Code Assistant with OpenAI & MLflow 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor MLflow OpenAI Flavor Introduction Autologging Support for the OpenAI integration Tracing with the OpenAI flavor What makes this Integration so Special? Features Getting Started with the MLflow OpenAI Flavor - Tutorials and Guides Detailed Documentation MLflow Sentence-Transformers Flavor MLflow LangChain Flavor MLflow LlamaIndex Flavor MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow OpenAI Flavor Building a Code Assistant with OpenAI & MLflow  Building a Code Assistant with OpenAI & MLflow  Overview Welcome to this comprehensive tutorial, where you\u00e2\u0080\u0099ll embark on a fascinating journey through the integration of OpenAI\u00e2\u0080\u0099s powerful language models with MLflow, where we\u00e2\u0080\u0099ll be building an actually useful tool that can, with the simple addition of a decorator to any function that we declare, get immediate feedback within an interactive environment on code under active development. Download this Notebook  Learning Objectives By the end of this tutorial, you will: Master OpenAI\u00e2\u0080\u0099s GPT-4 for Code Assistance : Understand how to leverage OpenAI\u00e2\u0080\u0099s GPT-4 model for providing real-time coding assistance. Learn to harness its capabilities for generating code suggestions, explanations, and improving overall coding efficiency. Utilize MLflow for Enhanced Model Tracking : Delve into MLflow\u00e2\u0080\u0099s powerful tracking systems to manage machine learning experiments. Learn how to adapt a pyfunc model from within MLflow to control how the output of an LLM is displayed from within an interactive coding environment. Seamlessly Combine OpenAI and MLflow : Discover the practical steps to integrate OpenAI\u00e2\u0080\u0099s AI capabilities with MLflow\u00e2\u0080\u0099s tracking and management systems. This integration exemplifies how combining these tools can streamline the development and deployment of intelligent applications. Develop and Deploy a Custom Python Code Assistant : Gain hands-on experience in creating a Python-based code assistant using OpenAI\u00e2\u0080\u0099s model. Then, actually see it in action as it is used within a Jupyter Notebook environment to give helpful assistance during development. Improve Code Quality with AI-driven Insights : Apply AI-powered analysis to review and enhance your code. Learn how an AI assistant can provide real-time feedback on code quality, suggest improvements, and help maintain high coding standards. Explore Advanced Python Features for Robust Development : Understand advanced Python features like decorators and functional programming. These are crucial for building efficient, scalable, and maintainable software solutions, especially when integrating AI capabilities.  Key Concepts Covered MLflow\u00e2\u0080\u0099s Model Management : Explore MLflow\u00e2\u0080\u0099s features for tracking experiments, packaging code into reproducible runs, and managing and deploying models. Custom Python Model : Learn how to use MLflow\u00e2\u0080\u0099s built-in customization for defining a generic Python function that will allow you to craft your own processing logic while interfacing with OpenAI to perform alternative handling to the LLM\u00e2\u0080\u0099s output. Python Decorators and Functional Programming : Learn about advanced Python concepts like decorators and functional programming for efficient code evaluation and enhancement. ",
        "id": "d18b079115e830e487e0eaac6c260b9d"
    },
    {
        "text": " Why Use MLflow for this? MLflow emerges as a pivotal element in this tutorial, making our use case not only feasible but also highly efficient. It offers a secure and seamless interface with OpenAI\u00e2\u0080\u0099s advanced language models. In this tutorial, we\u00e2\u0080\u0099ll explore how MLflow greatly simplifies the process of storing specific instructional prompts for OpenAI, and enhances the user experience by adding readable formatting to the returned text. The flexibility and scalability of MLflow make it a robust choice for integrating with various tools, particularly in interactive coding environments like Jupyter Notebooks. We\u00e2\u0080\u0099ll witness firsthand how MLflow facilitates rapid experimentation and iteration, allowing us to create a functional tool with minimal effort. This tool will not just assist in development but will also elevate the overall coding and model management experience. By leveraging MLflow\u00e2\u0080\u0099s comprehensive features, we\u00e2\u0080\u0099ll navigate\nthrough a seamless end-to-end workflow, from setting up intricate models to executing complex tasks efficiently.  Important Cost Considerations for GPT-4 Usage  High(er) Cost of GPT-4 It\u00e2\u0080\u0099s crucial to note that using GPT-4, as opposed to GPT-4o-mini, can incur higher costs . GPT-4\u00e2\u0080\u0099s advanced capabilities and enhanced performance come with a price premium, making it a more expensive option compared to earlier models like GPT-3.5.  Why Choose GPT-4 in This Tutorial Enhanced Capabilities : We opt for GPT-4 in this tutorial primarily due to its superior capabilities, especially in areas such as code refactoring and detecting issues in code implementations. Demonstration Purposes : The use of GPT-4 here serves as a demonstration to showcase the cutting-edge advancements in language model technology and its applications in complex tasks.  Consider Alternatives for Cost-Effectiveness For projects where cost is a significant concern, or where the advanced features of GPT-4 are not essential, consider using GPT-4o-mini or other more cost-effective alternatives . These models still offer robust performance for a wide range of applications but at a lower cost.  Budgeting for GPT-4 If you choose to proceed with GPT-4, it is recommended to: - Monitor Usage Closely : Keep track of your API usage to manage costs effectively. - Budget Accordingly : Allocate sufficient resources to cover the higher costs associated with GPT-4. By being mindful of these cost considerations, you can make informed decisions about which OpenAI model best suits your project\u00e2\u0080\u0099s needs and budget. [1]: import warnings # Disable a few less-than-useful UserWarnings from setuptools and pydantic warnings . filterwarnings ( \"ignore\" , category = UserWarning ) [2]: import functools import inspect import os import textwrap import openai import mlflow from mlflow.models.signature import ModelSignature from mlflow.pyfunc import PythonModel from mlflow.types.schema import ColSpec , ParamSchema , ParamSpec , Schema # Run a quick validation that we have an entry for the OPEN_API_KEY within environment variables assert \"OPENAI_API_KEY\" in os . environ , \"OPENAI_API_KEY environment variable must be set\"  Initializing the MLflow Client Depending on where you are running this notebook, your configuration may vary for how you initialize the MLflow Client. If you are uncertain about how to configure and use an MLflow Tracking server or what options are available (The easiest is to use the free managed service within Databricks Community Edition ), you can see the guide to running notebooks here for\nmore information on setting the tracking server uri and configuring access to either managed or self-managed MLflow tracking servers. ",
        "id": "34b876ed05ec4412ead8448cac8ae600"
    },
    {
        "text": " Setting the MLflow Experiment In this section of the tutorial, we use MLflow\u00e2\u0080\u0099s set_experiment function to define an experiment named \u00e2\u0080\u009cCode Helper\u00e2\u0080\u009d. This step is essential in MLflow\u00e2\u0080\u0099s workflow for several reasons: Unique Identification : A unique and distinct experiment name like \u00e2\u0080\u009cCode Helper\u00e2\u0080\u009d is crucial for easy identification and segregation of the runs pertaining to this specific project, especially when working on multiple projects or experiments simultaneously. Simplified Tracking : Naming the experiment enables effortless tracking of all the runs and models associated with it, maintaining a clear history of model development, parameters, metrics, and results. Ease of Access in MLflow UI : A distinct experiment name ensures quick location and access to our experiment\u00e2\u0080\u0099s runs and models within the MLflow UI, facilitating analysis, comparison of different runs, and sharing findings. Facilitates Better Organization : As projects grow in complexity, having a well-named experiment aids in better organization and management of the machine learning lifecycle, making it easier to navigate through different stages of the experiment. The use of a unique experiment name like \u00e2\u0080\u009cCode Helper\u00e2\u0080\u009d lays the foundation for efficient model management and tracking, a critical aspect of any machine learning workflow, especially in dynamic and collaborative environments. [3]: mlflow . set_experiment ( \"Code Helper\" ) [3]: <Experiment: artifact_location='file:///Users/benjamin.wilson/repos/mlflow-fork/mlflow/docs/source/llms/openai/notebooks/mlruns/703316263508654123', creation_time=1701891935339, experiment_id='703316263508654123', last_update_time=1701891935339, lifecycle_stage='active', name='Code Helper', tags={}>  Defining the Instruction Set for the AI Model In this part of the tutorial, we define a specific set of instructions to guide the behavior of our AI model. This is achieved through the instruction array, which outlines the roles and expected interactions between the system (AI model) and the user. Here\u00e2\u0080\u0099s a breakdown of its components: System Role : The first element of the array defines the role of the AI model as a \u00e2\u0080\u0098system\u00e2\u0080\u0099. It describes the model as a \u00e2\u0080\u0098helpful expert Software Engineer\u00e2\u0080\u0099 whose purpose is to assist in code analysis and provide educational support. The AI model is expected to: Offer clear explanations of the code\u00e2\u0080\u0099s intent. Assess the code\u00e2\u0080\u0099s correctness and readability. Suggest improvements while focusing on simplicity, maintainability, and adherence to best coding practices. User Role : The second element represents the \u00e2\u0080\u0098user\u00e2\u0080\u0099 role. This part is where the user (in this case, the person learning from the tutorial) interacts with the AI model by submitting code for review. The user is expected to: Provide code snippets for evaluation. Seek feedback and suggestions for code improvement from the AI model. This instruction set is crucial for creating an interactive learning experience. It guides the AI model in providing targeted, constructive feedback, making it an invaluable tool for understanding coding practices and enhancing coding skills. [4]: instruction = [ { \"role\" : \"system\" , \"content\" : ( \"As an AI specializing in code review, your task is to analyze and critique the submitted code. For each code snippet, provide a detailed review that includes: \" \"1. Identification of any errors or bugs. \" \"2. Suggestions for optimizing code efficiency and structure. \" \"3. Recommendations for enhancing code readability and maintainability. \" \"4. Best practice advice relevant to the code\u00e2\u0080\u0099s language and functionality. \" \"Your feedback should help the user improve their coding skills and understand best practices in software development.\" ), }, { \"role\" : \"user\" , \"content\" : \"Review my code and suggest improvements: {code} \" }, ] ",
        "id": "132dfdecb7fb616f46ab8ae84397f5bf"
    },
    {
        "text": " Defining and Utilizing the Model Signature in MLflow In this part of the tutorial, we define a ModelSignature for our OpenAI model, which is a crucial step in both saving the base model and later in our custom Python Model implementation. Here\u00e2\u0080\u0099s an overview of the process: Model Signature Definition : We create a ModelSignature object that specifies the input, output, and parameters of our model. The inputs and outputs are defined as schemas with a single string column, indicating that our model will be processing string type data. The params schema includes two parameters: max_tokens and temperature , each with a default value and data type defined. Note We\u00e2\u0080\u0099re explicitly defining the model signature here for purposes of demonstration. The schema will be automatically inferred if you do not specify one and will be set based on the task that is defined when logging or saving the model. Logging the Base OpenAI Model : Using mlflow.openai.log_model , we log the base OpenAI model ( gpt-4 ) along with the instruction set we defined earlier. The signature we defined is also passed in this step, ensuring that the model is saved with the correct specifications for inputs, outputs, and parameters. This dual-purpose signature is vital as it ensures consistency in how the model processes data both in its base form and when it\u00e2\u0080\u0099s later wrapped in a custom Python Model. This approach streamlines the workflow and maintains uniformity across different stages of model implementation and deployment. [5]: # Define the model signature that will be used for both the base model and the eventual custom pyfunc implementation later. signature = ModelSignature ( inputs = Schema ([ ColSpec ( type = \"string\" , name = None )]), outputs = Schema ([ ColSpec ( type = \"string\" , name = None )]), params = ParamSchema ( [ ParamSpec ( name = \"max_tokens\" , default = 500 , dtype = \"long\" ), ParamSpec ( name = \"temperature\" , default = 0 , dtype = \"float\" ), ] ), ) # Log the base OpenAI model with the included instruction set (prompt) with mlflow . start_run (): model_info = mlflow . openai . log_model ( model = \"gpt-4\" , task = openai . chat . completions , artifact_path = \"base_model\" , messages = instruction , signature = signature , )  Our logged model in the MLflow UI After logging the model, you can open up the MLflow UI and see the components that have been logged. Notice that the configuration for our model, including the model type (gpt-4), the endpoint API type (task) is recorded (chat.completions), and the prompt have all been logged. ",
        "id": "1445e804e83d985a19216c70eb6935c4"
    },
    {
        "text": " Enhancing User Experience with Custom Pyfunc Implementation In this section, we introduce a custom Python Model, CodeHelper , which significantly improves the user experience when interacting with the OpenAI model in an interactive development environment like Jupyter Notebook. The CodeHelper class is designed to format the output from the OpenAI model, making it more readable and visually appealing, similar to a chat interface. Here\u00e2\u0080\u0099s how it works: Initialization and Model Loading : The CodeHelper class inherits from PythonModel . The load_context method is used to load the OpenAI model, which is saved as self.model . This model is loaded from the context.artifacts , ensuring that the appropriate model is used for predictions. Response Formatting : The _format_response method is crucial for enhancing the output format. It processes each item in the response, handling text and code blocks differently. Text lines outside of code blocks are wrapped to a width of 80 characters for better readability. Lines within code blocks (marked by ` ``) are not wrapped, preserving the code structure. This formatting creates an output that resembles a chat interface, making the interaction more intuitive and user-friendly. Making Predictions : The predict method is where the model\u00e2\u0080\u0099s prediction occurs. It calls the loaded OpenAI model to get the raw response for the given input. The raw response is then passed to the _format_response method for formatting. The formatted response is returned, providing a clear and easy-to-read output. By implementing this custom pyfunc , we enhance the user\u00e2\u0080\u0099s interaction with the AI code helper. It not only makes the output easier to understand but also presents it in a familiar format, akin to messaging, which is especially beneficial in interactive coding environments. [6]: # Custom pyfunc implementation that applies text and code formatting to the output results from the OpenAI model class CodeHelper ( PythonModel ): def __init__ ( self ): self . model = None def load_context ( self , context ): self . model = mlflow . pyfunc . load_model ( context . artifacts [ \"model_path\" ]) @staticmethod def _format_response ( response ): formatted_output = \"\" in_code_block = False for item in response : lines = item . split ( \" \\n \" ) for line in lines : # Check for the start/end of a code block if line . strip () . startswith ( \"```\" ): in_code_block = not in_code_block formatted_output += line + \" \\n \" continue if in_code_block : # Don't wrap lines inside code blocks formatted_output += line + \" \\n \" else : # Wrap lines outside of code blocks wrapped_lines = textwrap . fill ( line , width = 80 ) formatted_output += wrapped_lines + \" \\n \" return formatted_output def predict ( self , context , model_input , params ): # Call the loaded OpenAI model instance to get the raw response raw_response = self . model . predict ( model_input , params = params ) # Return the formatted response so that it is easier to read return self . _format_response ( raw_response ) ",
        "id": "226b0a682d1676ee7b30c6b9d9924014"
    },
    {
        "text": " Saving the Custom Python Model with MLflow This part of the tutorial demonstrates how to save the custom Python model, CodeHelper , using MLflow. The process involves specifying the model\u00e2\u0080\u0099s location and additional information to ensure it is properly stored and can be retrieved for future use. Here\u00e2\u0080\u0099s an overview: Defining Artifacts : An artifacts dictionary is created with a key \"model_path\" pointing to the location of the base OpenAI model. This step is important to link our custom model with the necessary base model files. We retrieve the location of the logged openai model from earlier by accessing the model_uri property from the return of the log_model() function. Saving the Model : The mlflow.pyfunc.save_model function is used to save the CodeHelper model. path : Specifies the location ( final_model_path ) where the model will be saved. python_model : An instance of the CodeHelper class is provided, indicating the model to be saved. input_example : An example input ( [\"x = 1\"] ) is given, which is useful for understanding the model\u00e2\u0080\u0099s expected input format. signature : The previously defined ModelSignature is passed, ensuring consistency in how the model processes data. artifacts : The artifacts dictionary is included to associate the base OpenAI model with our custom model. This step is crucial for encapsulating the entire functionality of our CodeHelper model in a format that MLflow can manage and track. It allows for easy deployment and versioning of the model, facilitating its use in various applications and environments. [7]: # Define the location of the base model that we'll be using within our custom pyfunc implementation artifacts = { \"model_path\" : model_info . model_uri } with mlflow . start_run (): helper_model = mlflow . pyfunc . log_model ( artifact_path = \"code_helper\" , python_model = CodeHelper (), input_example = [ \"x = 1\" ], signature = signature , artifacts = artifacts , )  Load our saved Custom Python Model In this next section, we load the model that we just saved so that we can use it! [8]: loaded_helper = mlflow . pyfunc . load_model ( helper_model . model_uri )  Comparing Two Approaches for Code Review with MLflow Models In this tutorial, we\u00e2\u0080\u0099ll explore two different approaches to utilizing MLflow models for reviewing and providing feedback on code. These approaches offer varying levels of complexity and integration, catering to different use cases and preferences.  Approach 1: The Simple review Function Our first approach is a straightforward review function. This method is less intrusive and does not modify the original function\u00e2\u0080\u0099s behavior. It\u00e2\u0080\u0099s ideal for scenarios where you want to manually trigger a review of the function\u00e2\u0080\u0099s code and don\u00e2\u0080\u0099t need to see the output result of the function to have context of the LLM\u00e2\u0080\u0099s analysis. How it works : The review function takes a function and an MLflow model as arguments. It then uses the model to evaluate the source code of the given function. Manual Invocation : You need to explicitly call review(my_func) to review my_func . This approach is manual and does not automatically integrate with function calls. Simplicity : This method is simpler and more direct, making it suitable for one-off evaluations or for use cases where automatic review is not required.  Approach 2: The Advanced code_inspector Decorator The second approach is an advanced decorator, code_inspector , which integrates more deeply by automatically reviewing the function and allowing the function\u00e2\u0080\u0099s evaluation to execute. This can be helpful for more complex functions where the output result, in conjunction with the evaluation from the code helper, can allow for a deeper understanding of any observed logical flaws. Automatic Evaluation : When applied as a decorator, code_inspector evaluates the function\u00e2\u0080\u0099s code automatically on each call. Error Handling : Includes robust error handling within the evaluation process. Function Modification : This method modifies the function\u00e2\u0080\u0099s behavior, incorporating an automatic review process. ",
        "id": "976f7e5f2f91c0012c8cfb5ce8471513"
    },
    {
        "text": " Introduction to the review Function We\u00e2\u0080\u0099ll start by examining the review function. This function will be defined in the next cell of our Jupyter notebook. Here\u00e2\u0080\u0099s a quick overview of what the review function does: Inputs : It takes a function and an MLflow model as inputs. Functionality : Extracts the source code of the input function and uses the MLflow model to provide feedback on it. Error Handling : Enhanced with error handling to manage exceptions gracefully. In the following Jupyter notebook cell, you\u00e2\u0080\u0099ll see the implementation of the review function, demonstrating its simplicity and effectiveness in evaluating code. After exploring the review function, we will delve into the more complex code_inspector decorator to understand its automatic evaluation process and error handling mechanisms. [9]: def review ( func , model ): \"\"\" Function to review the source code of a given function using a specified MLflow model. Args: func (function): The function to review. model (MLflow pyfunc model): The MLflow pyfunc model used for evaluation. Returns: The model's prediction or an error message. \"\"\" try : # Extracting the source code of the function source_code = inspect . getsource ( func ) # Using the model to predict/evaluate the source code prediction = model . predict ([ source_code ]) print ( prediction ) except Exception as e : # Handling any exceptions that occur and returning an error message return f \"Error during model prediction or source code inspection: { e } \"  Explanation and Review of process_data Function  Function Overview The process_data function aims to process a list by identifying unique elements and counting duplicates. However, the implementation has several inefficiencies and readability issues. ",
        "id": "8130187e8c1eef508cbe254c3982ddb6"
    },
    {
        "text": " Suggested Revised Code The output from GPT-4\u00e2\u0080\u0099s analysis provides clear and concise feedback, precisely as the prompt instructed it to. With the MLflow integration of this application, the simplicity of using the tool is evident, allowing us to get high-quality guidance during the development process with as little as a single, simple function call. [10]: def process_data ( lst ): s = 0 q = [] for i in range ( len ( lst )): a = lst [ i ] for j in range ( i + 1 , len ( lst )): b = lst [ j ] if a == b : s += 1 else : q . append ( b ) rslt = [ x for x in lst if x not in q ] k = [] for i in rslt : if i not in k : k . append ( i ) final_data = sorted ( k , reverse = True ) return final_data , s review ( process_data , loaded_helper ) Your code seems to be trying to find the count of duplicate elements in a list\nand return a sorted list of unique elements in descending order along with the\ncount of duplicates. Here are some suggestions to improve your code:\n\n1. **Errors or Bugs**: There are no syntax errors in your code, but the logic is\nflawed. The variable `s` is supposed to count the number of duplicate elements,\nbut it only counts the number of times an element is equal to another element in\nthe list, which is not the same thing. Also, the way you're trying to get unique\nelements is inefficient and can lead to incorrect results.\n\n2. **Optimizing Code Efficiency and Structure**: You can use Python's built-in\n`set` and `list` data structures to simplify your code and make it more\nefficient. A `set` in Python is an unordered collection of unique elements. You\ncan convert your list to a set to remove duplicates, and then convert it back to\na list. The length of the original list minus the length of the list with\nduplicates removed will give you the number of duplicate elements.\n\n3. **Enhancing Code Readability and Maintainability**: Use meaningful variable\nnames to make your code easier to understand. Also, add comments to explain what\neach part of your code does.\n\n4. **Best Practice Advice**: It's a good practice to write a docstring at the\nbeginning of your function to explain what it does.\n\nHere's a revised version of your code incorporating these suggestions:\n\n```python\ndef process_data(lst):\n    \"\"\"\n    This function takes a list as input, removes duplicate elements, sorts the remaining elements in descending order,\n    and counts the number of duplicate elements in the original list.\n    It returns a tuple containing the sorted list of unique elements and the count of duplicate elements.\n    \"\"\"\n    # Convert the list to a set to remove duplicates, then convert it back to a list\n    unique_elements = list(set(lst))\n\n    # Sort the list of unique elements in descending order\n    sorted_unique_elements = sorted(unique_elements, reverse=True)\n\n    # Count the number of duplicate elements\n    duplicate_count = len(lst) - len(unique_elements)\n\n    return sorted_unique_elements, duplicate_count\n```\nThis version of the code is simpler, more efficient, and easier to understand.\nIt also correctly counts the number of duplicate elements in the list. ",
        "id": "3e713a93f525f7d60dd478b4afdb4031"
    },
    {
        "text": " The code_inspector Decorator Function The code_inspector function is a Python decorator designed to augment functions with automatic code review capabilities using an MLflow pyfunc model. This decorator enhances the functionality of functions, allowing them to be automatically reviewed for code quality and correctness using an MLflow pyfunc model, thereby enriching the development and learning experience. As compared to the above implementation for the review() function, this approach will allow the function to be executed\nwhen called, enhancing the contextual information when paired with the automated code review. [11]: import functools import inspect def code_inspector ( model ): \"\"\" Decorator for automatic code review using an MLflow pyfunc model. Args: model: The MLflow pyfunc model for code evaluation. \"\"\" def decorator_check_my_function ( func ): # Decorator that wraps around the given function @functools . wraps ( func ) def wrapper ( * args , ** kwargs ): try : # Extracting the source code of the decorated function parsed_func = inspect . getsource ( func ) # Using the MLflow model to evaluate the extracted source code response = model . predict ([ parsed_func ]) # Printing the response for code review feedback print ( response ) except Exception as e : # Handling exceptions during model prediction or source code extraction print ( \"Error during model prediction or formatting:\" , e ) # Executing and returning the original function's output return func ( * args , ** kwargs ) return wrapper return decorator_check_my_function  First Usage Trial: The summing_function with code_inspector We apply the code_inspector decorator to a function named summing_function . This function is designed to calculate the sum of sums for a given range. Here\u00e2\u0080\u0099s an insight into its functionality and the enhancement brought by code_inspector : Function Overview : summing_function calculates the cumulative sum of numbers up to n . It does so by iterating over a range and summing the intermediate sums at each step. A dictionary, intermediate_sums , is used to store these sums, which are then aggregated to find the final sum. Using ``code_inspector`` : The function is decorated with code_inspector(loaded_helper) . This means that each time summing_function is called, the MLflow model loaded as loaded_helper analyzes its code. The decorator provides real-time feedback on the code, assessing aspects like quality, efficiency, and best practices. Educational Benefit : This setup is ideal for learning, allowing users to receive instant, actionable feedback on their code. It offers a practical way to understand the logic behind the function and learn coding optimizations and improvements. By integrating code_inspector with summing_function , the tutorial demonstrates an interactive approach to enhancing coding skills, with immediate feedback aiding in understanding and improvement. Before proceeding to see the response from GPT-4, can you identify all of the issues in this code (there are more than a few)? [12]: @code_inspector ( loaded_helper ) def summing_function ( n ): sum_result = 0 intermediate_sums = {} for i in range ( 1 , n + 1 ): intermediate_sums [ str ( i )] = sum ( x for x in range ( 1 , i + 1 )) for key in intermediate_sums : if key == str ( i ): sum_result = intermediate_sums [ key ] # noqa: F841 final_sum = sum ([ intermediate_sums [ key ] for key in intermediate_sums if int ( key ) == n ]) return int ( str ( final_sum )) ",
        "id": "cdaf67ebffa1e4e86f66013505707415"
    },
    {
        "text": " Execution and Analysis of summing_function(1000) When we execute summing_function(1000) , several key processes take place, utilizing our custom MLflow model through the code_inspector decorator. Here\u00e2\u0080\u0099s what happens: Decorator Activation : On calling summing_function(1000) , the code_inspector decorator is the first to activate. This decorator is designed to use the loaded_helper model to analyze the decorated function. Model Analyzes the Function Code : code_inspector retrieves the source code of summing_function using the inspect module. This source code is then passed to the loaded_helper model, which performs an analysis based on its training and provided instructions. The model predicts feedback on code quality, efficiency, and best practices. Feedback Presentation : The feedback generated by the model is printed out. This feedback might include suggestions for code optimization, identification of potential errors, or general advice on coding practices. This step provides an educational insight into the code quality before the function executes its logic. Function Execution : After the feedback is displayed, the summing_function proceeds to execute with the input 1000 . The function calculates the cumulative sum of numbers up to 1000, but due to its inefficient implementation, this process may be slower and more resource-intensive than necessary. Return of Result : The function returns the final computed sum, which is the result of the summing logic implemented within it. This demonstration highlights how the code_inspector decorator, combined with our custom MLflow model, provides a unique, real-time code analysis and feedback mechanism, enhancing the learning and development experience in an interactive environment. [13]: summing_function ( 1000 ) Here's a detailed review of your code:\n\n1. Errors or bugs: There are no syntax errors in your code, but there is a\nlogical error. The summing_function is supposed to calculate the sum of numbers\nfrom 1 to n, but it's doing more than that. It's calculating the sum of numbers\nfrom 1 to i for each i in the range 1 to n, storing these sums in a dictionary,\nand then summing these sums again. This is unnecessary and inefficient.\n\n2. Optimizing code efficiency and structure: The function can be simplified\nsignificantly. The sum of numbers from 1 to n can be calculated directly using\nthe formula n*(n+1)/2. This eliminates the need for the loop and the dictionary,\nmaking the function much more efficient.\n\n3. Enhancing code readability and maintainability: The code can be made more\nreadable by simplifying it and removing unnecessary parts. The use of the\ndictionary and the conversion of numbers to strings and back to numbers is\nconfusing and unnecessary.\n\n4. Best practice advice: In Python, it's best to keep things simple and\nreadable. Avoid unnecessary complexity and use built-in functions and operators\nwhere possible. Also, avoid unnecessary type conversions.\n\nHere's a simplified version of your function:\n\n```python\ndef summing_function(n):\n    return n * (n + 1) // 2\n```\n\nThis function does exactly the same thing as your original function, but it's\nmuch simpler, more efficient, and more readable. [13]: 500500 ",
        "id": "4e51d3cf6b9e6d74b5cb2ab6731f86c3"
    },
    {
        "text": " Analysis of one_liner Function The one_liner function, decorated with code_inspector , demonstrates an interesting approach but has several issues: Complexity : The function uses nested lambda expressions to calculate the factorial of n . While compact, this approach is overly complex and hard to read, making the code less maintainable and understandable. Readability : Good coding practice emphasizes readability, which is compromised here due to the one-liner approach. Such code can be challenging to debug and understand, especially for those unfamiliar with the specific coding style. Best Practices : While demonstrating Python\u00e2\u0080\u0099s capabilities for writing concise code, this example strays from common best practices, particularly in terms of clarity and simplicity. When reviewed by the code_inspector model, these issues are likely to be highlighted, emphasizing the importance of balancing clever coding with readability and maintainability. [14]: @code_inspector ( loaded_helper ) def one_liner ( n ): return ( ( lambda f , n : f ( f , n ))( lambda f , n : n * f ( f , n - 1 ) if n > 1 else 1 , n ) if isinstance ( n , int ) and n >= 0 else \"Invalid input\" ) [15]: one_liner ( 10 ) The code you've provided is a one-liner function that calculates the factorial\nof a given number `n`. It uses a lambda function to recursively calculate the\nfactorial. Here's a review of your code:\n\n1. Errors or bugs: There are no syntax errors or bugs in your code. It correctly\nchecks if the input is a non-negative integer and calculates the factorial. If\nthe input is not a non-negative integer, it returns \"Invalid input\".\n\n2. Optimizing code efficiency and structure: The code is already quite efficient\nas it uses recursion to calculate the factorial. However, the structure of the\ncode is quite complex due to the use of a lambda function for recursion. This\ncan make the code difficult to understand and maintain.\n\n3. Enhancing code readability and maintainability: The code could be made more\nreadable by breaking it down into multiple lines and adding comments to explain\nwhat each part of the code does. The use of a lambda function for recursion\nmakes the code more difficult to understand than necessary. A more\nstraightforward recursive function could be used instead.\n\n4. Best practice advice: In Python, it's generally recommended to use clear and\nsimple code over complex one-liners. This is because clear code is easier to\nread, understand, and maintain. While one-liners can be fun and clever, they can\nalso be difficult to understand and debug.\n\nHere's a revised version of your code that's easier to understand:\n\n```python\ndef factorial(n):\n    # Check if the input is a non-negative integer\n    if not isinstance(n, int) or n < 0:\n        return \"Invalid input\"\n\n    # Base case: factorial of 0 is 1\n    if n == 0:\n        return 1\n\n    # Recursive case: n! = n * (n-1)!\n    return n * factorial(n - 1)\n```\n\nThis version of the code does the same thing as your original code, but it's\nmuch easier to understand because it uses a straightforward recursive function\ninstead of a lambda function. [15]: 3628800 ",
        "id": "61bcb6f6c26a532f63f0543af7bc1237"
    },
    {
        "text": " Reviewing find_phone_numbers Function The find_phone_numbers function, enhanced with the code_inspector , is designed to extract phone numbers from a given text but contains a few notable issues and expected behaviors: Typographical Error : The function incorrectly uses re.complie instead of re.compile , leading to a runtime exception. Pattern Matching Inaccuracy : The regular expression pattern \"(\\d{3})-\\d{3}-\\d{4}\" , while formatted for typical phone numbers, can result in errors if a phone number does not appear in the string. Lack of Error Handling : Directly accessing the first element in phone_numbers without checking if the list is empty can lead to an IndexError . Import Statement Position : The import re statement is inside the function, which is unconventional. Imports are typically placed at the top of a script for clarity. Analysis and Exception Handling : Due to how we crafted our custom MLflow model in code_inspector , the function\u00e2\u0080\u0099s issues will be analyzed and feedback will be returned before the function\u00e2\u0080\u0099s logic is executed. After this analysis, the execution of the function will likely result in an exception (due to the typographical error), demonstrating the importance of careful code review and testing. The code_inspector model\u00e2\u0080\u0099s review will highlight these coding missteps, emphasizing the value of proper syntax, pattern accuracy, and error handling in Python programming. [16]: import re @code_inspector ( loaded_helper ) def find_phone_numbers ( text ): pattern = r \"(\\d {3} )-\\d {3} -\\d {4} \" compiled_pattern = re . complie ( pattern ) phone_numbers = compiled_pattern . findall ( text ) first_number = phone_numbers [ 0 ] print ( f \"First found phone number: { first_number } \" ) return phone_numbers [17]: find_phone_numbers ( \"Give us a call at 888-867-5309\" ) Here's a detailed review of your code:\n\n1. Errors or Bugs:\n   - There's a typo in the `re.compile` function. You've written `re.complie`\ninstead of `re.compile`.\n\n2. Suggestions for Optimizing Code Efficiency and Structure:\n   - The import statement `import re` is inside the function. It's a good\npractice to keep all import statements at the top of the file. This makes it\neasier to see what modules are being used in the script.\n   - The function will throw an error if no phone numbers are found in the text\nbecause you're trying to access the first element of `phone_numbers` without\nchecking if it exists. You should add a check to see if any phone numbers were\nfound before trying to access the first one.\n\n3. Recommendations for Enhancing Code Readability and Maintainability:\n   - The function name `find_phone_numbers`",
        "id": "f2d04fabd48b3e75f46a65abda6eb19e"
    },
    {
        "text": "o access the first element of `phone_numbers` without\nchecking if it exists. You should add a check to see if any phone numbers were\nfound before trying to access the first one.\n\n3. Recommendations for Enhancing Code Readability and Maintainability:\n   - The function name `find_phone_numbers` is clear and descriptive, which is\ngood. However, the variable `pattern` could be more descriptive. Consider\nrenaming it to `phone_number_pattern` or something similar.\n   - You should add docstrings to your function to describe what it does, what\nits parameters are, and what it returns.\n\n4. Best Practice Advice:\n   - Use exception handling to catch potential errors and make your program more\nrobust.\n   - Avoid using print statements in functions that are meant to return a value.\nIf you want to debug, consider using logging instead.\n\nHere's how you could improve your code:\n\n```python\nimport re\n\ndef find_phone_numbers(text):\n    \"\"\"\n    This function finds all phone numbers in the given text.\n\n    Parameters:\n    text (str): The text to search for phone numbers.\n\n    Returns:\n    list: A list of all found phone numbers.\n    \"\"\"\n    phone_number_pattern = \"(\\d{3})-\\d{3}-\\d{4}\"\n    compiled_pattern = re.compile(phone_number_pattern)\n\n    phone_numbers = compiled_pattern.findall(text)\n\n    if phone_numbers:\n        print(f\"First found phone number: {phone_numbers[0]}\")\n\n    return phone_numbers\n```\n\nRemember, the print statement is not recommended in production code. It's there\nfor the sake of this example. --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) /var/folders/cd/n8n0rm2x53l_s0xv_j_xklb00000gp/T/ipykernel_38633/78508464.py in <cell line: 1> () ----> 1 find_phone_numbers ( \"Give us a call at 888-867-5309\" ) /var/folders/cd/n8n0rm2x53l_s0xv_j_xklb00000gp/T/ipykernel_38633/2021999358.py in wrapper (*args, **kwargs) 18 except Exception as e : 19 print ( \"Error during model prediction or formatting:\" , e ) ---> 20 return func ( * args , ** kwargs ) 21 22 return wrapper /var/folders/cd/n8n0rm2x53l_s0xv_j_xklb00000gp/T/ipykernel_38633/773713950.py in find_phone_numbers (text) 5 import re 6 ----> 7 compiled_pattern = re . complie ( pattern ) 8 9 phone_numbers = compiled_pattern . findall ( text ) AttributeError : module 're' has no attribute 'complie' ",
        "id": "5c3d911f40352934fa682e0e83ea6cdf"
    },
    {
        "text": " Conclusion: Harnessing the Power of MLflow in AI-Assisted Development As we conclude this tutorial, we have traversed through the integration of OpenAI\u00e2\u0080\u0099s language models with the robust capabilities of MLflow, creating a powerful toolkit for AI-assisted software development. Here\u00e2\u0080\u0099s a recap of our journey and the key takeaways: Integrating OpenAI with MLflow : We explored how to seamlessly integrate OpenAI\u00e2\u0080\u0099s advanced language models within the MLflow framework. This integration highlighted the potential of combining AI intelligence with robust model management. Implementing a Custom Python Model : Our journey included creating a custom CodeHelper model, which showcased MLflow\u00e2\u0080\u0099s flexibility in handling custom Python functions. This model significantly enhanced the user experience by formatting AI responses into a more readable format. Real-Time Code Analysis and Feedback : By employing the code_inspector decorator, we demonstrated MLflow\u00e2\u0080\u0099s utility in providing real-time, insightful feedback on code quality and efficiency, fostering a learning environment that guides towards best coding practices. Handling Complex Code Analysis : The tutorial presented complex code examples, revealing how MLflow, combined with OpenAI, can handle intricate code analysis, offering suggestions and identifying potential issues. Learning from Interactive Feedback : The interactive feedback loop, enabled by our MLflow model, illustrated a practical approach to learning and improving coding skills, making this toolset particularly valuable for educational and development purposes. Flexibility and Scalability of MLflow : Throughout the tutorial, MLflow\u00e2\u0080\u0099s flexibility and scalability were evident. Whether it\u00e2\u0080\u0099s managing simple Python functions or integrating state-of-the-art AI models, MLflow proved to be an invaluable asset in streamlining the model management process. In summary, this tutorial not only provided insights into effective coding practices but also underscored the versatility of MLflow in enhancing AI-assisted software development. It stands as a testament to how machine learning tools and models can be innovatively applied to improve code quality, efficiency, and the overall development experience.  What\u00e2\u0080\u0099s Next? To continue your learning journey, see the additional advanced tutorials for MLflow\u00e2\u0080\u0099s OpenAI flavor . Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "f1a7fd65128f123d18beeef197c20c68"
    },
    {
        "text": "Advanced Tutorial: Embeddings Support with OpenAI in MLflow 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor MLflow OpenAI Flavor Introduction Autologging Support for the OpenAI integration Tracing with the OpenAI flavor What makes this Integration so Special? Features Getting Started with the MLflow OpenAI Flavor - Tutorials and Guides Detailed Documentation MLflow Sentence-Transformers Flavor MLflow LangChain Flavor MLflow LlamaIndex Flavor MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow OpenAI Flavor Advanced Tutorial: Embeddings Support with OpenAI in MLflow  Advanced Tutorial: Embeddings Support with OpenAI in MLflow Welcome to this advanced guide on implementing OpenAI embeddings within the MLflow framework. This tutorial delves into the configuration and utilization of OpenAI\u00e2\u0080\u0099s powerful embeddings, a key component in modern machine learning models. Download this Notebook  Understanding Embeddings Embeddings are a form of representation learning where words, phrases, or even entire documents are converted into vectors in a high-dimensional space. These vectors capture semantic meaning, enabling models to understand and process language more effectively. Embeddings are extensively used in natural language processing (NLP) for tasks like text classification, sentiment analysis, and language translation.  How Embeddings Work Embeddings work by mapping textual data to vectors such that the distance and direction between vectors represent relationships between the words or phrases. For example, in a well-trained embedding space, synonyms are located closer together, while unrelated terms are farther apart. This spatial arrangement allows algorithms to recognize context and semantics, enhancing their ability to interpret and respond to natural language.  In This Tutorial Embedding Endpoint Configuration : Setting up and utilizing OpenAI\u00e2\u0080\u0099s embedding endpoints in MLflow. Real-world Application : Practical example of comparing the text content of various web pages to one another to determine the amount of similarity in their contextually-specific content. Efficiency and Precision Enhancements : Techniques for improving model performance using OpenAI embeddings. By the end of this tutorial, you\u00e2\u0080\u0099ll have a thorough understanding of how to integrate and leverage OpenAI embeddings in your MLflow projects, harnessing the power of advanced NLP techniques. You\u00e2\u0080\u0099ll also see a real-world application of using text embeddings of documents to compare their similarity. This use case is particularly useful for web content development as a critical task when performing search engine optimization (SEO) to ensure that site page contents are not too similar to one another\n(which could result in a downgrade in page rankings). ",
        "id": "4197bdea02ef90dc7402612c6502bf1e"
    },
    {
        "text": " Required packages In order to run this tutorial, you will need to install beautifulsoup4 from PyPI. Let\u00e2\u0080\u0099s dive into the world of embeddings and explore their transformative impact on machine learning models! [1]: import warnings # Disable a few less-than-useful UserWarnings from setuptools and pydantic warnings . filterwarnings ( \"ignore\" , category = UserWarning ) [2]: import os import numpy as np import openai import requests from bs4 import BeautifulSoup from sklearn.metrics.pairwise import cosine_similarity , euclidean_distances import mlflow from mlflow.models.signature import ModelSignature from mlflow.types.schema import ColSpec , ParamSchema , ParamSpec , Schema , TensorSpec assert \"OPENAI_API_KEY\" in os . environ , \" OPENAI_API_KEY environment variable must be set\"  Integrating OpenAI Model with MLflow for Document Similarity In this tutorial segment, we demonstrate the process of setting up and utilizing an OpenAI embedding model within MLflow for document similarity tasks.  Key Steps Setting an MLflow Experiment : We begin by setting the experiment context in MLflow, specifically for document similarity, using mlflow.set_experiment(\"Documentation Similarity\") . Logging the Model in MLflow : We initiate an MLflow run and log metadata and access configuration parameters to communicate with a specific OpenAI endpoint. The OpenAI endpoint that we\u00e2\u0080\u0099ve chosen here points to the model \u00e2\u0080\u009ctext-embedding-ada-002\u00e2\u0080\u009d, chosen for its robust embedding capabilities. During this step, we detail these access configurations, the embedding task, input/output schemas, and parameters like batch size. Loading the Logged Model for Use : After logging the MLflow model, we proceed to load it using MLflow\u00e2\u0080\u0099s pyfunc module. This is a critical step for applying the model to perform document similarity tasks within the MLflow ecosystem. These steps are essential for integrating access to OpenAI\u00e2\u0080\u0099s embedding model into MLflow, facilitating advanced NLP operations like document similarity analysis. [3]: mlflow . set_experiment ( \"Documenatation Similarity\" ) with mlflow . start_run (): model_info = mlflow . openai . log_model ( model = \"text-embedding-ada-002\" , task = openai . embeddings , artifact_path = \"model\" , signature = ModelSignature ( inputs = Schema ([ ColSpec ( type = \"string\" , name = None )]), outputs = Schema ([ TensorSpec ( type = np . dtype ( \"float64\" ), shape = ( - 1 ,))]), params = ParamSchema ([ ParamSpec ( name = \"batch_size\" , dtype = \"long\" , default = 1024 )]), ), ) # Load the model in pyfunc format model = mlflow . pyfunc . load_model ( model_info . model_uri )  Webpage Text Extraction for Embedding Analysis This section of the tutorial introduces functions designed to extract and prepare text from webpages, a crucial step before applying embedding models for analysis. ",
        "id": "a6677091dc62280881c73207dc458c33"
    },
    {
        "text": " Overview of Functions insert_space_after_tags : Adds a space after specific HTML tags in a BeautifulSoup object for better text readability. extract_text_from_url : Extracts text from a specified webpage section using its URL and a target ID. Filters and organizes the text from tags like , , and , excluding certain irrelevant sections. These functions are integral to preprocessing web content, ensuring that the text fed into the embedding model is clean, relevant, and well-structured. [4]: def insert_space_after_tags ( soup , tags ): \"\"\" Insert a space after each tag specified in the provided BeautifulSoup object. Args: soup: BeautifulSoup object representing the parsed HTML. tags: List of tag names (as strings) after which space should be inserted. \"\"\" for tag_name in tags : for tag in soup . find_all ( tag_name ): tag . insert_after ( \" \" ) def extract_text_from_url ( url , id ): \"\"\" Extract and return text content from a specific section of a webpage. \"\"\" try : response = requests . get ( url ) response . raise_for_status () # Raises HTTPError for bad requests (4XX, 5XX) except requests . exceptions . RequestException as e : return f \"Request failed: { e } \" soup = BeautifulSoup ( response . text , \"html.parser\" ) target_div = soup . find ( \"div\" , { \"class\" : \"section\" , \"id\" : id }) if not target_div : return \"Target element not found.\" insert_space_after_tags ( target_div , [ \"strong\" , \"a\" ]) content_tags = target_div . find_all ([ \"h1\" , \"h2\" , \"h3\" , \"h4\" , \"h5\" , \"h6\" , \"li\" , \"p\" ]) filtered_tags = [ tag for tag in content_tags if not ( ( tag . name == \"li\" and tag . find ( \"p\" ) and tag . find ( \"a\" , class_ = \"reference external\" )) or ( tag . name == \"p\" and tag . find_parent ( \"ul\" )) or ( tag . get_text ( strip = True ) . lower () == \"note\" ) ) ] return \" \\n \" . join ( tag . get_text ( separator = \" \" , strip = True ) for tag in filtered_tags )  Detailed Workflow: The function extract_text_from_url first fetches the webpage content using the requests library. It then parses the HTML content using BeautifulSoup. Specific HTML tags are targeted for text extraction, ensuring that the content is relevant and well-structured for embedding analysis. The insert_space_after_tags function is called within extract_text_from_url to improve text readability post-extraction.  Measuring Similarity and Distance Between Embeddings In this next part of the tutorial, we utilize two functions from sklearn to measure the similarity and distance between document embeddings, essential for evaluating and comparing text-based machine learning models.  Function Overviews cosine_similarity : Purpose : Calculates the cosine similarity between two embedding vectors. How It Works : This function computes similarity by finding the cosine of the angle between the two vectors, a common method for assessing how similar two documents are in terms of their content. Relevance : Very useful in NLP, especially for tasks like document retrieval and clustering, where the goal is to find documents with similar content. euclidean_distances : Purpose : Computes the Euclidean distance between two embedding vectors. Functionality : Similar to cosine_similarity this function calculates the Euclidean distance, which is the \u00e2\u0080\u009cstraight line\u00e2\u0080\u009d distance between the two points in the embedding space. This measure is useful for understanding how different two documents are. Relevance within NLP : Offers a more intuitive physical distance metric, useful for tasks like document classification and anomaly detection. These functions are crucial for analyzing and comparing the outputs of embedding models, providing insights into the relationships between different text data in terms of similarity and distinction.  Comparing Webpages Using Embeddings This section of the tutorial introduces a function, compare_pages , designed to compare the content of two webpages using embedding models. This function is key for understanding how similar or different two given webpages are in terms of their textual content. ",
        "id": "1d5a512ac7ac57e93c5c94d4f94daec7"
    },
    {
        "text": " Function Overview Function Name : compare_pages Purpose : Compares two webpages and returns a similarity score based on their content. Parameters : url1 and url2 : URLs of the webpages to be compared. id1 and id2 : Target IDs for the main text content divs on each page.  How It Works Text Extraction : The function starts by extracting text from the specified sections of each webpage using the extract_text_from_url function. Embedding Prediction : It then uses the previously loaded OpenAI model to generate embeddings for the extracted texts. Similarity and Distance Measurement : The function calculates both the cosine similarity and Euclidean distance between the two embeddings. These metrics provide a quantifiable measure of how similar or dissimilar the webpage contents are. Result : Returns a tuple containing the cosine similarity score and the Euclidean distance. If text extraction fails, it returns an error message.  Practical Application This function is particularly useful in scenarios where comparing the content of different webpages is necessary, such as in content curation, plagiarism detection, or similarity analysis for SEO purposes. By leveraging the power of embeddings and similarity metrics, compare_pages provides a robust method for quantitatively assessing webpage content similarities and differences. [5]: def compare_pages ( url1 , url2 , id1 , id2 ): \"\"\" Compare two webpages and return the similarity score. Args: url1: URL of the first webpage. url2: URL of the second webpage. id1: The target id for the div containing the main text content of the first page id2: The target id for the div containing the main text content of the second page Returns: A tuple of floats representing the similarity score for cosine similarity and euclidean distance. \"\"\" text1 = extract_text_from_url ( url1 , id1 ) text2 = extract_text_from_url ( url2 , id2 ) if text1 and text2 : embedding1 = model . predict ([ text1 ]) embedding2 = model . predict ([ text2 ]) return ( cosine_similarity ( embedding1 , embedding2 ), euclidean_distances ( embedding1 , embedding2 ), ) else : return \"Failed to retrieve content.\"  Similarity Analysis Between MLflow Documentation Pages In this tutorial segment, we demonstrate the practical application of the compare_pages function by comparing two specific pages from the MLflow documentation. Our goal is to assess how similar the content of the main Large Language Models (LLMs) page is to the LLM Evaluation page within the 2.8.1 release of MLflow.  Process Overview Target Webpages : The main LLMs page: LLMs page for MLflow 2.8.1 release The LLM Evaluation page: LLM Evaluation for MLflow 2.8.1 Content IDs : We use \u00e2\u0080\u0098llms\u00e2\u0080\u0099 for the main LLMs page and \u00e2\u0080\u0098mlflow-llm-evaluate\u00e2\u0080\u0099 for the LLM Evaluation page to target specific content sections. Comparison Execution : The compare_pages function is called with these URLs and content IDs to perform the analysis.  Results Cosine Similarity and Euclidean Distance : The function returns two key metrics: Cosine Similarity: Measures the cosine of the angle between the embedding vectors of the two pages. A higher value indicates greater similarity. Euclidean Distance: Represents the \u00e2\u0080\u0098straight-line\u00e2\u0080\u0099 distance between the two points in the embedding space, with lower values indicating closer similarity.  Interpretation The results show a high degree of cosine similarity (0.8792), suggesting that the content of the two pages is quite similar in terms of context and topics covered. The Euclidean distance of 0.4914, while relatively low, offers a complementary perspective, indicating some level of distinctiveness in the content. ",
        "id": "0b5943b73c25b04f0d36d1a84e39d8e2"
    },
    {
        "text": " Conclusion This analysis highlights the effectiveness of using embeddings and similarity metrics for comparing webpage content. In practical terms, it helps in understanding the overlap and differences in documentation, aiding in content optimization, redundancy reduction, and ensuring comprehensive coverage of topics. [6]: # Get the similarity between the main LLMs page in the MLflow Docs and the LLM Evaluation page for the 2.8.1 release of MLflow llm_cosine , llm_euclid = compare_pages ( url1 = \"https://www.mlflow.org/docs/2.8.1/llms/index.html\" , url2 = \"https://www.mlflow.org/docs/2.8.1/llms/llm-evaluate/index.html\" , id1 = \"llms\" , id2 = \"mlflow-llm-evaluate\" , ) print ( f \"The cosine similarity between the LLMs page and the LLM Evaluation page is: { llm_cosine } and the euclidean distance is: { llm_euclid } \" ) The cosine similarity between the LLMs page and the LLM Evaluation page is: [[0.879243]] and the euclidean distance is: [[0.49144073]]  Brief Overview of Similarity Between MLflow LLMs and Plugins Pages This section demonstrates a quick similarity analysis between the MLflow Large Language Models (LLMs) page and the Plugins page from the 2.8.1 release.  Analysis Execution Pages Compared : LLMs page: LLMs page for MLflow 2.8.1 release Plugins page: Plugins page for MLflow 2.8.1 release IDs Used : \u00e2\u0080\u0098llms\u00e2\u0080\u0099 for the LLMs page and \u00e2\u0080\u0098mflow-plugins\u00e2\u0080\u0099 for the Plugins page. Function : compare_pages is utilized for the comparison.  Results Cosine Similarity : 0.6806, indicating moderate similarity in content. Euclidean Distance : 0.7992, suggesting a noticeable difference in the context and topics covered by the two pages. The results reflect a moderate level of similarity between the LLMs and Plugins pages, with a significant degree of distinctiveness in their content. This analysis is useful for understanding the relationship and content overlap between different sections of the MLflow documentation. [7]: # Get the similarity between the main LLMs page in the MLflow Docs and the Plugins page for the 2.8.1 release of MLflow plugins_cosine , plugins_euclid = compare_pages ( url1 = \"https://www.mlflow.org/docs/2.8.1/llms/index.html\" , url2 = \"https://www.mlflow.org/docs/2.8.1/plugins.html\" , id1 = \"llms\" , id2 = \"mflow-plugins\" , ) print ( f \"The cosine similarity between the LLMs page and the MLflow Projects page is: { plugins_cosine } and the euclidean distance is: { plugins_euclid } \" ) The cosine similarity between the LLMs page and the MLflow Projects page is: [[0.68062298]] and the euclidean distance is: [[0.79922088]]  Tutorial Recap: Leveraging OpenAI Embeddings in MLflow As we conclude this tutorial, let\u00e2\u0080\u0099s recap the key concepts and techniques we\u00e2\u0080\u0099ve explored regarding the use of OpenAI embeddings within the MLflow framework.  Key Takeaways Integrating OpenAI Models in MLflow : We learned how to log and load OpenAI\u00e2\u0080\u0099s \u00e2\u0080\u009ctext-embedding-ada-002\u00e2\u0080\u009d model within MLflow, an essential step for utilizing these embeddings in machine learning workflows. Text Extraction and Preprocessing : The tutorial introduced methods for extracting and preprocessing text from webpages, ensuring the data is clean and structured for embedding analysis. Calculating Similarity and Distance : We delved into functions for measuring cosine similarity and Euclidean distance between document embeddings, vital for comparing textual content. Real-World Application: Webpage Content Comparison : Practical application of these concepts was demonstrated through the comparison of different MLflow documentation pages. We analyzed the similarity and differences in their content using the embeddings generated by the OpenAI model. Interpreting Results : The tutorial provided insights into interpreting the results of similarity and distance metrics, highlighting their relevance in understanding content relationships. ",
        "id": "91bf42369edf4990751ea67573c7859a"
    },
    {
        "text": " Conclusion This advanced tutorial aimed to enhance your skills in applying OpenAI embeddings in MLflow, focusing on real-world applications like document similarity analysis. By integrating these powerful NLP tools, we\u00e2\u0080\u0099ve showcased how to extract more value and insights from textual data, a crucial aspect of modern machine learning projects. We hope this guide has been informative and instrumental in advancing your understanding and application of OpenAI embeddings within the MLflow framework.  What\u00e2\u0080\u0099s Next? To continue your learning journey, see the additional advanced tutorials for MLflow\u00e2\u0080\u0099s OpenAI flavor . Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "87e9f0b5b0f551dedbd2dd28bf1395d1"
    },
    {
        "text": "OpenAI within MLflow 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor MLflow OpenAI Flavor Introduction Autologging Support for the OpenAI integration Tracing with the OpenAI flavor What makes this Integration so Special? Features Getting Started with the MLflow OpenAI Flavor - Tutorials and Guides Detailed Documentation MLflow Sentence-Transformers Flavor MLflow LangChain Flavor MLflow LlamaIndex Flavor MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow OpenAI Flavor OpenAI within MLflow  OpenAI within MLflow Attention The openai flavor is under active development and is marked as Experimental. Public APIs may change and new features are\nsubject to be added as additional functionality is brought to the flavor.  Overview The integration of OpenAI\u00e2\u0080\u0099s advanced language models within MLflow opens up new frontiers in creating and using NLP-based applications. It enables users to harness\nthe cutting-edge capabilities of models like GPT-4 for varied tasks, ranging from conversational AI to complex text analysis\nand embeddings generation. This integration is a leap forward in making advanced NLP accessible and manageable within a robust framework like MLflow.  Beyond Simple Deployment: Building Powerful NLP Applications with OpenAI and MLflow While the openai flavor within MLflow simplifies the logging and deployment of OpenAI models, its true potential lies in unlocking the full power of NLP\napplications. By seamlessly integrating with MLflow, you can:  Craft Task-Specific Services Raw access to a large language model doesn\u00e2\u0080\u0099t guarantee a valuable service. While powerful, unprompted models can be overly general, leading to unintended\noutputs or inappropriate responses for the intent of the application. MLflow enables users to tailor models for specific tasks, achieving desired functionalities\nwhile ensuring context and control. This allows you to: Define prompts and parameters : Instead of relying on open-ended inputs, you can define specific prompts and parameters that guide the model\u00e2\u0080\u0099s responses, focusing its capabilities on the desired task. Save and deploy customized models : The saved models, along with their prompts and parameters, can be easily deployed and shared, ensuring consistent behavior and performance. Perform champion/challenger evaluations : MLflow allows users to easily compare different prompts, parameters, and deployment configurations, facilitating the selection of the most effective model for a specific task.  Simplify Deployment and Comparison MLflow streamlines the deployment process, enabling you to: Package and deploy models as applications : The openai flavor simplifies model packaging, including prompts, configuration parameters, and inference parameters, into a single, portable artifact. Compare different approaches : With consistent packaging, you can easily compare different models, prompts, configurations, and deployment options, facilitating informed decision-making. Leverage MLflow\u00e2\u0080\u0099s ecosystem : MLflow integrates with various tools and platforms, allowing users to deploy models on diverse environments, from cloud platforms to local servers. ",
        "id": "c63ca20f6178100e42fc8f0cd1e5799c"
    },
    {
        "text": " Advanced Prompt Engineering and Version Tracking with MLflow and OpenAI: Unleashing the True Potential of LLMs The integration of MLflow and OpenAI marks a paradigm shift in the field of prompt engineering for large language models (LLMs). While basic prompts can\nenable rudimentary functionalities, this powerful combination unlocks the full potential of LLMs, empowering developers and data scientists to meticulously\ncraft and refine prompts, ushering in a new era of targeted and impactful applications.  Beyond the Basics: Embracing Iterative Experimentation Forget static prompts and limited applications! MLflow and OpenAI revolutionize the process by facilitating iterative experimentation through: Tracking and Comparison : MLflow logs and meticulously tracks every iteration of a prompt alongside its performance metrics. This allows for a granular comparison of different versions, enabling informed decisions and identification of the most effective prompts. Version Control for Reproducible Experimentation : Each prompt iteration is safely stored and version-controlled within MLflow. This allows for easy rollback and comparison, fostering experimentation and refinement while ensuring reproducibility, a crucial aspect of scientific advancement. Flexible Parameterization : MLflow enables control over which parameters are permitted to be modified at inference time, giving you the power to control creativity (temperature) and maximum token length (for cost).  Refining for Optimum Results: A/B Testing and Fine-Tuning MLflow and OpenAI empower you to push the boundaries of LLM performance by: A/B Testing for Optimal Prompt Selection : Perform efficient A/B testing of different prompt variations and parameter configurations. This allows for the identification of the most effective combination for specific tasks and user profiles, leading to remarkable performance gains. Tailoring Prompts for Desired Outcomes : Iterative and organized experimentation allows you to focus on what makes the most sense for your applications. Whether you prioritize factual accuracy, creative expression, or conversational fluency, MLflow and OpenAI empower you to tailor prompts to optimize specific performance metrics. This ensures that your LLM applications deliver the desired results, time and time again.  Collaboration and Sharing: Fueling Innovation and Progress The power of MLflow and OpenAI extends beyond individual projects. By facilitating collaboration and sharing, they accelerate the advancement of LLM applications: Shareable Artifacts for Collaborative Innovation : MLflow packages prompts, parameters, model versions, and performance metrics into shareable artifacts. This enables researchers and developers to collaborate seamlessly, leveraging each other\u00e2\u0080\u0099s insights and refined prompts to accelerate progress.  Leveraging MLflow for Optimized Prompt Engineering Iterative Improvement : MLflow\u00e2\u0080\u0099s tracking system supports an iterative approach to prompt engineering. By logging each experiment, users can incrementally refine their prompts, driving towards the most effective model interaction. Collaborative Experimentation : MLflow\u00e2\u0080\u0099s collaborative features enable teams to share and discuss prompt versions and experiment results, fostering a collaborative environment for prompt development.  Real-World Impact In real-world applications, the ability to track and refine prompts using MLflow and OpenAI leads to more accurate, reliable, and efficient language model\nimplementations. Whether in customer service chatbots, content generation, or complex decision support systems, the meticulous management of prompts\nand model versions directly translates to enhanced performance and user experience. This integration not only simplifies the complexities of working with advanced LLMs but also opens up new avenues for innovation in NLP applications,\nensuring that each prompt-driven interaction is as effective and impactful as possible. ",
        "id": "e10f047e7b995ecc36ab2c95e540b497"
    },
    {
        "text": " Direct OpenAI Service Usage Direct usage of OpenAI\u00e2\u0080\u0099s service through MLflow allows for seamless interaction with the latest GPT models for a variety of NLP tasks. import logging import os import openai import pandas as pd import mlflow from mlflow.models.signature import ModelSignature from mlflow.types.schema import ColSpec , ParamSchema , ParamSpec , Schema logging . getLogger ( \"mlflow\" ) . setLevel ( logging . ERROR ) # Uncomment the following lines to run this script without using a real OpenAI API key. # os.environ[\"MLFLOW_TESTING\"] = \"true\" # os.environ[\"OPENAI_API_KEY\"] = \"test\" assert \"OPENAI_API_KEY\" in os . environ , \"Please set the OPENAI_API_KEY environment variable.\" print ( \"\"\" # ****************************************************************************** # Single variable # ****************************************************************************** \"\"\" ) with mlflow . start_run (): model_info = mlflow . openai . log_model ( model = \"gpt-4o-mini\" , task = openai . chat . completions , artifact_path = \"model\" , messages = [{ \"role\" : \"user\" , \"content\" : \"Tell me a joke about {animal} .\" }], ) model = mlflow . pyfunc . load_model ( model_info . model_uri ) df = pd . DataFrame ( { \"animal\" : [ \"cats\" , \"dogs\" , ] } ) print ( model . predict ( df )) list_of_dicts = [ { \"animal\" : \"cats\" }, { \"animal\" : \"dogs\" }, ] print ( model . predict ( list_of_dicts )) list_of_strings = [ \"cats\" , \"dogs\" , ] print ( model . predict ( list_of_strings )) print ( \"\"\" # ****************************************************************************** # Multiple variables # ****************************************************************************** \"\"\" ) with mlflow . start_run (): model_info = mlflow . openai . log_model ( model = \"gpt-4o-mini\" , task = openai . chat . completions , artifact_path = \"model\" , messages = [{ \"role\" : \"user\" , \"content\" : \"Tell me a {adjective} joke about {animal} .\" }], ) model = mlflow . pyfunc . load_model ( model_info . model_uri ) df = pd . DataFrame ( { \"adjective\" : [ \"funny\" , \"scary\" ], \"animal\" : [ \"cats\" , \"dogs\" ], } ) print ( model . predict ( df )) list_of_dicts = [ { \"adjective\" : \"funny\" , \"animal\" : \"cats\" }, { \"adjective\" : \"scary\" , \"animal\" : \"dogs\" }, ] print ( model . predict ( list_of_dicts )) print ( \"\"\" # ****************************************************************************** # Multiple prompts # ****************************************************************************** \"\"\" ) with mlflow . start_run (): model_info = mlflow . openai . log_model ( model = \"gpt-4o-mini\" , task = openai . chat . completions , artifact_path = \"model\" , messages = [ { \"role\" : \"system\" , \"content\" : \"You are {person} \" }, { \"role\" : \"user\" , \"content\" : \"Let me hear your thoughts on {topic} \" }, ], ) model = mlflow . pyfunc . load_model ( model_info . model_uri ) df = pd . DataFrame ( { \"perso",
        "id": "471f86563a65e8548bfa8abedd973e7a"
    },
    {
        "text": "pt-4o-mini\" , task = openai . chat . completions , artifact_path = \"model\" , messages = [ { \"role\" : \"system\" , \"content\" : \"You are {person} \" }, { \"role\" : \"user\" , \"content\" : \"Let me hear your thoughts on {topic} \" }, ], ) model = mlflow . pyfunc . load_model ( model_info . model_uri ) df = pd . DataFrame ( { \"person\" : [ \"Elon Musk\" , \"Jeff Bezos\" ], \"topic\" : [ \"AI\" , \"ML\" ], } ) print ( model . predict ( df )) list_of_dicts = [ { \"person\" : \"Elon Musk\" , \"topic\" : \"AI\" }, { \"person\" : \"Jeff Bezos\" , \"topic\" : \"ML\" }, ] print ( model . predict ( list_of_dicts )) print ( \"\"\" # ****************************************************************************** # No input variables # ****************************************************************************** \"\"\" ) with mlflow . start_run (): model_info = mlflow . openai . log_model ( model = \"gpt-4o-mini\" , task = openai . chat . completions , artifact_path = \"model\" , messages = [{ \"role\" : \"system\" , \"content\" : \"You are Elon Musk\" }], ) model = mlflow . pyfunc . load_model ( model_info . model_uri ) df = pd . DataFrame ( { \"question\" : [ \"Let me hear your thoughts on AI\" , \"Let me hear your thoughts on ML\" , ], } ) print ( model . predict ( df )) list_of_dicts = [ { \"question\" : \"Let me hear your thoughts on AI\" }, { \"question\" : \"Let me hear your thoughts on ML\" }, ] model = mlflow . pyfunc . load_model ( model_info . model_uri ) print ( model . predict ( list_of_dicts )) list_of_strings = [ \"Let me hear your thoughts on AI\" , \"Let me hear your thoughts on ML\" , ] model = mlflow . pyfunc . load_model ( model_info . model_uri ) print ( model . predict ( list_of_strings )) print ( \"\"\" # ****************************************************************************** # Inference parameters with chat completions # ****************************************************************************** \"\"\" ) with mlflow . start_run (): model_info = mlflow . openai . log_model ( model = \"gpt-4o-mini\" , task = openai . chat . completions , artifact_path = \"model\" , messages = [{ \"role\" : \"user\" , \"content\" : \"Tell me a joke about {animal} .\" }], signature = ModelSignature ( inputs = Schema ([ ColSpec ( type = \"string\" , name = None )]), outputs = Schema ([ ColSpec ( type = \"string\" , name = None )]), params = ParamSchema ( [ ParamSpec ( name = \"temperature\" , default = 0 , dtype = \"float\" ), ] ), ), ) model = mlflow . pyfunc . load_model ( model_info . model_uri ) df = pd . DataFrame ( { \"animal\" : [ \"cats\" , \"dogs\" , ] } ) print ( model . predict ( df , params = { \"temperature\" : 1 })) ",
        "id": "6464cab6b9f3c2b4dffda749ec704dbd"
    },
    {
        "text": " Azure OpenAI Service Integration The openai flavor supports logging models that use the Azure OpenAI Service .\nThere are a few notable differences between the Azure OpenAI Service and the OpenAI Service that need to be considered when logging models that target Azure endpoints.  Environment Configuration for Azure Integration To successfully log a model targeting Azure OpenAI Service, specific environment variables are essential for authentication and functionality. Note The following environment variables contain highly sensitive access keys . Ensure that you do not commit these values to source control or declare them in an interactive\nenvironment. Environment variables should be set from within your terminal via an export command, an addition to your user profile configurations (i.e., .bashrc or .zshrc),\nor set through your IDE\u00e2\u0080\u0099s environment variable configuration. Please do not leak your credentials. OPENAI_API_KEY : The API key for the Azure OpenAI Service. This can be found in the Azure Portal under the \u00e2\u0080\u009cKeys and Endpoint\u00e2\u0080\u009d section of the \u00e2\u0080\u009cKeys and Endpoint\u00e2\u0080\u009d tab. You can use either KEY1 or KEY2 . OPENAI_API_BASE : The base endpoint for your Azure OpenAI resource (e.g., https://<your-service-name>.openai.azure.com/ ). Within the Azure OpenAI documentation and guides, this key is referred to as AZURE_OPENAI_ENDPOINT or simply ENDPOINT . OPENAI_API_VERSION : The API version to use for the Azure OpenAI Service. More information can be found in the Azure OpenAI documentation , including up-to-date lists of supported versions. OPENAI_API_TYPE : If using Azure OpenAI endpoints, this value should be set to \"azure\" . OPENAI_DEPLOYMENT_NAME : The deployment name that you chose when you deployed the model in Azure. To learn more, visit the Azure OpenAI deployment documentation .  Azure OpenAI Service in MLflow Integrating Azure OpenAI models within MLflow follows similar procedures to direct OpenAI service usage, with additional Azure-specific configurations. import openai import pandas as pd import mlflow \"\"\" Set environment variables for Azure OpenAI service export OPENAI_API_KEY=\"<AZURE OPENAI KEY>\" # OPENAI_API_BASE should be the endpoint of your Azure OpenAI resource # e.g. https://<service-name>.openai.azure.com/ export OPENAI_API_BASE=\"<AZURE OPENAI BASE>\" # OPENAI_API_VERSION e.g. 2023-05-15 export OPENAI_API_VERSION=\"<AZURE OPENAI API VERSION>\" export OPENAI_API_TYPE=\"azure\" export OPENAI_DEPLOYMENT_NAME=\"<AZURE OPENAI DEPLOYMENT ID OR NAME>\" \"\"\" with mlflow . start_run (): model_info = mlflow . openai . log_model ( # Your Azure OpenAI model e.g. gpt-4o-mini model = \"<YOUR AZURE OPENAI MODEL>\" , task = openai . chat . completions , artifact_path = \"model\" , messages = [{ \"role\" : \"user\" , \"content\" : \"Tell me a joke about {animal} .\" }], ) # Load native OpenAI model native_model = mlflow . openai . load_model ( model_info . model_uri ) completion = openai . chat . completions . create ( deployment_id = native_model [ \"deployment_id\" ], messages = native_model [ \"messages\" ], ) print ( completion [ \"choices\" ][ 0 ][ \"message\" ][ \"content\" ]) # Load as Pyfunc model model = mlflow . pyfunc . load_model ( model_info . model_uri ) df = pd . DataFrame ( { \"animal\" : [ \"cats\" , \"dogs\" , ] } ) print ( model . predict ( df )) list_of_dicts = [ { \"animal\" : \"cats\" }, { \"animal\" : \"dogs\" }, ] print ( model . predict ( list_of_dicts )) list_of_strings = [ \"cats\" , \"dogs\" , ] print ( model . predict ( list_of_strings )) list_of_strings = [ \"Let me hear your thoughts on AI\" , \"Let me hear your thoughts on ML\" , ] model = mlflow . pyfunc . load_model ( model_info . model_uri ) print ( model . predict ( list_of_strings ))  OpenAI Autologging Attention Autologging is only supported for OpenAI >= 1.17. To learn more about autologging support for the OpenAI flavor, please see the autologging guide . For more examples, please click here . ",
        "id": "2465af4f4065e26de8f0fc97d14f2930"
    },
    {
        "text": " Next Steps in Your NLP Journey We invite you to harness the combined power of MLflow and OpenAI for developing innovative NLP applications. Whether it\u00e2\u0080\u0099s creating interactive\nAI-driven platforms, enhancing data analysis with deep NLP insights, or exploring new frontiers in AI, this integration serves as a robust foundation\nfor your explorations  Supplementary Learnings If you\u00e2\u0080\u0099re a bit curious about what really sets apart OpenAI\u00e2\u0080\u0099s GPT models from other language models, we\u00e2\u0080\u0099ve included a brief (and heavily simplified) overview of\ntheir training process below.\nThis is but one small aspect of why they\u00e2\u0080\u0099re so good and capable of responding in such a human-like manner, but it\u00e2\u0080\u0099s a fascintating insight into how different the\nfine-tuning process is for these models as compared to the more familiar process of traditional supervised machine learning.  RLHF in GPT Models One of the defining features of OpenAI\u00e2\u0080\u0099s GPT models is their training process, particularly the use of Reinforcement Learning from Human Feedback\n(RLHF). This methodology sets GPT models apart from traditional language models in several ways (although they are not the only organization to use this\nstrategy, it is a key process component that greatly helps to enhance the quality of their services).  The RLHF Process Supervised Fine-Tuning (SFT) : Initially, GPT models undergo supervised fine-tuning using a large dataset of text. This process imparts the basic understanding of language and context. Reward Modeling (RM) : Human trainers review the model\u00e2\u0080\u0099s outputs and rate them based on criteria such as relevance, accuracy, and safety. This feedback is used to create a \u00e2\u0080\u0098reward model\u00e2\u0080\u0099\u00e2\u0080\u0094a system that evaluates the quality of the model\u00e2\u0080\u0099s responses. Proximal Policy Optimization (PPO) : In this stage, the model is trained using reinforcement learning techniques, guided by the reward model. The model learns to generate responses that are more aligned with the values and preferences as judged by human trainers. Iterative Improvement : The model undergoes continuous refinement through human feedback, ensuring that it evolves and adapts to produce responses that are aligned with the feedback preferences provided by the human reviewers.  Why RLHF Matters Human-Like Responses : RLHF enables GPT models to generate responses that closely mimic human thought processes, making them more relatable and effective in practical applications. Safety and Relevance : Through human feedback, GPT models learn to avoid generating harmful or irrelevant content, thereby increasing their reliability and applicability. Cost-Effective Training : RLHF allows for more efficient and cost-effective training compared to extensively curating the training dataset to ensure that only desired outputs are generated.  Simplified overview of RLHF Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "eb99a1c72d0762b68302996b93db8cf7"
    },
    {
        "text": "Introduction to Sentence Transformers and MLflow 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor MLflow OpenAI Flavor MLflow Sentence-Transformers Flavor Introduction What makes this Library so Special? Features Getting Started with the MLflow Sentence Transformers Flavor - Tutorials and Guides Detailed Documentation Learning More About Sentence Transformers MLflow LangChain Flavor MLflow LlamaIndex Flavor MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow Sentence-Transformers Flavor Introduction to Sentence Transformers and MLflow  Introduction to Sentence Transformers and MLflow Welcome to our tutorial on leveraging Sentence Transformers with MLflow for advanced natural language processing and model management. Download this Notebook  Learning Objectives Set up a pipeline for sentence embeddings with sentence-transformers . Log models and configurations using MLflow. Understand and apply model signatures in MLflow to sentence-transformers . Deploy and use models for inference with MLflow\u00e2\u0080\u0099s features.  What are Sentence Transformers? Sentence Transformers, an extension of the Hugging Face Transformers library, are designed for generating semantically rich sentence embeddings. They utilize models like BERT and RoBERTa, fine-tuned for tasks such as semantic search and text clustering, producing high-quality sentence-level embeddings.  Benefits of Integrating MLflow with Sentence Transformers Combining MLflow with Sentence Transformers enhances NLP projects by: Streamlining experiment management and logging. Offering better control over model versions and configurations. Ensuring reproducibility of results and model predictions. Simplifying the deployment process in production environments. This integration empowers efficient tracking, management, and deployment of NLP applications. [1]: # Disable tokenizers warnings when constructing pipelines % env TOKENIZERS_PARALLELISM=false import warnings # Disable a few less-than-useful UserWarnings from setuptools and pydantic warnings . filterwarnings ( \"ignore\" , category = UserWarning ) env: TOKENIZERS_PARALLELISM=false  Setting Up the Environment for Sentence Embedding Begin your journey with Sentence Transformers and MLflow by establishing the core working environment.  Key Steps for Initialization Import necessary libraries: SentenceTransformer and mlflow . Initialize the \"all-MiniLM-L6-v2\" Sentence Transformer model.  Model Initialization The compact and efficient \"all-MiniLM-L6-v2\" model is chosen for its effectiveness in generating meaningful sentence embeddings. Explore more models at the Hugging Face Hub .  Purpose of the Model This model excels in transforming sentences into semantically rich embeddings, applicable in various NLP tasks like semantic search and clustering. [2]: from sentence_transformers import SentenceTransformer import mlflow model = SentenceTransformer ( \"all-MiniLM-L6-v2\" )  Defining the Model Signature with MLflow Defining the model signature is a crucial step in setting up our Sentence Transformer model for consistent and expected behavior during inference.  Steps for Signature Definition Prepare Example Sentences : Define example sentences to demonstrate the model\u00e2\u0080\u0099s input and output formats. Generate Model Signature : Use the mlflow.models.infer_signature function with the model\u00e2\u0080\u0099s input and output to automatically define the signature. ",
        "id": "4a3c2b96b9e34f76d430a759f3a4eb14"
    },
    {
        "text": " Importance of the Model Signature Clarity in Data Formats : Ensures clear documentation of the data types and structures the model expects and produces. Model Deployment and Usage : Crucial for deploying models to production, ensuring the model receives inputs in the correct format and produces expected outputs. Error Prevention : Helps in preventing errors during model inference by enforcing consistent data formats. NOTE : The List[str] input type is equivalent at inference time to str . The MLflow flavor uses a ColSpec[str] definition for the input type. [3]: example_sentences = [ \"A sentence to encode.\" , \"Another sentence to encode.\" ] # Infer the signature of the custom model by providing an input example and the resultant prediction output. # We're not including any custom inference parameters in this example, but you can include them as a third argument # to infer_signature(), as you will see in the advanced tutorials for Sentence Transformers. signature = mlflow . models . infer_signature ( model_input = example_sentences , model_output = model . encode ( example_sentences ), ) # Visualize the signature signature [3]: inputs:\n  [string]\noutputs:\n  [Tensor('float32', (-1, 384))]\nparams:\n  None  Creating an experiment We create a new MLflow Experiment so that the run we\u00e2\u0080\u0099re going to log our model to does not log to the default experiment and instead has its own contextually relevant entry. [4]: # If you are running this tutorial in local mode, leave the next line commented out. # Otherwise, uncomment the following line and set your tracking uri to your local or remote tracking server. # mlflow.set_tracking_uri(\"http://127.0.0.1:8080\") mlflow . set_experiment ( \"Introduction to Sentence Transformers\" ) [4]: <Experiment: artifact_location='file:///Users/benjamin.wilson/repos/mlflow-fork/mlflow/docs/source/llms/sentence-transformers/tutorials/quickstart/mlruns/469990615226680434', creation_time=1701280211449, experiment_id='469990615226680434', last_update_time=1701280211449, lifecycle_stage='active', name='Introduction to Sentence Transformers', tags={}>  Logging the Sentence Transformer Model with MLflow Logging the model in MLflow is essential for tracking, version control, and deployment, following the initialization and signature definition of our Sentence Transformer model.  Steps for Logging the Model Start an MLflow Run : Initiate a new run with mlflow.start_run() , grouping all logging operations. Log the Model : Use mlflow.sentence_transformers.log_model to log the model, providing the model object, artifact path, signature, and an input example.  Importance of Model Logging Model Management : Facilitates the model\u00e2\u0080\u0099s lifecycle management from training to deployment. Reproducibility and Tracking : Enables tracking of model versions and ensures reproducibility. Ease of Deployment : Simplifies deployment by allowing models to be easily deployed for inference. [5]: with mlflow . start_run (): logged_model = mlflow . sentence_transformers . log_model ( model = model , artifact_path = \"sbert_model\" , signature = signature , input_example = example_sentences , )  Loading the Model and Testing Inference After logging the Sentence Transformer model in MLflow, we demonstrate how to load and test it for real-time inference.  Loading the Model as a PyFunc Why PyFunc : Load the logged model using mlflow.pyfunc.load_model for seamless integration into Python-based services or applications. Model URI : Use the logged_model.model_uri to accurately locate and load the model from MLflow.  Conducting Inference Tests Test Sentences : Define sentences to test the model\u00e2\u0080\u0099s embedding generation capabilities. Performing Predictions : Use the model\u00e2\u0080\u0099s predict method with test sentences to obtain embeddings. Printing Embedding Lengths : Verify embedding generation by checking the length of embedding arrays, corresponding to the dimensionality of each sentence representation. ",
        "id": "ec813d66eca7be37a19f1d9a98985d53"
    },
    {
        "text": " Importance of Inference Testing Model Validation : Confirm the model\u00e2\u0080\u0099s expected behavior and data processing capability upon loading. Deployment Readiness : Validate the model\u00e2\u0080\u0099s readiness for real-time integration into application services. [6]: inference_test = [ \"I enjoy pies of both apple and cherry.\" , \"I prefer cookies.\" ] # Load our custom model by providing the uri for where the model was logged. loaded_model_pyfunc = mlflow . pyfunc . load_model ( logged_model . model_uri ) # Perform a quick test to ensure that our loaded model generates the correct output embeddings_test = loaded_model_pyfunc . predict ( inference_test ) # Verify that the output is a list of lists of floats (our expected output format) print ( f \"The return structure length is: { len ( embeddings_test ) } \" ) for i , embedding in enumerate ( embeddings_test ): print ( f \"The size of embedding { i + 1 } is: { len ( embeddings_test [ i ]) } \" ) The return structure length is: 2\nThe size of embedding 1 is: 384\nThe size of embedding 2 is: 384  Displaying Samples of Generated Embeddings Examine the content of embeddings to verify their quality and understand the model\u00e2\u0080\u0099s output.  Inspecting the Embedding Samples Purpose of Sampling : Inspect a sample of the entries in each embedding to understand the vector representations generated by the model. Printing Embedding Samples : Print the first 10 entries of each embedding vector using embedding[:10] to get a glimpse into the model\u00e2\u0080\u0099s output.  Why Sampling is Important Quality Check : Sampling provides a quick way to verify the embeddings\u00e2\u0080\u0099 quality and ensures they are meaningful and non-degenerate. Understanding Model Output : Seeing parts of the embedding vectors offers an intuitive understanding of the model\u00e2\u0080\u0099s output, beneficial for debugging and development. [7]: for i , embedding in enumerate ( embeddings_test ): print ( f \"The sample of the first 10 entries in embedding { i + 1 } is: { embedding [: 10 ] } \" ) The sample of the first 10 entries in embedding 1 is: [ 0.04866192 -0.03687946  0.02408808  0.03534171 -0.12739632  0.00999414\n  0.07135344 -0.01433522  0.04296691 -0.00654414]\nThe sample of the first 10 entries in embedding 2 is: [-0.03879027 -0.02373698  0.01314073  0.03589077 -0.01641303 -0.0857707\n  0.08282158 -0.03173266  0.04507608  0.02777079]  Native Model Loading in MLflow for Extended Functionality Explore the full range of Sentence Transformer functionalities with MLflow\u00e2\u0080\u0099s support for native model loading.  Why Support Native Loading? Access to Native Functionalities : Native loading unlocks all the features of the Sentence Transformer model, essential for advanced NLP tasks. Loading the Model Natively : Use mlflow.sentence_transformers.load_model to load the model with its full capabilities, enhancing flexibility and efficiency. ",
        "id": "7489d767fd8171f6d15060a6d748e221"
    },
    {
        "text": " Generating Embeddings Using Native Model Model Encoding : Employ the model\u00e2\u0080\u0099s native encode method to generate embeddings, taking advantage of optimized functionality. Importance of Native Encoding : Native encoding ensures the utilization of the model\u00e2\u0080\u0099s full embedding generation capabilities, suitable for large-scale or complex NLP applications. [8]: # Load the saved model as a native Sentence Transformers model (unlike above, where we loaded as a generic python function) loaded_model_native = mlflow . sentence_transformers . load_model ( logged_model . model_uri ) # Use the native model to generate embeddings by calling encode() (unlike for the generic python function which uses the single entrypoint of `predict`) native_embeddings = loaded_model_native . encode ( inference_test ) for i , embedding in enumerate ( native_embeddings ): print ( f \"The sample of the native library encoding call for embedding { i + 1 } is: { embedding [: 10 ] } \" ) 2023/11/30 15:50:24 INFO mlflow.sentence_transformers: 'runs:/eeab3c1b13594fdea13e07585b1c0596/sbert_model' resolved as 'file:///Users/benjamin.wilson/repos/mlflow-fork/mlflow/docs/source/llms/sentence-transformers/tutorials/quickstart/mlruns/469990615226680434/eeab3c1b13594fdea13e07585b1c0596/artifacts/sbert_model' The sample of the native library encoding call for embedding 1 is: [ 0.04866192 -0.03687946  0.02408808  0.03534171 -0.12739632  0.00999414\n  0.07135344 -0.01433522  0.04296691 -0.00654414]\nThe sample of the native library encoding call for embedding 2 is: [-0.03879027 -0.02373698  0.01314073  0.03589077 -0.01641303 -0.0857707\n  0.08282158 -0.03173266  0.04507608  0.02777079]  Conclusion: Embracing the Power of Sentence Transformers with MLflow As we reach the end of our Introduction to Sentence Transformers tutorial, we have successfully navigated the basics of integrating the Sentence Transformers library with MLflow. This foundational knowledge sets the stage for more advanced and specialized applications in the field of Natural Language Processing (NLP).  Recap of Key Learnings Integration Basics : We covered the essential steps of loading and logging a Sentence Transformer model using MLflow. This process demonstrated the simplicity and effectiveness of integrating cutting-edge NLP tools within MLflow\u00e2\u0080\u0099s ecosystem. Signature and Inference : Through the creation of a model signature and the execution of inference tasks, we showcased how to operationalize the Sentence Transformer model, ensuring that it\u00e2\u0080\u0099s ready for real-world applications. Model Loading and Prediction : We explored two ways of loading the model - as a PyFunc model and using the native Sentence Transformers loading mechanism. This dual approach highlighted the versatility of MLflow in accommodating different model interaction methods. Embeddings Exploration : By generating and examining sentence embeddings, we glimpsed the transformative potential of transformer models in capturing semantic information from text.  Looking Ahead Expanding Horizons : While this tutorial focused on the foundational aspects of Sentence Transformers and MLflow, there\u00e2\u0080\u0099s a whole world of advanced applications waiting to be explored. From semantic similarity analysis to paraphrase mining, the potential use cases are vast and varied. Continued Learning : We strongly encourage you to delve into the other tutorials in this series, which dive deeper into more intriguing use cases like similarity analysis, semantic search, and paraphrase mining. These tutorials will provide you with a broader understanding and more practical applications of Sentence Transformers in various NLP tasks. ",
        "id": "16ff0551fbb87afaab849ce4bb042c48"
    },
    {
        "text": " Final Thoughts The journey into NLP with Sentence Transformers and MLflow is just beginning. With the skills and insights gained from this tutorial, you are well-equipped to explore more complex and exciting applications. The integration of advanced NLP models with MLflow\u00e2\u0080\u0099s robust management and deployment capabilities opens up new avenues for innovation and exploration in the field of language understanding and beyond. Thank you for joining us on this introductory journey, and we look forward to seeing how you apply these tools and concepts in your NLP endeavors! Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "94a2c0ad8b75c8b650e9d44490f48e02"
    },
    {
        "text": "Introduction to Advanced Semantic Similarity Analysis with Sentence Transformers and MLflow 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor MLflow OpenAI Flavor MLflow Sentence-Transformers Flavor Introduction What makes this Library so Special? Features Getting Started with the MLflow Sentence Transformers Flavor - Tutorials and Guides Detailed Documentation Learning More About Sentence Transformers MLflow LangChain Flavor MLflow LlamaIndex Flavor MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow Sentence-Transformers Flavor Introduction to Advanced Semantic Similarity Analysis with Sentence Transformers and MLflow  Introduction to Advanced Semantic Similarity Analysis with Sentence Transformers and MLflow Dive into advanced Semantic Similarity Analysis using Sentence Transformers and MLflow in this comprehensive tutorial. Download this Notebook  Learning Objectives Configure sentence-transformers for semantic similarity analysis. Explore custom PythonModel implementation in MLflow. Log models and manage configurations with MLflow. Deploy and apply models for inference using MLflow\u00e2\u0080\u0099s features.  Unveiling the Power of Sentence Transformers for NLP Sentence Transformers, specialized adaptations of transformer models, excel in producing semantically rich sentence embeddings. Ideal for semantic search and similarity analysis, these models bring a deeper semantic understanding to NLP tasks. #### MLflow: Pioneering Flexible Model Management and Deployment MLflow\u00e2\u0080\u0099s integration with Sentence Transformers introduces enhanced experiment tracking and flexible model management, crucial for NLP projects. Learn to implement a custom PythonModel within MLflow, extending functionalities for unique requirements. Throughout this tutorial, you\u00e2\u0080\u0099ll gain hands-on experience in managing and deploying sophisticated NLP models with MLflow, enhancing your skills in semantic similarity analysis and model lifecycle management. [1]: # Disable tokenizers warnings when constructing pipelines % env TOKENIZERS_PARALLELISM=false import warnings # Disable a few less-than-useful UserWarnings from setuptools and pydantic warnings . filterwarnings ( \"ignore\" , category = UserWarning ) env: TOKENIZERS_PARALLELISM=false  Implementing a Custom SimilarityModel with MLflow Discover how to create a custom SimilarityModel class using MLflow\u00e2\u0080\u0099s PythonModel to assess semantic similarity between sentences.  Overview of SimilarityModel The SimilarityModel is a tailored Python class that leverages MLflow\u00e2\u0080\u0099s flexible PythonModel interface. It is specifically designed to encapsulate the intricacies of computing semantic similarity between sentence pairs using sophisticated sentence embeddings.  Key Components of the Custom Model Importing Libraries : Essential libraries from MLflow, data handling, and Sentence Transformers are imported to facilitate model functionality. Custom PythonModel - SimilarityModel : The load_context method focuses on efficient and safe model loading, crucial for handling complex models like Sentence Transformers. The predict method, equipped with input type checking and error handling, ensures that the model delivers accurate cosine similarity scores, reflecting semantic correlations. ",
        "id": "e81cffd7d476588896dd74ab1e62d049"
    },
    {
        "text": " Significance of Custom SimilarityModel Flexibility and Customization : The model\u00e2\u0080\u0099s design allows for specialized handling of inputs and outputs, aligning perfectly with unique requirements of semantic similarity tasks. Robust Error Handling : Detailed input type checking guarantees a user-friendly experience, preventing common input errors and ensuring the predictability of model behavior. Efficient Model Loading : The strategic use of the load_context method for model initialization circumvents serialization challenges, ensuring a smooth operational flow. Targeted Functionality : The custom predict method directly computes similarity scores, showcasing the model\u00e2\u0080\u0099s capability to deliver task-specific, actionable insights. This custom SimilarityModel exemplifies the adaptability of MLflow\u00e2\u0080\u0099s PythonModel in crafting bespoke NLP solutions, setting a precedent for similar endeavors in various machine learning projects. [2]: import numpy as np import pandas as pd from sentence_transformers import SentenceTransformer , util import mlflow from mlflow.models.signature import infer_signature from mlflow.pyfunc import PythonModel class SimilarityModel ( PythonModel ): def load_context ( self , context ): \"\"\"Load the model context for inference.\"\"\" from sentence_transformers import SentenceTransformer try : self . model = SentenceTransformer . load ( context . artifacts [ \"model_path\" ]) except Exception as e : raise ValueError ( f \"Error loading model: { e } \" ) def predict ( self , context , model_input , params ): \"\"\"Predict method for comparing similarity between two sentences.\"\"\" from sentence_transformers import util if isinstance ( model_input , pd . DataFrame ): if model_input . shape [ 1 ] != 2 : raise ValueError ( \"DataFrame input must have exactly two columns.\" ) sentence_1 , sentence_2 = model_input . iloc [ 0 , 0 ], model_input . iloc [ 0 , 1 ] elif isinstance ( model_input , dict ): sentence_1 = model_input . get ( \"sentence_1\" ) sentence_2 = model_input . get ( \"sentence_2\" ) if sentence_1 is None or sentence_2 is None : raise ValueError ( \"Both 'sentence_1' and 'sentence_2' must be provided in the input dictionary.\" ) else : raise TypeError ( f \"Unexpected type for model_input: { type ( model_input ) } . Must be either a Dict or a DataFrame.\" ) embedding_1 = self . model . encode ( sentence_1 ) embedding_2 = self . model . encode ( sentence_2 ) return np . array ( util . cos_sim ( embedding_1 , embedding_2 ) . tolist ())  Preparing the Sentence Transformer Model and Signature Explore the essential steps for setting up the Sentence Transformer model for logging and deployment with MLflow.  Loading and Saving the Pre-trained Model Model Initialization : A pre-trained Sentence Transformer model, \"all-MiniLM-L6-v2\" , is loaded for its efficiency in generating high-quality embeddings suitable for diverse NLP tasks. Model Saving : The model is saved locally to /tmp/sbert_model to facilitate easy access by MLflow, a prerequisite for model logging in the platform.  Preparing Input Example and Artifacts Input Example Creation : A DataFrame with sample sentences is prepared, representing typical model inputs and aiding in defining the model\u00e2\u0080\u0099s input format. Defining Artifacts : The saved model\u00e2\u0080\u0099s file path is specified as an artifact in MLflow, an essential step for associating the model with MLflow runs.  Generating Test Output for Signature Test Output Calculation : The cosine similarity between sentence embeddings is computed, providing a practical example of the model\u00e2\u0080\u0099s output. Signature Inference : MLflow\u00e2\u0080\u0099s infer_signature function is utilized to generate a signature that encapsulates the expected input and output formats, reinforcing the model\u00e2\u0080\u0099s operational schema. ",
        "id": "3652517eb95fd78f83291be852f55d70"
    },
    {
        "text": " Importance of These Steps Model Readiness : These preparatory steps ensure the model is primed for efficient logging and seamless deployment within the MLflow ecosystem. Input-Output Contract : The established signature acts as a clear contract, defining the model\u00e2\u0080\u0099s input-output dynamics, pivotal for maintaining consistency and accuracy in deployment scenarios. Having meticulously prepared the Sentence Transformer model and its signature, we are now well-equipped to advance towards its integration and management in MLflow. [3]: # Load a pre-trained sentence transformer model model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) # Create an input example DataFrame input_example = pd . DataFrame ([{ \"sentence_1\" : \"I like apples\" , \"sentence_2\" : \"I like oranges\" }]) # Save the model in the /tmp directory model_directory = \"/tmp/sbert_model\" model . save ( model_directory ) # Define artifacts with the absolute path artifacts = { \"model_path\" : model_directory } # Generate test output for signature test_output = np . array ( util . cos_sim ( model . encode ( input_example [ \"sentence_1\" ][ 0 ]), model . encode ( input_example [ \"sentence_2\" ][ 0 ]) ) . tolist () ) # Define the signature associated with the model signature = infer_signature ( input_example , test_output ) # Visualize the signature signature [3]: inputs:\n  ['sentence_1': string, 'sentence_2': string]\noutputs:\n  [Tensor('float64', (-1, 1))]\nparams:\n  None  Creating an experiment We create a new MLflow Experiment so that the run we\u00e2\u0080\u0099re going to log our model to does not log to the default experiment and instead has its own contextually relevant entry. [4]: # If you are running this tutorial in local mode, leave the next line commented out. # Otherwise, uncomment the following line and set your tracking uri to your local or remote tracking server. # mlflow.set_tracking_uri(\"http://127.0.0.1:8080\") mlflow . set_experiment ( \"Semantic Similarity\" ) [4]: <Experiment: artifact_location='file:///Users/benjamin.wilson/repos/mlflow-fork/mlflow/docs/source/llms/sentence-transformers/tutorials/semantic-similarity/mlruns/577235153137414660', creation_time=1701280997564, experiment_id='577235153137414660', last_update_time=1701280997564, lifecycle_stage='active', name='Semantic Similarity', tags={}>  Logging the Custom Model with MLflow Learn how to log the custom SimilarityModel with MLflow for effective model management and deployment.  Creating a Path for the PyFunc Model We establish pyfunc_path , a temporary storage location for the Python model. This path is crucial for MLflow to serialize and store the model effectively.  Logging the Model in MLflow Initiating MLflow Run : An MLflow run is started, encapsulating all model logging processes within a structured framework. Model Logging Details : The model is identified as \"similarity\" , providing a clear reference for future model retrieval and analysis. An instance of SimilarityModel is logged, encapsulating the Sentence Transformer model and similarity prediction logic. An illustrative DataFrame demonstrates the expected model input format, aiding in user comprehension and model usability. The inferred signature, detailing the input-output schema, is included, reinforcing the correct usage of the\nmodel. The artifacts dictionary specifies the location of the serialized Sentence Transformer model, crucial for model reconstruction. Dependencies like sentence_transformers and numpy are listed, ensuring the model\u00e2\u0080\u0099s functional integrity in varied deployment environments. ",
        "id": "7e617aa95aaf9e67e22a37a1b910b7cb"
    },
    {
        "text": " Significance of Model Logging Model Tracking and Versioning : Logging facilitates comprehensive tracking and effective versioning, enhancing model lifecycle management. Reproducibility and Deployment : The logged model, complete with its dependencies, input example, and signature, becomes easily reproducible and deployable, promoting consistent application across environments. Having logged our SimilarityModel in MLflow, it stands ready for advanced applications such as comparative analysis, version management, and deployment for practical inference use cases. [5]: pyfunc_path = \"/tmp/sbert_pyfunc\" with mlflow . start_run () as run : model_info = mlflow . pyfunc . log_model ( \"similarity\" , python_model = SimilarityModel (), input_example = input_example , signature = signature , artifacts = artifacts , pip_requirements = [ \"sentence_transformers\" , \"numpy\" ], ) 2023/11/30 16:10:34 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false  Model Inference and Testing Similarity Prediction Demonstrate the use of the SimilarityModel to compute semantic similarity between sentences after logging it with MLflow.  Loading the Model for Inference Loading with MLflow : Utilize mlflow.pyfunc.load_model with the model\u00e2\u0080\u0099s URI to load the custom SimilarityModel for inference. Model Readiness : The loaded model, named loaded_dynamic , is equipped with the logic defined in the SimilarityModel and is ready to compute similarities.  Preparing Data for Similarity Prediction Creating Input Data : Construct a DataFrame, similarity_data , with pairs of sentences for which similarity will be computed, showcasing the model\u00e2\u0080\u0099s input flexibility.  Computing and Displaying Similarity Score Predicting Similarity : Invoke the predict method on loaded_dynamic with similarity_data to calculate the cosine similarity between sentence embeddings. Interpreting the Result : The resulting similarity_score numerically represents the semantic similarity, offering immediate insights into the model\u00e2\u0080\u0099s output.  Importance of This Testing Model Validation : Confirm the custom model\u00e2\u0080\u0099s expected behavior when predicting on new data, ensuring its validity. Practical Application : Highlight the model\u00e2\u0080\u0099s practical utility in real-world scenarios, demonstrating its capability in semantic similarity analysis. [6]: # Load our custom semantic similarity model implementation by providing the uri that the model was logged to loaded_dynamic = mlflow . pyfunc . load_model ( model_info . model_uri ) # Create an evaluation test DataFrame similarity_data = pd . DataFrame ([{ \"sentence_1\" : \"I like apples\" , \"sentence_2\" : \"I like oranges\" }]) # Verify that the model generates a reasonable prediction similarity_score = loaded_dynamic . predict ( similarity_data ) print ( f \"The similarity between these sentences is: { similarity_score } \" ) The similarity between these sentences is: [[0.63414472]]  Evaluating Semantic Similarity with Distinct Text Pairs Explore the model\u00e2\u0080\u0099s capability to discern varying degrees of semantic similarity with carefully chosen text pairs.  Selection of Text Pairs Low Similarity Pair : Diverse themes in sentences predict a low similarity score, showcasing the model\u00e2\u0080\u0099s ability to recognize contrasting semantic contents. High Similarity Pair : Sentences with similar themes and tones anticipate a high similarity score, demonstrating the model\u00e2\u0080\u0099s semantic parallel detection.  sBERT Model\u00e2\u0080\u0099s Role in Similarity Calculation Semantic Understanding : Utilizing sBERT to encode semantic essence into vectors. Cosine Similarity : Calculating similarity scores to quantify semantic closeness.  Computing and Displaying Similarity Scores Predicting for Low Similarity Pair : Observing the model\u00e2\u0080\u0099s interpretation of semantically distant sentences. Predicting for High Similarity Pair : Assessing the model\u00e2\u0080\u0099s ability to detect semantic similarities in contextually related sentences. ",
        "id": "2e9c45ec5a3bf4125e8f3d6b022791b2"
    },
    {
        "text": " Why This Matters Model Validation : These tests affirm the model\u00e2\u0080\u0099s nuanced language understanding and semantic relationship quantification. Practical Implications : Insights from the model\u00e2\u0080\u0099s processing of semantic content inform applications in content recommendation, information retrieval, and text comparison. [7]: low_similarity = { \"sentence_1\" : \"The explorer stood at the edge of the dense rainforest, \" \"contemplating the journey ahead. The untamed wilderness was \" \"a labyrinth of exotic plants and unknown dangers, a challenge \" \"for even the most seasoned adventurer, brimming with the \" \"prospect of new discoveries and uncharted territories.\" , \"sentence_2\" : \"To install the software, begin by downloading the latest \" \"version from the official website. Once downloaded, run the \" \"installer and follow the on-screen instructions. Ensure that \" \"your system meets the minimum requirements and agree to the \" \"license terms to complete the installation process successfully.\" , } high_similarity = { \"sentence_1\" : \"Standing in the shadow of the Great Pyramids of Giza, I felt a \" \"profound sense of awe. The towering structures, a testament to \" \"ancient ingenuity, rose majestically against the clear blue sky. \" \"As I walked around the base of the pyramids, the intricate \" \"stonework and sheer scale of these wonders of the ancient world \" \"left me speechless, enveloped in a deep sense of history.\" , \"sentence_2\" : \"My visit to the Great Pyramids of Giza was an unforgettable \" \"experience. Gazing upon these monumental structures, I was \" \"captivated by their grandeur and historical significance. Each \" \"step around these ancient marvels filled me with a deep \" \"appreciation for the architectural prowess of a civilization long \" \"gone, yet still speaking through these timeless monuments.\" , } # Validate that semantically unrelated texts return a low similarity score low_similarity_score = loaded_dynamic . predict ( low_similarity ) print ( f \"The similarity score for the 'low_similarity' pair is: { low_similarity_score } \" ) # Validate that semantically similar texts return a high similarity score high_similarity_score = loaded_dynamic . predict ( high_similarity ) print ( f \"The similarity score for the 'high_similarity' pair is: { high_similarity_score } \" ) The similarity score for the 'low_similarity' pair is: [[-0.00052751]]\nThe similarity score for the 'high_similarity' pair is: [[0.83703309]]  Conclusion: Harnessing the Power of Custom MLflow Python Functions in NLP As we conclude this tutorial, let\u00e2\u0080\u0099s recap the significant strides we\u00e2\u0080\u0099ve made in understanding and applying advanced NLP techniques using Sentence Transformers and MLflow.  Key Takeaways from the Tutorial Versatile NLP Modeling : We explored how to harness the advanced capabilities of Sentence Transformers for semantic similarity analysis, a critical task in many NLP applications. Custom MLflow Python Function : The implementation of the custom SimilarityModel in MLflow demonstrated the power and flexibility of using Python functions to extend and adapt the functionality of pre-trained models to suit specific project needs. Model Management and Deployment : We delved into the process of logging, managing, and deploying these models with MLflow, showcasing how MLflow streamlines these aspects of the machine learning lifecycle. Practical Semantic Analysis : Through hands-on examples, we demonstrated the model\u00e2\u0080\u0099s ability to discern varying degrees of semantic similarity between sentence pairs, validating its effectiveness in real-world semantic analysis tasks. ",
        "id": "b9643e4921061f380f00b7188ae5b831"
    },
    {
        "text": " The Power and Flexibility of MLflow\u00e2\u0080\u0099s Python Functions Customization for Specific Needs : One of the tutorial\u00e2\u0080\u0099s highlights is the demonstration of how MLflow\u00e2\u0080\u0099s PythonModel can be customized. This customization is not only powerful but also necessary for tailoring models to specific NLP tasks that go beyond standard model functionalities. Adaptability and Extension : The PythonModel framework in MLflow provides a solid foundation for implementing a wide range of NLP models. Its adaptability allows for the extension of base model functionalities, such as transforming a sentence embedding model into a semantic similarity comparison tool.  Empowering Advanced NLP Applications Ease of Modification : The tutorial showcased that modifying the provided PythonModel implementation for different flavors in MLflow can be done with relative ease, empowering you to create models that align precisely with your project\u00e2\u0080\u0099s requirements. Wide Applicability : Whether it\u00e2\u0080\u0099s semantic search, content recommendation, or automated text comparison, the approach outlined in this tutorial can be adapted to a broad spectrum of NLP tasks, opening doors to innovative applications in the field.  Moving Forward Armed with the knowledge and skills acquired in this tutorial, you are now well-equipped to apply these advanced NLP techniques in your projects. The seamless integration of Sentence Transformers with MLflow\u00e2\u0080\u0099s robust model management and deployment capabilities paves the way for developing sophisticated, efficient, and effective NLP solutions. Thank you for joining us on this journey through advanced NLP modeling with Sentence Transformers and MLflow. We hope this tutorial has inspired you to explore further and innovate in your NLP endeavors! Happy Modeling! Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "65356b3f532b65efcce365ff1a144d98"
    },
    {
        "text": "Advanced Semantic Search with Sentence Transformers and MLflow 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor MLflow OpenAI Flavor MLflow Sentence-Transformers Flavor Introduction What makes this Library so Special? Features Getting Started with the MLflow Sentence Transformers Flavor - Tutorials and Guides Detailed Documentation Learning More About Sentence Transformers MLflow LangChain Flavor MLflow LlamaIndex Flavor MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow Sentence-Transformers Flavor Advanced Semantic Search with Sentence Transformers and MLflow  Advanced Semantic Search with Sentence Transformers and MLflow Embark on a hands-on journey exploring Advanced Semantic Search using Sentence Transformers and MLflow. Download this Notebook  What You Will Learn Implement advanced semantic search with sentence-transformers . Customize MLflow\u00e2\u0080\u0099s PythonModel for unique project requirements. Manage and log models within MLflow\u00e2\u0080\u0099s ecosystem. Deploy complex models for practical applications using MLflow.  Understanding Semantic Search Semantic search transcends keyword matching, using language nuances and context to find relevant results. This advanced approach reflects human language understanding, considering the varied meanings of words in different scenarios.  Harnessing Power of Sentence Transformers for Search Sentence Transformers, specialized for context-rich sentence embeddings, transform search queries and text corpora into semantic vectors. This enables the identification of semantically similar entries, a cornerstone of semantic search.  MLflow: A Vanguard in Model Management and Deployment MLflow enhances NLP projects with efficient experiment logging and customizable model environments. It brings efficiency to experiment tracking and adds a layer of customization, vital for unique NLP tasks. Join us in this tutorial to master advanced semantic search techniques and discover how MLflow can revolutionize your approach to NLP model deployment and management. [1]: import warnings # Disable a few less-than-useful UserWarnings from setuptools and pydantic warnings . filterwarnings ( \"ignore\" , category = UserWarning )  Understanding the Semantic Search Model with MLflow and Sentence Transformers Delve into the intricacies of the SemanticSearchModel , a custom implementation for semantic search using MLflow and Sentence Transformers.  MLflow and Custom PyFunc Models MLflow\u00e2\u0080\u0099s custom Python function ( pyfunc ) models provide a flexible and deployable solution for integrating complex logic, ideal for our SemanticSearchModel .  The Model\u00e2\u0080\u0099s Core Functionalities Context Loading : Essential for initializing the Sentence Transformer model and preparing the corpus for semantic comparison. Predict Method : The central function for semantic search, encompassing input validation, query encoding, and similarity computation. ",
        "id": "cdaa990f4f5a08ebaf0b194fcf264470"
    },
    {
        "text": " Detailed Breakdown of Predict Method Input Validation : Ensures proper format and extraction of the query sentence. Query Encoding : Converts the query into an embedding for comparison. Cosine Similarity Computation : Determines the relevance of each corpus entry to the query. Top Results Extraction : Identifies the most relevant entries based on similarity scores. Relevancy Filtering : Filters results based on a minimum relevancy threshold, enhancing practical usability. Warning Mechanism : Issues a warning if all top results are below the relevancy threshold, ensuring a result is always provided.  Conclusion This semantic search model exemplifies the integration of NLP with MLflow, showcasing flexibility, user-friendliness, and practical application in modern machine learning workflows. [2]: import warnings import numpy as np import pandas as pd from sentence_transformers import SentenceTransformer , util import mlflow from mlflow.models.signature import infer_signature from mlflow.pyfunc import PythonModel class SemanticSearchModel ( PythonModel ): def load_context ( self , context ): \"\"\"Load the model context for inference, including the corpus from a file.\"\"\" try : # Load the pre-trained sentence transformer model self . model = SentenceTransformer . load ( context . artifacts [ \"model_path\" ]) # Load the corpus from the specified file corpus_file = context . artifacts [ \"corpus_file\" ] with open ( corpus_file ) as file : self . corpus = file . read () . splitlines () # Encode the corpus and convert it to a tensor self . corpus_embeddings = self . model . encode ( self . corpus , convert_to_tensor = True ) except Exception as e : raise ValueError ( f \"Error loading model and corpus: { e } \" ) def predict ( self , context , model_input , params = None ): \"\"\"Predict method to perform semantic search over the corpus.\"\"\" if isinstance ( model_input , pd . DataFrame ): if model_input . shape [ 1 ] != 1 : raise ValueError ( \"DataFrame input must have exactly one column.\" ) model_input = model_input . iloc [ 0 , 0 ] elif isinstance ( model_input , dict ): model_input = model_input . get ( \"sentence\" ) if model_input is None : raise ValueError ( \"The input dictionary must have a key named 'sentence'.\" ) else : raise TypeError ( f \"Unexpected type for model_input: { type ( model_input ) } . Must be either a Dict or a DataFrame.\" ) # Encode the query query_embedding = self . model . encode ( model_input , convert_to_tensor = True ) # Compute cosine similarity scores cos_scores = util . cos_sim ( query_embedding , self . corpus_embeddings )[ 0 ] # Determine the number of top results to return top_k = params . get ( \"top_k\" , 3 ) if params else 3 # Default to 3 if not specified minimum_relevancy = ( params . get ( \"minimum_relevancy\" , 0.2 ) if params else 0.2 ) # Default to 0.2 if not specified # Get the top_k most similar sentences from the corpus top_results = np . argsort ( cos_scores , axis = 0 )[ - top_k :] # Prepare the initial results list initial_results = [ ( self . corpus [ idx ], cos_scores [ idx ] . item ()) for idx in reversed ( top_results ) ] # Filter the results based on the minimum relevancy threshold filtered_results = [ result for result in initial_results if result [ 1 ] >= minimum_relevancy ] # If all results are below the threshold, issue a warning and return the top result if not filtered_results : warnings . warn ( \"All top results are below the minimum relevancy threshold. \" \"Returning the highest match instead.\" , RuntimeWarning , ) return [ initial_results [ 0 ]] else : return filtered_results  Building and Preparing the Semantic Search Corpus Explore constructing and preparing the corpus for the semantic search model, a critical component for search functionality.  Simulating a Real-World Use Case We create a simplified corpus of synthetic blog posts to demonstrate the model\u00e2\u0080\u0099s core functionality, replicating a scaled-down version of a typical real-world scenario. ",
        "id": "f8abc25251003b602a9baf2d4e52906e"
    },
    {
        "text": " Key Steps in Corpus Preparation Corpus Creation : Formation of a list representing individual blog post entries. Writing to a File : Saving the corpus to a text file, mimicking the process of data extraction and preprocessing in a real application.  Efficient Data Handling for Scalability Our model encodes the corpus into embeddings for rapid comparison, demonstrating an efficient approach suitable for scaling to larger datasets.  Production Considerations Storing Embeddings : Discusses options for efficient storage and retrieval of embeddings, crucial in large-scale applications. Scalability : Highlights the importance of scalable storage systems for handling extensive datasets and complex queries. Updating the Corpus : Outlines strategies for managing and updating the corpus in dynamic, evolving use cases. ",
        "id": "9f20613634f999925840711e533808df"
    },
    {
        "text": " Realizing the Semantic Search Concept This setup, while simplified, reflects the essential steps for developing a robust and scalable semantic search system, combining NLP techniques with efficient data management. In a real production use-case, the processing of a corpus (creating embeddings) would be an external process to that which is running the semantic search. The corpus example below is intended to showcase functionality solely for the purposes of demonstration. [3]: corpus = [ \"Perfecting a Sourdough Bread Recipe: The Joy of Baking. Baking sourdough bread \" \"requires patience, skill, and a good understanding of yeast fermentation. Each \" \"loaf is unique, telling its own story of the baker's journey.\" , \"The Mars Rover's Discoveries: Unveiling the Red Planet. NASA's Mars rover has \" \"sent back stunning images and data, revealing the planet's secrets. These \" \"discoveries may hold the key to understanding Mars' history.\" , \"The Art of Growing Herbs: Enhancing Your Culinary Skills. Growing your own \" \"herbs can transform your cooking, adding fresh and vibrant flavors. Whether it's \" \"basil, thyme, or rosemary, each herb has its own unique characteristics.\" , \"AI in Software Development: Transforming the Tech Landscape. The rapid \" \"advancements in artificial intelligence are reshaping how we approach software \" \"development. From automation to machine learning, the possibilities are endless.\" , \"Backpacking Through Europe: A Journey of Discovery. Traveling across Europe by \" \"backpack allows one to immerse in diverse cultures and landscapes. It's an \" \"adventure that combines the thrill of exploration with personal growth.\" , \"Shakespeare's Timeless Influence: Reshaping Modern Storytelling. The works of \" \"William Shakespeare continue to inspire and influence contemporary literature. \" \"His mastery of language and deep understanding of human nature are unparalleled.\" , \"The Rise of Renewable Energy: A Sustainable Future. Embracing renewable energy \" \"is crucial for achieving a sustainable and environmentally friendly lifestyle. \" \"Solar, wind, and hydro power are leading the way in this green revolution.\" , \"The Magic of Jazz: An Exploration of Sound and Harmony. Jazz music, known for \" \"its improvisation and complex harmonies, has a rich and diverse history. It \" \"evokes a range of emotions, often reflecting the soul of the musician.\" , \"Yoga for Mind and Body: The Benefits of Regular Practice. Engaging in regular \" \"yoga practice can significantly improve flexibility, strength, and mental \" \"well-being. It's a holistic approach to health, combining physical and spiritual \" \"aspects.\" , \"The Egyptian Pyramids: Monuments of Ancient Majesty. The ancient Egyptian \" \"pyramids, monumental tombs for pharaohs, are marvels of architectural \" \"ingenuity. They stand as a testament to the advanced skills of ancient builders.\" , \"Vegan Cuisine: A World of Flavor. Exploring vegan cuisine reveals a world of \" \"nutritious and delicious possibilities. From hearty soups to delectable desserts, \" \"plant-based dishes are diverse and satisfying.\" , \"Extraterrestrial Life: The Endless Search. The quest to find life beyond Earth \" \"continues to captivate scientists and the public alike. Advances in space \" \"technology are bringing us closer to answering this age-old question.\" , \"The Art of Plant Pruning: Promoting Healthy Growth. Regular pruning is essential \" \"for maintaining healthy and vibrant plants. It's not just about cutting back, but \" \"understanding each plant's growth patterns and needs.\" , \"Cybersecurity in the Digital Age: Protecting Our Data. With the rise of digital \" \"technology, cybersecurity has become a critical concern. Protecting sensitive \" \"information from cyber threats is an ongoing challenge for individuals and \" \"businesses alike.\" , \"The Great Wall of China: A Historical Journey. Visiting the Great Wall offers \" \"more than just breathtaking views; it's a journey through history. This ancient \" \"structure tells stories of empires, invasions, and human resilience.\" , \"Mystery Novels: Crafting Suspense and Intrigue. A great mystery novel captivates \" \"the reader with intricate plots and unexpected twists. It's a genre that combines \" \"intellectual challenge with entertainment.\" , \"Conserving Endangered Species: A Global Effort. Protecting endangered spe",
        "id": "737830d1063d8e7c4ebbd3610bd1859f"
    },
    {
        "text": "y. Visiting the Great Wall offers \" \"more than just breathtaking views; it's a journey through history. This ancient \" \"structure tells stories of empires, invasions, and human resilience.\" , \"Mystery Novels: Crafting Suspense and Intrigue. A great mystery novel captivates \" \"the reader with intricate plots and unexpected twists. It's a genre that combines \" \"intellectual challenge with entertainment.\" , \"Conserving Endangered Species: A Global Effort. Protecting endangered species \" \"is a critical task that requires international collaboration. From rainforests to \" \"oceans, every effort counts in preserving our planet's biodiversity.\" , \"Emotions in Classical Music: A Symphony of Feelings. Classical music is not just \" \"an auditory experience; it's an emotional journey. Each composition tells a story, \" \"conveying feelings from joy to sorrow, tranquility to excitement.\" , \"CrossFit: A Test of Strength and Endurance. CrossFit is more than just a fitness \" \"regimen; it's a lifestyle that challenges your physical and mental limits. It \" \"combines various disciplines to create a comprehensive workout.\" , \"The Renaissance: An Era of Artistic Genius. The Renaissance marked a period of \" \"extraordinary artistic and scientific achievements. It was a time when creativity \" \"and innovation flourished, reshaping the course of history.\" , \"Exploring International Cuisines: A Culinary Adventure. Discovering international \" \"cuisines is an adventure for the palate. Each dish offers a glimpse into the \" \"culture and traditions of its origin.\" , \"Astronaut Training: Preparing for the Unknown. Becoming an astronaut involves \" \"rigorous training to prepare for the extreme conditions of space. It's a journey \" \"that tests both physical endurance and mental resilience.\" , \"Sustainable Gardening: Nurturing the Environment. Sustainable gardening is not \" \"just about growing plants; it's about cultivating an ecosystem. By embracing \" \"environmentally friendly practices, gardeners can have a positive impact on the \" \"planet.\" , \"The Smartphone Revolution: Changing Communication. Smartphones have transformed \" \"how we communicate, offering unprecedented connectivity and convenience. This \" \"technology continues to evolve, shaping our daily interactions.\" , \"Experiencing African Safaris: Wildlife and Wilderness. An African safari is an \" \"unforgettable experience that brings you face-to-face with the wonders of \" \"wildlife. It's a journey that connects you with the raw beauty of nature.\" , \"Graphic Novels: A Blend of Art and Story. Graphic novels offer a unique medium \" \"where art and narrative intertwine to tell compelling stories. They challenge \" \"traditional forms of storytelling, offering visual and textual richness.\" , \"Addressing Ocean Pollution: A Call to Action. The increasing levels of pollution \" \"in our oceans are a pressing environmental concern. Protecting marine life and \" \"ecosystems requires concerted global efforts.\" , \"The Origins of Hip Hop: A Cultural Movement. Hip hop music, originating from the \" \"streets of New York, has grown into a powerful cultural movement. Its beats and \" \"lyrics reflect the experiences and voices of a community.\" , \"Swimming: A Comprehensive Workout. Swimming offers a full-body workout that is \" \"both challenging and refreshing. It's an exercise that enhances cardiovascular \" \"health, builds muscle, and improves endurance.\" , \"The Fall of the Berlin Wall: A Historical Turning Point. The fall of the Berlin \" \"Wall was not just a physical demolition; it was a symbol of political and social \" \"change. This historic event marked the end of an era and the beginning of a new \" \"chapter in world history.\" , ] # Write the corpus to a file corpus_file = \"/tmp/search_corpus.txt\" with open ( corpus_file , \"w\" ) as file : for sentence in corpus : file . write ( sentence + \" \\n \" ) ",
        "id": "e3eab9c4e7d0d681d87750394d5bb5b0"
    },
    {
        "text": " Model Preparation and Configuration in MLflow Explore the steps to prepare and configure the Sentence Transformer model for integration with MLflow, essential for deployment readiness.  Loading and Saving the Sentence Transformer Model Model Initialization : Loading the \"all-MiniLM-L6-v2\" model, known for its balance in performance and speed, suitable for semantic search tasks. Model Storage : Saving the model to a directory, essential for later deployment via MLflow. The choice of /tmp/search_model is for tutorial convenience so that your current working directory is not filled with the model files. You can change this to any location of your choosing.  Preparing Model Artifacts and Signature Artifacts Dictionary : Creating a dictionary with paths to model and corpus file, guiding MLflow to the components that are required to initialize the custom model object. Input Example and Test Output : Defining sample input and output to illustrate the model\u00e2\u0080\u0099s expected data formats. Model Signature : Using infer_signature for automatic signature generation, encompassing input, output, and operational parameters.  Importance of the Model Signature The signature ensures data consistency between training and deployment, enhancing model usability and reducing error potential. Having a signature specified ensures that type validation occurs at inference time, preventing unexpected behavior with invalid type conversions that could render incorrect or confusing inference results.  Conclusion This comprehensive preparation process guarantees the model is deployment-ready, with all dependencies and operational requirements explicitly defined. [4]: # Load a pre-trained sentence transformer model model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) # Create an input example DataFrame input_example = [ \"Something I want to find matches for.\" ] # Save the model in the /tmp directory model_directory = \"/tmp/search_model\" model . save ( model_directory ) artifacts = { \"model_path\" : model_directory , \"corpus_file\" : corpus_file } # Generate test output for signature test_output = [ \"match 1\" , \"match 2\" , \"match 3\" ] # Define the signature associated with the model signature = infer_signature ( input_example , test_output , params = { \"top_k\" : 3 , \"minimum_relevancy\" : 0.2 } ) # Visualize the signature signature [4]: inputs:\n  [string]\noutputs:\n  [string]\nparams:\n  ['top_k': long (default: 3), 'minimum_relevancy': double (default: 0.2)]  Creating an experiment We create a new MLflow Experiment so that the run we\u00e2\u0080\u0099re going to log our model to does not log to the default experiment and instead has its own contextually relevant entry. [5]: # If you are running this tutorial in local mode, leave the next line commented out. # Otherwise, uncomment the following line and set your tracking uri to your local or remote tracking server. # mlflow.set_tracking_uri(\"http://127.0.0.1:8080\") mlflow . set_experiment ( \"Semantic Similarity\" ) [5]: <Experiment: artifact_location='file:///Users/benjamin.wilson/repos/mlflow-fork/mlflow/docs/source/llms/sentence-transformers/tutorials/semantic-search/mlruns/405641275158666585', creation_time=1701278766302, experiment_id='405641275158666585', last_update_time=1701278766302, lifecycle_stage='active', name='Semantic Similarity', tags={}>  Logging the Model with MLflow Discover the process of logging the model in MLflow, a crucial step for managing and deploying the model within the MLflow framework.  Starting an MLflow Run Context Management : Initiating an MLflow run using with mlflow.start_run() , essential for tracking and managing model-related operations.  Logging the Model Model Logging : Utilizing mlflow.pyfunc.log_model to log the custom SemanticSearchModel , including key arguments like model name, instance, input example, signature, artifacts, and requirements. ",
        "id": "bfaf8501d346ad1c9c408fdec416a5b6"
    },
    {
        "text": " Outcome of Model Logging Model Registration : Ensures the model is registered with all necessary components in MLflow, ready for deployment. Reproducibility and Traceability : Facilitates consistent model deployment and tracks versioning and associated data.  Conclusion Completing this critical step transitions the model from development to a deployment-ready state, encapsulated within the MLflow ecosystem. [6]: with mlflow . start_run () as run : model_info = mlflow . pyfunc . log_model ( \"semantic_search\" , python_model = SemanticSearchModel (), input_example = input_example , signature = signature , artifacts = artifacts , pip_requirements = [ \"sentence_transformers\" , \"numpy\" ], ) 2023/11/30 15:57:53 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false  Model Inference and Prediction Demonstration Observe the practical application of our semantic search model, demonstrating its ability to respond to user queries with relevant predictions.  Loading the Model for Inference Model Loading : Utilizing mlflow.pyfunc.load_model to load the model, preparing it to process semantic search queries.  Making a Prediction Running a Query : Passing a sample query to the loaded model, demonstrating its semantic search capability.  Understanding the Prediction Output Output Format : Analysis of the prediction output, showcasing the model\u00e2\u0080\u0099s semantic understanding through relevance scores. Example Results : Illustrating the model\u00e2\u0080\u0099s results, including relevance scores for various query-related entries.  Conclusion This demonstration underscores the model\u00e2\u0080\u0099s efficacy in semantic search, highlighting its potential in recommendation and knowledge retrieval applications. [7]: # Load our model as a PyFuncModel. # Note that unlike the example shown in the Introductory Tutorial, there is no 'native' flavor for PyFunc models. # This model cannot be loaded with `mlflow.sentence_transformers.load_model()` because it is not in the native model format. loaded_dynamic = mlflow . pyfunc . load_model ( model_info . model_uri ) # Make sure that it generates a reasonable output loaded_dynamic . predict ([ \"I'd like some ideas for a meal to cook.\" ]) [7]: [('Exploring International Cuisines: A Culinary Adventure. Discovering international cuisines is an adventure for the palate. Each dish offers a glimpse into the culture and traditions of its origin.',\n  0.43857115507125854),\n ('Vegan Cuisine: A World of Flavor. Exploring vegan cuisine reveals a world of nutritious and delicious possibilities. From hearty soups to delectable desserts, plant-based dishes are diverse and satisfying.',\n  0.34688490629196167),\n (\"The Art of Growing Herbs: Enhancing Your Culinary Skills. Growing your own herbs can transform your cooking, adding fresh and vibrant flavors. Whether it's basil, thyme, or rosemary, each herb has its own unique characteristics.\",\n  0.22686949372291565)]  Advanced Query Handling with Customizable Parameters and Warning Mechanism Explore the model\u00e2\u0080\u0099s advanced features, including customizable search parameters and a unique warning mechanism for optimal user experience.  Executing a Customized Prediction with Warnings Customized Query with Challenging Parameters : Testing the model\u00e2\u0080\u0099s ability to discern highly relevant content with a high relevancy threshold query. Triggering the Warning : A mechanism to alert users when search criteria are too restrictive, enhancing user feedback.  Understanding the Model\u00e2\u0080\u0099s Response Result in Challenging Scenarios : Analyzing the model\u00e2\u0080\u0099s response to stringent search criteria, including cases where the relevancy threshold is not met.  Implications and Best Practices Balancing Relevancy and Coverage : Discussing the importance of setting appropriate relevancy thresholds to ensure a balance between precision and result coverage. User Feedback for Corpus Improvement : Utilizing warnings as feedback for refining the corpus and enhancing the search system. ",
        "id": "dad55eda8921f361ec1c7d1f56803bdc"
    },
    {
        "text": " Conclusion This advanced feature set demonstrates the model\u00e2\u0080\u0099s adaptability and the importance of fine-tuning search parameters for a dynamic and responsive search experience. [8]: # Verify that the fallback logic works correctly by returning the 'best, closest' result, even though the parameters submitted should return no results. # We are also validating that the warning is issued, alerting us to the fact that this behavior is occurring. loaded_dynamic . predict ( [ \"Latest stories on computing\" ], params = { \"top_k\" : 10 , \"minimum_relevancy\" : 0.4 } ) /var/folders/cd/n8n0rm2x53l_s0xv_j_xklb00000gp/T/ipykernel_55915/1325605132.py:71: RuntimeWarning: All top results are below the minimum relevancy threshold. Returning the highest match instead.\n  warnings.warn( [8]: [('AI in Software Development: Transforming the Tech Landscape. The rapid advancements in artificial intelligence are reshaping how we approach software development. From automation to machine learning, the possibilities are endless.',\n  0.2533860206604004)]  Conclusion: Crafting Custom Logic with MLflow\u00e2\u0080\u0099s PythonModel As we wrap up this tutorial, let\u00e2\u0080\u0099s reflect on the key learnings and the powerful capabilities of MLflow\u00e2\u0080\u0099s PythonModel in crafting custom logic for real-world applications, particularly when integrating advanced libraries like sentence-transformers .  Key Takeaways Flexibility of PythonModel : The PythonModel in MLflow offers unparalleled flexibility in defining custom logic. Throughout this tutorial, we leveraged this to build a semantic search model tailored to our specific requirements. This flexibility proves invaluable when dealing with complex use cases that go beyond standard model implementations. Integration with Sentence Transformers : We seamlessly integrated the sentence-transformers library within our MLflow model. This demonstrated how advanced NLP capabilities can be embedded within custom models to handle sophisticated tasks like semantic search. The use of transformer models for generating embeddings showcased how cutting-edge NLP techniques could be applied in practical scenarios. Customization and User Experience : Our model not only performed the core task of semantic search but also allowed for customizable search parameters ( top_k and minimum_relevancy ). This level of customization is crucial for aligning the model\u00e2\u0080\u0099s output with varying user needs. The inclusion of a warning mechanism further enriched the model by providing valuable feedback, enhancing the user experience. Real-World Application and Scalability : While our tutorial focused on a controlled dataset, the principles and methodologies apply to much larger, real-world datasets. The discussion around using vector databases and in-memory databases like Redis or Elasticsearch for scalability highlighted how the model could be adapted for large-scale applications.  Empowering Real-World Applications The combination of MLflow\u00e2\u0080\u0099s PythonModel and advanced libraries like sentence-transformers simplifies the creation of sophisticated, real-world applications. The ability to encapsulate complex logic, manage dependencies, and ensure model portability makes MLflow an invaluable tool in the modern data scientist\u00e2\u0080\u0099s toolkit.  Moving Forward As we conclude, remember that the journey doesn\u00e2\u0080\u0099t end here. The concepts and techniques explored in this tutorial lay the groundwork for further exploration and innovation in the field of NLP and beyond. We encourage you to take these learnings, experiment with your datasets, and continue pushing the boundaries of what\u00e2\u0080\u0099s possible with MLflow and advanced NLP technologies. Thank you for joining us on this enlightening journey through semantic search with Sentence Transformers and MLflow! Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "24013a2422704bb900375c8dc9145738"
    },
    {
        "text": "Advanced Paraphrase Mining with Sentence Transformers and MLflow 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor MLflow OpenAI Flavor MLflow Sentence-Transformers Flavor Introduction What makes this Library so Special? Features Getting Started with the MLflow Sentence Transformers Flavor - Tutorials and Guides Detailed Documentation Learning More About Sentence Transformers MLflow LangChain Flavor MLflow LlamaIndex Flavor MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow Sentence-Transformers Flavor Advanced Paraphrase Mining with Sentence Transformers and MLflow  Advanced Paraphrase Mining with Sentence Transformers and MLflow Embark on an enriching journey through advanced paraphrase mining using Sentence Transformers, enhanced by MLflow. Download this Notebook  Learning Objectives Apply sentence-transformers for advanced paraphrase mining. Develop a custom PythonModel in MLflow tailored for this task. Effectively manage and track models within the MLflow ecosystem. Deploy paraphrase mining models using MLflow\u00e2\u0080\u0099s deployment capabilities.  Exploring Paraphrase Mining Discover the process of identifying semantically similar but textually distinct sentences, a key aspect in various NLP applications such as document summarization and chatbot development.  The Role of Sentence Transformers in Paraphrase Mining Learn how Sentence Transformers, specialized for generating rich sentence embeddings, are used to capture deep semantic meanings and compare textual content.  MLflow: Simplifying Model Management and Deployment Delve into how MLflow streamlines the process of managing and deploying NLP models, with a focus on efficient tracking and customizable model implementations. Join us to develop a nuanced understanding of paraphrase mining and master the art of managing and deploying NLP models with MLflow. [1]: import warnings # Disable a few less-than-useful UserWarnings from setuptools and pydantic warnings . filterwarnings ( \"ignore\" , category = UserWarning )  Introduction to the Paraphrase Mining Model Initiate the Paraphrase Mining Model, integrating Sentence Transformers and MLflow for advanced NLP tasks.  Overview of the Model Structure Loading Model and Corpus ``load_context`` Method : Essential for loading the Sentence Transformer model and the text corpus for paraphrase identification. Paraphrase Mining Logic ``predict`` Method : Integrates custom logic for input validation and paraphrase mining, offering customizable parameters. Sorting and Filtering Matches ``_sort_and_filter_matches`` Helper Method : Ensures relevant and unique paraphrase identification by sorting and filtering based on similarity scores.  Key Features Advanced NLP Techniques : Utilizes Sentence Transformers for semantic text understanding. Custom Logic Integration : Demonstrates flexibility in model behavior customization. User Customization Options : Allows end users to adjust match criteria for various use cases. Efficiency in Processing : Pre-encodes the corpus for efficient paraphrase mining operations. Robust Error Handling : Incorporates validations for reliable model performance. ",
        "id": "2c8cab9cfeafc43c1bcfed86d034681a"
    },
    {
        "text": " Practical Implications This model provides a powerful tool for paraphrase detection in diverse applications, exemplifying the effective use of custom models within the MLflow framework. [2]: import warnings import pandas as pd from sentence_transformers import SentenceTransformer , util import mlflow from mlflow.models.signature import infer_signature from mlflow.pyfunc import PythonModel class ParaphraseMiningModel ( PythonModel ): def load_context ( self , context ): \"\"\"Load the model context for inference, including the customer feedback corpus.\"\"\" try : # Load the pre-trained sentence transformer model self . model = SentenceTransformer . load ( context . artifacts [ \"model_path\" ]) # Load the customer feedback corpus from the specified file corpus_file = context . artifacts [ \"corpus_file\" ] with open ( corpus_file ) as file : self . corpus = file . read () . splitlines () except Exception as e : raise ValueError ( f \"Error loading model and corpus: { e } \" ) def _sort_and_filter_matches ( self , query : str , paraphrase_pairs : list [ tuple ], similarity_threshold : float ): \"\"\"Sort and filter the matches by similarity score.\"\"\" # Convert to list of tuples and sort by score sorted_matches = sorted ( paraphrase_pairs , key = lambda x : x [ 1 ], reverse = True ) # Filter and collect paraphrases for the query, avoiding duplicates query_paraphrases = {} for score , i , j in sorted_matches : if score < similarity_threshold : continue paraphrase = self . corpus [ j ] if self . corpus [ i ] == query else self . corpus [ i ] if paraphrase == query : continue if paraphrase not in query_paraphrases or score > query_paraphrases [ paraphrase ]: query_paraphrases [ paraphrase ] = score return sorted ( query_paraphrases . items (), key = lambda x : x [ 1 ], reverse = True ) def predict ( self , context , model_input , params = None ): \"\"\"Predict method to perform paraphrase mining over the corpus.\"\"\" # Validate and extract the query input if isinstance ( model_input , pd . DataFrame ): if model_input . shape [ 1 ] != 1 : raise ValueError ( \"DataFrame input must have exactly one column.\" ) query = model_input . iloc [ 0 , 0 ] elif isinstance ( model_input , dict ): query = model_input . get ( \"query\" ) if query is None : raise ValueError ( \"The input dictionary must have a key named 'query'.\" ) else : raise TypeError ( f \"Unexpected type for model_input: { type ( model_input ) } . Must be either a Dict or a DataFrame.\" ) # Determine the minimum similarity threshold similarity_threshold = params . get ( \"similarity_threshold\" , 0.5 ) if params else 0.5 # Add the query to the corpus for paraphrase mining extended_corpus = self . corpus + [ query ] # Perform paraphrase mining paraphrase_pairs = util . paraphrase_mining ( self . model , extended_corpus , show_progress_bar = False ) # Convert to list of tuples and sort by score sorted_paraphrases = self . _sort_and_filter_matches ( query , paraphrase_pairs , similarity_threshold ) # Warning if no paraphrases found if not sorted_paraphrases : warnings . warn ( \"No paraphrases found above the similarity threshold.\" , UserWarning ) return { sentence [ 0 ]: str ( sentence [ 1 ]) for sentence in sorted_paraphrases }  Preparing the Corpus for Paraphrase Mining Set up the foundation for paraphrase mining by creating and preparing a diverse corpus.  Corpus Creation Define a corpus comprising a range of sentences from various topics, including space exploration, AI, gardening, and more. This diversity enables the model to identify paraphrases across a broad spectrum of subjects.  Writing the Corpus to a File The corpus is saved to a file named feedback.txt , mirroring a common practice in large-scale data handling. This step also prepares the corpus for efficient processing within the Paraphrase Mining Model. ",
        "id": "00843c7f8b8f8765d6b89363975f16b6"
    },
    {
        "text": " Significance of the Corpus The corpus serves as the key dataset for the model to find semantically similar sentences. Its variety ensures the model\u00e2\u0080\u0099s adaptability and effectiveness across diverse use cases. [3]: corpus = [ \"Exploring ancient cities in Europe offers a glimpse into history.\" , \"Modern AI technologies are revolutionizing industries.\" , \"Healthy eating contributes significantly to overall well-being.\" , \"Advancements in renewable energy are combating climate change.\" , \"Learning a new language opens doors to different cultures.\" , \"Gardening is a relaxing hobby that connects you with nature.\" , \"Blockchain technology could redefine digital transactions.\" , \"Homemade Italian pasta is a delight to cook and eat.\" , \"Practicing yoga daily improves both physical and mental health.\" , \"The art of photography captures moments in time.\" , \"Baking bread at home has become a popular quarantine activity.\" , \"Virtual reality is creating new experiences in gaming.\" , \"Sustainable travel is becoming a priority for eco-conscious tourists.\" , \"Reading books is a great way to unwind and learn.\" , \"Jazz music provides a rich tapestry of sound and rhythm.\" , \"Marathon training requires discipline and perseverance.\" , \"Studying the stars helps us understand our universe.\" , \"The rise of electric cars is an important environmental development.\" , \"Documentary films offer deep insights into real-world issues.\" , \"Crafting DIY projects can be both fun and rewarding.\" , \"The history of ancient civilizations is fascinating to explore.\" , \"Exploring the depths of the ocean reveals a world of marine wonders.\" , \"Learning to play a musical instrument can be a rewarding challenge.\" , \"Artificial intelligence is shaping the future of personalized medicine.\" , \"Cycling is not only a great workout but also eco-friendly transportation.\" , \"Home automation with IoT devices is enhancing living experiences.\" , \"Understanding quantum computing requires a grasp of complex physics.\" , \"A well-brewed cup of coffee is the perfect start to the day.\" , \"Urban farming is gaining popularity as a sustainable food source.\" , \"Meditation and mindfulness can lead to a more balanced life.\" , \"The popularity of podcasts has revolutionized audio storytelling.\" , \"Space exploration continues to push the boundaries of human knowledge.\" , \"Wildlife conservation is essential for maintaining biodiversity.\" , \"The fusion of technology and fashion is creating new trends.\" , \"E-learning platforms have transformed the educational landscape.\" , \"Dark chocolate has surprising health benefits when enjoyed in moderation.\" , \"Robotics in manufacturing is leading to more efficient production.\" , \"Creating a personal budget is key to financial well-being.\" , \"Hiking in nature is a great way to connect with the outdoors.\" , \"3D printing is innovating the way we create and manufacture objects.\" , \"Sommeliers can identify a wine's characteristics with just a taste.\" , \"Mind-bendin",
        "id": "a135edb50a920e93c363143e376b565e"
    },
    {
        "text": " is leading to more efficient production.\" , \"Creating a personal budget is key to financial well-being.\" , \"Hiking in nature is a great way to connect with the outdoors.\" , \"3D printing is innovating the way we create and manufacture objects.\" , \"Sommeliers can identify a wine's characteristics with just a taste.\" , \"Mind-bending puzzles and riddles are great for cognitive exercise.\" , \"Social media has a profound impact on communication and culture.\" , \"Urban sketching captures the essence of city life on paper.\" , \"The ethics of AI is a growing field in tech philosophy.\" , \"Homemade skincare remedies are becoming more popular.\" , \"Virtual travel experiences can provide a sense of adventure at home.\" , \"Ancient mythology still influences modern storytelling and literature.\" , \"Building model kits is a hobby that requires patience and precision.\" , \"The study of languages opens windows into different worldviews.\" , \"Professional esports has become a major global phenomenon.\" , \"The mysteries of the universe are unveiled through space missions.\" , \"Astronauts' experiences in space stations offer unique insights into life beyond Earth.\" , \"Telescopic observations bring distant galaxies within our view.\" , \"The study of celestial bodies helps us understand the cosmos.\" , \"Space travel advancements could lead to interplanetary exploration.\" , \"Observing celestial events provides valuable data for astronomers.\" , \"The development of powerful rockets is key to deep space exploration.\" , \"Mars rover missions are crucial in searching for extraterrestrial life.\" , \"Satellites play a vital role in our understanding of Earth's atmosphere.\" , \"Astrophysics is central to unraveling the secrets of space.\" , \"Zero gravity environments in space pose unique challenges and opportunities.\" , \"Space tourism might soon become a reality for many.\" , \"Lunar missions have contributed significantly to our knowledge of the moon.\" , \"The International Space Station is a hub for groundbreaking space research.\" , \"Studying comets and asteroids reveals information about the early solar system.\" , \"Advancements in space technology have implications for many scientific fields.\" , \"The possibility of life on other planets continues to intrigue scientists.\" , \"Black holes are among the most mysterious phenomena in space.\" , \"The history of space exploration is filled with remarkable achievements.\" , \"Future space missions could unlock the mysteries of dark matter.\" , ] # Write out the corpus to a file corpus_file = \"/tmp/feedback.txt\" with open ( corpus_file , \"w\" ) as file : for sentence in corpus : file . write ( sentence + \" \\n \" ) ",
        "id": "79d091257aa25ce6288f279e3aaca91e"
    },
    {
        "text": " Setting Up the Paraphrase Mining Model Prepare the Sentence Transformer model for integration with MLflow to harness its paraphrase mining capabilities.  Loading the Sentence Transformer Model Initialize the all-MiniLM-L6-v2 Sentence Transformer model, ideal for generating sentence embeddings suitable for paraphrase mining.  Preparing the Input Example Create a DataFrame as an input example to illustrate the type of query the model will handle, aiding in defining the model\u00e2\u0080\u0099s input structure.  Saving the Model Save the model to /tmp/paraphrase_search_model for portability and ease of loading during deployment with MLflow.  Defining Artifacts and Corpus Path Specify paths to the saved model and corpus as artifacts in MLflow, crucial for model logging and reproduction.  Generating Test Output for Signature Generate a sample output, illustrating the model\u00e2\u0080\u0099s expected output format for paraphrase mining.  Creating the Model Signature Use MLflow\u00e2\u0080\u0099s infer_signature to define the model\u00e2\u0080\u0099s input and output schema, adding the similarity_threshold parameter for inference flexibility. [4]: # Load a pre-trained sentence transformer model model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) # Create an input example DataFrame input_example = pd . DataFrame ({ \"query\" : [ \"This product works well. I'm satisfied.\" ]}) # Save the model in the /tmp directory model_directory = \"/tmp/paraphrase_search_model\" model . save ( model_directory ) # Define the path for the corpus file corpus_file = \"/tmp/feedback.txt\" # Define the artifacts (paths to the model and corpus file) artifacts = { \"model_path\" : model_directory , \"corpus_file\" : corpus_file } # Generate test output for signature # Sample output for paraphrase mining could be a list of tuples (paraphrase, score) test_output = [{ \"This product is satisfactory and functions as expected.\" : \"0.8\" }] # Define the signature associated with the model # The signature includes the structure of the input and the expected output, as well as any parameters that # we would like to expose for overriding at inference time (including their default values if they are not overridden). signature = infer_signature ( model_input = input_example , model_output = test_output , params = { \"similarity_threshold\" : 0.5 } ) # Visualize the signature, showing our overridden inference parameter and its default. signature [4]: inputs:\n  ['query': string]\noutputs:\n  ['This product is satisfactory and functions as expected.': string]\nparams:\n  ['similarity_threshold': double (default: 0.5)]  Creating an experiment We create a new MLflow Experiment so that the run we\u00e2\u0080\u0099re going to log our model to does not log to the default experiment and instead has its own contextually relevant entry. [5]: # If you are running this tutorial in local mode, leave the next line commented out. # Otherwise, uncomment the following line and set your tracking uri to your local or remote tracking server. # mlflow.set_tracking_uri(\"http://127.0.0.1:8080\") mlflow . set_experiment ( \"Paraphrase Mining\" ) [5]: <Experiment: artifact_location='file:///Users/benjamin.wilson/repos/mlflow-fork/mlflow/docs/source/llms/sentence-transformers/tutorials/paraphrase-mining/mlruns/380691166097743403', creation_time=1701282619556, experiment_id='380691166097743403', last_update_time=1701282619556, lifecycle_stage='active', name='Paraphrase Mining', tags={}>  Logging the Paraphrase Mining Model with MLflow Log the custom Paraphrase Mining Model with MLflow, a key step for model management and deployment.  Initiating an MLflow Run Start an MLflow run to create a comprehensive record of model logging and tracking within the MLflow framework.  Logging the Model in MLflow Use MLflow\u00e2\u0080\u0099s Python model logging function to integrate the custom model into the MLflow ecosystem. Provide a unique name for the model for easy identification in MLflow. Log the instantiated Paraphrase Mining Model, along with an input example, model signature, artifacts, and Python dependencies. ",
        "id": "693a52470787a3fe99b5213131cbf9eb"
    },
    {
        "text": " Outcomes and Benefits of Model Logging Register the model within MLflow for streamlined management and deployment, enhancing its accessibility and trackability. Ensure model reproducibility and version control across deployment environments. [6]: with mlflow . start_run () as run : model_info = mlflow . pyfunc . log_model ( \"paraphrase_model\" , python_model = ParaphraseMiningModel (), input_example = input_example , signature = signature , artifacts = artifacts , pip_requirements = [ \"sentence_transformers\" ], ) 2023/11/30 15:41:39 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false  Model Loading and Paraphrase Mining Prediction Illustrate the real-world application of the Paraphrase Mining Model by loading it with MLflow and executing a prediction.  Loading the Model for Inference Utilize MLflow\u00e2\u0080\u0099s load_model function to retrieve and prepare the model for inference. Locate and load the model using its unique URI within the MLflow registry.  Executing a Paraphrase Mining Prediction Make a prediction using the model\u00e2\u0080\u0099s predict method, applying the paraphrase mining logic embedded in the model class. Pass a representative query with a set similarity_threshold to find matching paraphrases in the corpus.  Interpreting the Model Output Review the list of semantically similar sentences to the query, highlighting the model\u00e2\u0080\u0099s paraphrase identification capabilities. Analyze the similarity scores to understand the degree of semantic relatedness between the query and corpus sentences.  Conclusion This demonstration validates the Paraphrase Mining Model\u00e2\u0080\u0099s effectiveness in real-world scenarios, underscoring its utility in content recommendation, information retrieval, and conversational AI. [7]: # Load our model by supplying the uri that was used to save the model artifacts loaded_dynamic = mlflow . pyfunc . load_model ( model_info . model_uri ) # Perform a quick validation that our loaded model is performing adequately loaded_dynamic . predict ( { \"query\" : \"Space exploration is fascinating.\" }, params = { \"similarity_threshold\" : 0.65 } ) [7]: {'Studying the stars helps us understand our universe.': '0.8207424879074097',\n 'The history of space exploration is filled with remarkable achievements.': '0.7770636677742004',\n 'Exploring ancient cities in Europe offers a glimpse into history.': '0.7461957335472107',\n 'Space travel advancements could lead to interplanetary exploration.': '0.7090306282043457',\n 'Space exploration continues to push the boundaries of human knowledge.': '0.6893945932388306',\n 'The mysteries of the universe are unveiled through space missions.': '0.6830739974975586',\n 'The study of celestial bodies helps us understand the cosmos.': '0.671358048915863'}  Conclusion: Insights and Potential Enhancements As we wrap up this tutorial, let\u00e2\u0080\u0099s reflect on our journey through the implementation of a Paraphrase Mining Model using Sentence Transformers and MLflow. We\u00e2\u0080\u0099ve successfully built and deployed a model capable of identifying semantically similar sentences, showcasing the flexibility and power of MLflow\u00e2\u0080\u0099s PythonModel implementation.  Key Takeaways We learned how to integrate advanced NLP techniques, specifically paraphrase mining, with MLflow. This integration not only enhances model management but also simplifies deployment and scalability. The flexibility of the PythonModel implementation in MLflow was a central theme. We saw firsthand how it allows for the incorporation of custom logic into the model\u00e2\u0080\u0099s predict function, catering to specific NLP tasks like paraphrase mining. Through our custom model, we explored the dynamics of sentence embeddings, semantic similarity, and the nuances of language understanding. This understanding is crucial in a wide range of applications, from content recommendation to conversational AI. ",
        "id": "650d4747f4e3bae8588dfe9d58e2469d"
    },
    {
        "text": " Ideas for Enhancing the Paraphrase Mining Model While our model serves as a robust starting point, there are several enhancements that could be made within the predict function to make it more powerful and feature-rich: Contextual Filters : Introduce filters based on contextual clues or specific keywords to refine the search results further. This feature would allow users to narrow down paraphrases to those most relevant to their particular context or subject matter. Sentiment Analysis Integration : Incorporate sentiment analysis to group paraphrases by their emotional tone. This would be especially useful in applications like customer feedback analysis, where understanding sentiment is as important as content. Multi-Lingual Support : Expand the model to support paraphrase mining in multiple languages. This enhancement would significantly broaden the model\u00e2\u0080\u0099s applicability in global or multi-lingual contexts.  Scalability with Vector Databases Moving beyond a static text file as a corpus, a more scalable and real-world approach would involve connecting the model to an external vector database or in-memory store. Pre-calculated embeddings could be stored and updated in such databases, accommodating real-time content generation without requiring model redeployment. This approach would dramatically improve the model\u00e2\u0080\u0099s scalability and responsiveness in real-world applications.  Final Thoughts The journey through building and deploying the Paraphrase Mining Model has been both enlightening and practical. We\u00e2\u0080\u0099ve seen how MLflow\u00e2\u0080\u0099s PythonModel offers a flexible canvas for crafting custom NLP solutions, and how sentence transformers can be leveraged to delve deep into the semantics of language. This tutorial is just the beginning. There\u00e2\u0080\u0099s a vast potential for further exploration and innovation in paraphrase mining and NLP as a whole. We encourage you to build upon this foundation, experiment with enhancements, and continue pushing the boundaries of what\u00e2\u0080\u0099s possible with MLflow and advanced NLP techniques. Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "ed6630cfcbd15818f3fc1c2f02fce1ad"
    },
    {
        "text": "Sentence Transformers within MLflow 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor MLflow OpenAI Flavor MLflow Sentence-Transformers Flavor Introduction What makes this Library so Special? Features Getting Started with the MLflow Sentence Transformers Flavor - Tutorials and Guides Detailed Documentation Learning More About Sentence Transformers MLflow LangChain Flavor MLflow LlamaIndex Flavor MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow Sentence-Transformers Flavor Sentence Transformers within MLflow  Sentence Transformers within MLflow Attention The sentence_transformers flavor is in active development and is marked as Experimental. Public APIs may change and new features are\nsubject to be added as additional functionality is brought to the flavor. The sentence_transformers model flavor enables logging of sentence-transformers models in MLflow format via\nthe mlflow.sentence_transformers.save_model() and mlflow.sentence_transformers.log_model() functions. Using these\nfunctions also adds the python_function flavor to the MLflow Models, enabling the model to be\ninterpreted as a generic Python function for inference via mlflow.pyfunc.load_model() .\nAdditionally, mlflow.sentence_transformers.load_model() can be used to load a saved or logged MLflow\nModel with the sentence_transformers flavor in the native sentence-transformers format.  Tutorials for Sentence Transformers Looking to get right in to some usable examples and tutorials that show how to leverage this library with MLflow? See the Tutorials  Input and Output Types for PyFunc The sentence_transformers python_function (pyfunc) model flavor standardizes\nthe process of embedding sentences and computing semantic similarity. This standardization allows for serving\nand batch inference by adapting the required data structures for sentence_transformers into formats compatible with JSON serialization and casting to Pandas DataFrames. Note The sentence_transformers flavor supports various models for tasks such as embedding generation, semantic similarity, and paraphrase mining. The specific input and output types will depend on the model and task being performed.  Saving and Logging Sentence Transformers Models You can save and log sentence-transformers models in MLflow. Here\u00e2\u0080\u0099s an example of both saving and logging a model: import mlflow from sentence_transformers import SentenceTransformer model = SentenceTransformer ( \"model_name\" ) # Saving the model mlflow . sentence_transformers . save_model ( model = model , path = \"path/to/save/directory\" ) # Logging the model with mlflow . start_run (): mlflow . sentence_transformers . log_model ( sentence_transformers_model = model , artifact_path = \"model_artifact_path\" ) ",
        "id": "dd14e3677fb22ccab95e294a40159c1a"
    },
    {
        "text": " Saving Sentence Transformers Models with an OpenAI-Compatible Inference Interface Note This feature is only available in MLflow 2.11.0 and above. MLflow\u00e2\u0080\u0099s sentence_transformers flavor allows you to pass in the task param with the string value \"llm/v1/embeddings\" when saving a model with mlflow.sentence_transformers.save_model() and mlflow.sentence_transformers.log_model() . For example: import mlflow from sentence_transformers import SentenceTransformer model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) mlflow . sentence_transformers . save_model ( model = model , path = \"path/to/save/directory\" , task = \"llm/v1/embeddings\" ) When task is set as \"llm/v1/embeddings\" , MLflow handles the following for you: Setting an embeddings compatible signature for the model Performing data pre- and post-processing to ensure the inputs and outputs conform to\nthe Embeddings API spec ,\nwhich is compatible with OpenAI\u00e2\u0080\u0099s API spec. Note that these modifications only apply when the model is loaded with mlflow.pyfunc.load_model() (e.g. when\nserving the model with the mlflow models serve CLI tool). If you want to load just the base pipeline, you can\nalways do so via mlflow.sentence_transformers.load_model() . Aside from the sentence-transformers flavor, the transformers flavor also support OpenAI-compatible inference interface ( \"llm/v1/chat\" and \"llm/v1/completions\" ). Refer to the Transformers flavor guide for more information.  Custom Python Function Implementation In addition to using pre-built models, you can create custom Python functions with the sentence_transformers flavor. Here\u00e2\u0080\u0099s an example of a custom\nimplementation for comparing the similarity between text documents: import mlflow from mlflow.pyfunc import PythonModel import pandas as pd import numpy as np from sentence_transformers import SentenceTransformer , util class DocumentSimilarityModel ( PythonModel ): def load_context ( self , context ): \"\"\"Load the model context for inference.\"\"\" self . model = SentenceTransformer . load ( context . artifacts [ \"model_path\" ]) def predict ( self , context , model_input ): \"\"\"Predict method for comparing similarity between documents.\"\"\" if isinstance ( model_input , pd . DataFrame ) and model_input . shape [ 1 ] == 2 : documents = model_input . values else : raise ValueError ( \"Input must be a DataFrame with exactly two columns.\" ) # Compute embeddings for each document separately embeddings1 = self . model . encode ( documents [:, 0 ], convert_to_tensor = True ) embeddings2 = self . model . encode ( documents [:, 1 ], convert_to_tensor = True ) # Calculate cosine similarity similarity_scores = util . cos_sim ( embeddings1 , embeddings2 ) return pd . DataFrame ( similarity_scores . numpy (), columns = [ \"similarity_score\" ]) # Example model saving and loading model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) model_path = \"/tmp/sentence_transformers_model\" model . save ( model_path ) # Example usage with mlflow . start_run (): model_info = mlflow . pyfunc . log_model ( artifact_path = \"document_similarity_model\" , python_model = DocumentSimilarityModel (), artifacts = { \"model_path\" : model_path }, ) loaded = mlflow . pyfunc . load_model ( model_info . model_uri ) # Test prediction df = pd . DataFrame ( { \"doc1\" : [ \"Sentence Transformers is a wonderful package!\" ], \"doc2\" : [ \"MLflow is pretty great too!\" ], } ) result = loaded . predict ( df ) print ( result ) Which will generate the similarity score for the documents passed, as shown below: similarity_score 0 0 .275423 Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "00963fce1de2780327ee187bee337030"
    },
    {
        "text": "Introduction to Using LangChain with MLflow 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor MLflow OpenAI Flavor MLflow Sentence-Transformers Flavor MLflow LangChain Flavor Why use MLflow with LangChain? Automatic Logging Supported Elements in MLflow LangChain Integration Overview of Chains, Agents, and Retrievers Getting Started with the MLflow LangChain Flavor - Tutorials and Guides Detailed Documentation FAQ MLflow LlamaIndex Flavor MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow LangChain Flavor Introduction to Using LangChain with MLflow  Introduction to Using LangChain with MLflow Welcome to this interactive tutorial designed to introduce you to LangChain and its integration with MLflow. This tutorial is structured as a notebook to provide a hands-on, practical learning experience with the simplest and most core features of LangChain. Download this Notebook  What You Will Learn Understanding LangChain : Get to know the basics of LangChain and how it is used in developing applications powered by language models. Chains in LangChain : Explore the concept of chains in LangChain, which are sequences of actions or operations orchestrated to perform complex tasks. Integration with MLflow : Learn how LangChain integrates with MLflow, a platform for managing the machine learning lifecycle, including logging, tracking, and deploying models. Practical Application : Apply your knowledge to build a LangChain chain that acts like a sous chef, focusing on the preparation steps of a recipe.  Background on LangChain LangChain is a Python-based framework that simplifies the development of applications using language models. It is designed to enhance context-awareness and reasoning in applications, allowing for more sophisticated and interactive functionalities.  What is a Chain? Chain Definition : In LangChain, a chain refers to a series of interconnected components or steps designed to accomplish a specific task. Chain Example : In our tutorial, we\u00e2\u0080\u0099ll create a chain that simulates a sous chef\u00e2\u0080\u0099s role in preparing ingredients and tools for a recipe.  Tutorial Overview In this tutorial, you will: Set Up LangChain and MLflow : Initialize and configure both LangChain and MLflow. Create a Sous Chef Chain : Develop a LangChain chain that lists ingredients, describes preparation techniques, organizes ingredient staging, and details cooking implements preparation for a given recipe. Log and Load the Model : Utilize MLflow to log the chain model and then load it for prediction. Run a Prediction : Execute the chain to see how it would prepare a restaurant dish for a specific number of customers. By the end of this tutorial, you will have a solid foundation in using LangChain with MLflow and an understanding of how to construct and manage chains for practical applications. Let\u00e2\u0080\u0099s dive in and explore the world of LangChain and MLflow! ",
        "id": "75a9561248fef82a14d82d8dddaf51aa"
    },
    {
        "text": " Prerequisites In order to get started with this tutorial, we\u00e2\u0080\u0099re going to need a few things first. An OpenAI API Account. You can sign up here to get access in order to start programatically accessing one of the leading highly sophisticated LLM services on the planet. An OpenAI API Key. You can access this once you\u00e2\u0080\u0099ve created an account by navigating to the API keys page . The OpenAI SDK. It\u00e2\u0080\u0099s available on PyPI here. For this tutorial, we\u00e2\u0080\u0099re going to be using version 0.28.1 (the last release prior to the 1.0 release). The LangChain package. You can find it here on PyPI .  Notebook compatibility With rapidly changing libraries such as langchain , examples can become outdated rather quickly and will no longer work. For the purposes of demonstration, here are the critical dependencies that are recommended to use to effectively run this notebook: Package Version langchain 0.1.16 lanchain-community 0.0.33 langchain-openai 0.0.8 openai 1.12.0 tiktoken 0.6.0 mlflow 2.12.1 If you attempt to execute this notebook with different versions, it may function correctly, but it is recommended to use the precise versions above to ensure that your code executes properly. To install the dependent packages simply run: pip install openai == 1 .12.0 tiktoken == 0 .6.0 langchain == 0 .1.16 langchain-openai == 0 .0.33 langchain-community == 0 .0.33 mlflow == 2 .12.1 NOTE: This tutorial does not support openai<1 and is not guaranteed to work with versions of langchain<1.16.0  API Key Security Overview API keys, especially for SaaS Large Language Models (LLMs), are as sensitive as financial information due to their connection to billing. If you\u00e2\u0080\u0099re interested in learning more about an alternative MLflow solution that securely manages your access keys, read about MLflow AI Gateway here .  Essential Practices: Confidentiality : Always keep API keys private. Secure Storage : Prefer environment variables or secure services. Frequent Rotation : Regularly update keys to avoid unauthorized access.  Configuring API Keys For secure usage, set API keys as environment variables. macOS/Linux : Refer to Apple\u00e2\u0080\u0099s guide on using environment variables in Terminal for detailed instructions. Windows : Follow the steps outlined in Microsoft\u00e2\u0080\u0099s documentation on environment variables . [1]: import os from langchain.chains import LLMChain from langchain.prompts import PromptTemplate from langchain_openai import OpenAI import mlflow assert \"OPENAI_API_KEY\" in os . environ , \"Please set the OPENAI_API_KEY environment variable.\" NOTE: If you\u00e2\u0080\u0099d like to use Azure OpenAI with LangChain, you need to install ``openai>=1.10.0`` and ``langchain-openai>=0.0.6``, as well as to specify the following credentials and parameters: [ ]: # NOTE: Only run this cell if you are using Azure interfaces with OpenAI. If you have a direct account with # OpenAI, ignore this cell. from langchain_openai import AzureOpenAI , AzureOpenAIEmbeddings # Set this to `azure` os . environ [ \"OPENAI_API_TYPE\" ] = \"azure\" # The API version you want to use: set this to `2023-05-15` for the released version. os . environ [ \"OPENAI_API_VERSION\" ] = \"2023-05-15\" assert ( \"AZURE_OPENAI_ENDPOINT\" in os . environ ), \"Please set the AZURE_OPENAI_ENDPOINT environment variable. It is the base URL for your Azure OpenAI resource. You can find this in the Azure portal under your Azure OpenAI resource.\" assert ( \"OPENAI_API_KEY\" in os . environ ), \"Please set the OPENAI_API_KEY environment variable. It is the API key for your Azure OpenAI resource. You can find this in the Azure portal under your Azure OpenAI resource.\" azure_openai_llm = AzureOpenAI ( deployment_name = \"<your-deployment-name>\" , model_name = \"gpt-4o-mini\" , ) azure_openai_embeddings = AzureOpenAIEmbeddings ( azure_deployment = \"<your-deployment-name>\" , ) ",
        "id": "f26d4670943a7f862197026c17a10fdc"
    },
    {
        "text": " Configuring the OpenAI Completions Model in LangChain In this section of the tutorial, we have configured the OpenAI model with specific parameters suitable for generating language completions. We\u00e2\u0080\u0099re using a Completions model, not ChatCompletions, which means each request is independent, and the entire prompt needs to be included every time to generate a response.  Understanding the Completions Model Completions Model : This model does not maintain contextual information across requests. It\u00e2\u0080\u0099s ideal for tasks where each request is standalone and doesn\u00e2\u0080\u0099t depend on past interactions. Offers flexibility for a variety of non-conversational applications. No Contextual Memory : The lack of memory of previous interactions means the model is best suited for one-off requests or scenarios where continuity of the conversation is not required. Comparisons with the ChatCompletions Model Type : Tailored for conversational AI, maintaining context across multiple exchanges for a continuous conversation. Suitable for chatbots or applications where dialogue history is crucial. In this tutorial, we use the Completions model for its simplicity and effectiveness in handling individual, independent requests, aligning with our tutorial\u00e2\u0080\u0099s focus on preparation steps before cooking. [2]: llm = OpenAI ( temperature = 0.1 , max_tokens = 1000 )  Explanation of the Template Instruction for Sous Chef Simulation In this part of the tutorial, we have crafted a detailed prompt template that simulates the role of a fine dining sous chef. This template is designed to guide the LangChain model in preparing for a dish, focusing exclusively on the mise-en-place process. ",
        "id": "dad27b0047146c7406860ee99cbe8b25"
    },
    {
        "text": " Breakdown of the Template Instruction Sous Chef Roleplay : The prompt places the language model in the role of a sous chef, emphasizing meticulous preparation. Task Outline : List the Ingredients : Instructs the model to itemize all necessary ingredients for a given dish. Preparation Techniques : Asks the model to describe necessary techniques for ingredient preparation, such as cutting and processing. Ingredient Staging : Requires the model to provide detailed staging instructions for each ingredient, considering the sequence and timing of use. Cooking Implements Preparation : Guides the model to list and prepare all cooking tools required for the dish\u00e2\u0080\u0099s preparation phase. Scope Limitation : The template is explicitly designed to stop at the preparation stage, avoiding the actual cooking process. It focuses on setting up everything needed for the chef to begin cooking. Dynamic Inputs : The template is adaptable to different recipes and customer counts, as indicated by placeholders {recipe} and {customer_count} . This template instruction is a key component of the tutorial, demonstrating how to leverage LangChain declaring instructive prompts with parametrized features geared toward single-purpose completions-style applications. [3]: template_instruction = ( \"Imagine you are a fine dining sous chef. Your task is to meticulously prepare for a dish, focusing on the mise-en-place process.\" \"Given a recipe, your responsibilities are: \" \"1. List the Ingredients: Carefully itemize all ingredients required for the dish, ensuring every element is accounted for. \" \"2. Preparation Techniques: Describe the techniques and operations needed for preparing each ingredient. This includes cutting, \" \"processing, or any other form of preparation. Focus on the art of mise-en-place, ensuring everything is perfectly set up before cooking begins.\" \"3. Ingredient Staging: Provide detailed instructions on how to stage and arrange each ingredient. Explain where each item should be placed for \" \"efficient access during the cooking process. Consider the timing and sequence of use for each ingredient. \" \"4. Cooking Implements Preparation: Enumerate all the cooking tools and implements needed for each phase of the dish's preparation. \" \"Detail any specific preparation these tools might need before the actual cooking starts and describe what pots, pans, dishes, and \" \"other tools will be needed for the final preparation.\" \"Remember, your guidance stops at the preparation stage. Do not delve into the actual cooking process of the dish. \" \"Your goal is to set the stage flawlessly for the chef to execute the cooking seamlessly.\" \"The recipe you are given is for: {recipe} for {customer_count} people. \" )  Constructing the LangChain Chain We start by setting up a PromptTemplate in LangChain, tailored to our sous chef scenario. The template is designed to dynamically accept inputs like the recipe name and customer count. Then, we initialize an LLMChain by combining our OpenAI language model with the prompt template, creating a chain that can simulate the sous chef\u00e2\u0080\u0099s preparation process.  Logging the Chain in MLflow With the chain ready, we proceed to log it in MLflow. This is done within an MLflow run, which not only logs the chain model under a specified name but also tracks various details about the model. The logging process ensures that all aspects of the chain are recorded, allowing for efficient version control and future retrieval. [4]: prompt = PromptTemplate ( input_variables = [ \"recipe\" , \"customer_count\" ], template = template_instruction , ) chain = LLMChain ( llm = llm , prompt = prompt ) mlflow . set_experiment ( \"Cooking Assistant\" ) with mlflow . start_run (): model_info = mlflow . langchain . log_model ( chain , \"langchain_model\" ) If we navigate to the MLflow UI, we\u00e2\u0080\u0099ll see our logged LangChain model. ",
        "id": "be1599cc556824cc6ef1ac7416be7b40"
    },
    {
        "text": " Loading the Model and Predicting with MLflow In this part of our tutorial, we demonstrate the practical application of the logged LangChain model using MLflow. We load the model and run a prediction for a specific dish, showcasing the model\u00e2\u0080\u0099s ability to assist in culinary preparation.  Model Loading and Execution After logging our LangChain chain with MLflow, we proceed to load the model using MLflow\u00e2\u0080\u0099s pyfunc.load_model function. This step is crucial as it brings our previously logged model into an executable state. We then input a specific recipe along with the customer count into our model. In this case, we use the recipe for \u00e2\u0080\u009cboeuf bourginon\u00e2\u0080\u009d and specify that it\u00e2\u0080\u0099s for 12 customers. The model, acting as a sous chef, processes this information and generates detailed preparation instructions. ",
        "id": "14bb8aa4b7e27625b0e20c5e04f058b2"
    },
    {
        "text": " Output from the Model The model\u00e2\u0080\u0099s output provides a comprehensive guide on preparing \u00e2\u0080\u009cboeuf bourginon,\u00e2\u0080\u009d covering several critical aspects: Ingredients List : A detailed enumeration of all necessary ingredients, quantified and tailored for the specified number of customers. Preparation Techniques : Step-by-step instructions on how to prepare each ingredient, following the principles of mise-en-place. Ingredient Staging : Guidance on how to organize and stage the ingredients, ensuring efficient access and use during the cooking process. Cooking Implements Preparation : Instructions on preparing the necessary cooking tools and implements, from pots and pans to bowls and colanders. This example demonstrates the power and utility of combining LangChain and MLflow in a practical scenario. It highlights how such an integration can effectively translate complex requirements into actionable steps, aiding in tasks that require precision and careful planning. [5]: loaded_model = mlflow . pyfunc . load_model ( model_info . model_uri ) dish1 = loaded_model . predict ({ \"recipe\" : \"boeuf bourginon\" , \"customer_count\" : \"4\" }) print ( dish1 [ 0 ]) 1. Ingredients:\n- 2 pounds beef chuck, cut into 1-inch cubes\n- 6 slices of bacon, diced\n- 2 tablespoons olive oil\n- 1 onion, diced\n- 2 carrots, diced\n- 2 cloves of garlic, minced\n- 1 tablespoon tomato paste\n- 1 bottle of red wine\n- 2 cups beef broth\n- 1 bouquet garni (thyme, bay leaf, parsley)\n- 1 pound pearl onions, peeled\n- 1 pound mushrooms, quartered\n- Salt and pepper to taste\n- Chopped parsley for garnish\n\n2. Preparation Techniques:\n- Cut the beef chuck into 1-inch cubes and set aside.\n- Dice the bacon and set aside.\n- Peel and dice the onion and carrots.\n- Mince the garlic cloves.\n- Prepare the bouquet garni by tying together a few sprigs of thyme, a bay leaf, and a few sprigs of parsley with kitchen twine.\n- Peel the pearl onions and quarter the mushrooms.\n\n3. Ingredient Staging:\n- Place the beef cubes in a bowl and season with salt and pepper.\n- In a large Dutch oven, heat the olive oil over medium-high heat.\n- Add the diced bacon and cook until crispy.\n- Remove the bacon from the pot and set aside.\n- In the same pot, add the seasoned beef cubes and cook until browned on all sides.\n- Remove the beef from the pot and set aside.\n- In the same pot, add the diced onion and carrots and cook until softened.\n- Add the minced garlic and cook for an additional minute.\n- Stir in the tomato paste and cook for another minute.\n- Add the beef and bacon back into the pot.\n- Pour in the red wine and beef broth.\n- Add the bouquet garni and bring to a simmer.\n- Cover the pot and let it simmer for 2 hours, stirring occasionally.\n- After 2 hours, add the pearl onions and mushrooms to the pot.\n- Continue to simmer for an additional hour, or until the beef is tender.\n- Remove the bouquet garni and discard.\n- Taste and adj",
        "id": "54dea9945857c9225f04969868ebae92"
    },
    {
        "text": "eef broth.\n- Add the bouquet garni and bring to a simmer.\n- Cover the pot and let it simmer for 2 hours, stirring occasionally.\n- After 2 hours, add the pearl onions and mushrooms to the pot.\n- Continue to simmer for an additional hour, or until the beef is tender.\n- Remove the bouquet garni and discard.\n- Taste and adjust seasoning with salt and pepper if needed.\n- Garnish with chopped parsley before serving.\n\n4. Cooking Implements Preparation:\n- Large Dutch oven or heavy-bottomed pot\n- Kitchen twine\n- Cutting board\n- Chef's knife\n- Wooden spoon\n- Measuring cups and spoons\n- Bowls for prepped ingredients\n- Tongs for handling meat\n- Ladle for serving\n- Serving dishes for the final dish. [6]: dish2 = loaded_model . predict ({ \"recipe\" : \"Okonomiyaki\" , \"customer_count\" : \"12\" }) print ( dish2 [ 0 ]) Ingredients:\n- 2 cups all-purpose flour\n- 2 teaspoons baking powder\n- 1/2 teaspoon salt\n- 2 eggs\n- 1 1/2 cups water\n- 1/2 head cabbage, thinly sliced\n- 1/2 cup green onions, thinly sliced\n- 1/2 cup carrots, grated\n- 1/2 cup red bell pepper, thinly sliced\n- 1/2 cup cooked shrimp, chopped\n- 1/2 cup cooked bacon, chopped\n- 1/2 cup pickled ginger, chopped\n- 1/2 cup tenkasu (tempura flakes)\n- 1/2 cup mayonnaise\n- 1/4 cup okonomiyaki sauce\n- 1/4 cup katsuobushi (dried bonito flakes)\n- Vegetable oil for cooking\n\nPreparation Techniques:\n1. In a large mixing bowl, combine the flour, baking powder, and salt.\n2. In a separate bowl, beat the eggs and water together.\n3. Slowly pour the egg mixture into the flour mixture, stirring until well combined.\n4. Set the batter aside to rest for 10 minutes.\n5. Thinly slice the cabbage, green onions, and red bell pepper.\n6. Grate the carrots.\n7. Chop the cooked shrimp, bacon, and pickled ginger.\n8. Prepare the tenkasu, mayonnaise, okonomiyaki sauce, and katsuobushi.\n\nIngredient Staging:\n1. Place the sliced cabbage, green onions, carrots, red bell pepper, shrimp, bacon, and pickled ginger in separate bowls.\n2. Arrange the tenkasu, mayonnaise, okonomiyaki sauce, and katsuobushi in small dishes.\n3. Set up a large griddle or non-stick pan for cooking the okonomiyaki.\n\nCooking Implements Preparation:\n1. Make sure the griddle or pan is clean and dry.\n2. Heat the griddle or pan over medium heat.\n3. Have a spatula, tongs, and a large plate ready for flipping and serving the okonomiyaki.\n4. Prepare a large plate or platter for serving the finished okonomiyaki.\n\nRemember, mise-en-place is key to a successful dish. Make sure all ingredients are prepped and ready to go before starting the cooking process. Happy cooking! ",
        "id": "292094acf4acf6b500e4631623c6662b"
    },
    {
        "text": " Conclusion In the final step of our tutorial, we execute another prediction using our LangChain model. This time, we explore the preparation for \u00e2\u0080\u009cOkonomiyaki,\u00e2\u0080\u009d a Japanese dish, for 12 customers. This demonstrates the model\u00e2\u0080\u0099s adaptability and versatility across various cuisines.  Additional Prediction with the Loaded Model The model processes the input for \u00e2\u0080\u009cOkonomiyaki\u00e2\u0080\u009d and outputs detailed preparation steps. This includes listing the ingredients, explaining the preparation techniques, guiding ingredient staging, and detailing the required cooking implements, showcasing the model\u00e2\u0080\u0099s capability to handle diverse recipes with precision.  What We\u00e2\u0080\u0099ve Learned Model Versatility : The tutorial highlighted the LangChain framework for assembling component parts of a basic LLM application, chaining a specific instructional prompt to a Completions-style LLM. MLflow\u00e2\u0080\u0099s Role in Model Management : The integration of LangChain with MLflow demonstrated effective model lifecycle management, from creation and logging to prediction execution.  Closing Thoughts This tutorial offered an insightful journey through creating, managing, and utilizing a LangChain model with MLflow for culinary preparation. It showcased the practical applications and adaptability of LangChain in complex scenarios. We hope this experience has provided valuable knowledge and encourages you to further explore and innovate using LangChain and MLflow in your projects. Happy coding!  What\u00e2\u0080\u0099s next? To continue learning about the capabilities of MLflow and LangChain in more complex examples, we encourage you to continue your learning with the additional LangChain tutorials . Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "5155cb9e16f64f92097989d8ef8b1f35"
    },
    {
        "text": "Introduction to RAG with MLflow and LangChain 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor MLflow OpenAI Flavor MLflow Sentence-Transformers Flavor MLflow LangChain Flavor Why use MLflow with LangChain? Automatic Logging Supported Elements in MLflow LangChain Integration Overview of Chains, Agents, and Retrievers Getting Started with the MLflow LangChain Flavor - Tutorials and Guides Detailed Documentation FAQ MLflow LlamaIndex Flavor MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow LangChain Flavor Introduction to RAG with MLflow and LangChain  Introduction to RAG with MLflow and LangChain Download this Notebook  Tutorial Overview Welcome to this tutorial, where we explore the integration of Retrieval Augmented Generation (RAG) with MLflow and LangChain. Our focus is on demonstrating how to create advanced RAG systems and showcasing the unique capabilities enabled by MLflow in these applications.  Understanding RAG and how to develop one with MLflow Retrieval Augmented Generation (RAG) combines the power of language model generation with information retrieval, allowing language models to access and incorporate external data. This approach significantly enriches the model\u00e2\u0080\u0099s responses with detailed and context-specific information. MLflow is instrumental in this process. As an open-source platform, it facilitates the logging, tracking, and deployment of complex models, including RAG chains. With MLflow, integrating LangChain becomes more streamlined, enhancing the development, evaluation, and deployment processes of RAG models. NOTE: In this tutorial, we\u00e2\u0080\u0099ll be using GPT-3.5 as our base language model. It\u00e2\u0080\u0099s important to note that the results obtained from a RAG system will differ from those obtained by interfacing directly with GPT models. RAG\u00e2\u0080\u0099s unique approach of combining external data retrieval with language model generation creates more nuanced and contextually rich responses.  Learning Outcomes By the end of this tutorial, you will learn: - How to establish a RAG chain using LangChain and MLflow. - Techniques for scraping and processing documents to feed into a RAG system. - Best practices for deploying and using RAG models to answer complex queries. - Understanding the practical implications and differences in responses when using RAG in comparison to direct language model interactions.  Setting up our Retriever Dependencies In order to have a place to store our vetted data (the information that we\u00e2\u0080\u0099re going to be retrieving), we\u00e2\u0080\u0099re going to use a Vector Database. The framework that we\u00e2\u0080\u0099re choosing to use (due to its simplicity, capabilities, and free-to-use characteristics) is FAISS, from Meta .  FAISS Installation for the Tutorial  Understanding FAISS For this tutorial, we will be utilizing FAISS (Facebook AI Similarity Search, developed and maintained by the Meta AI research group ), an efficient similarity search and clustering library. It\u00e2\u0080\u0099s a highly useful library that easily handles large datasets and is capable of performing operations such as nearest neighbor search, which are critical in Retrieval Augmented Generation (RAG) systems. There are\nnumerous other vector database solutions that can perform similar functionality; we are using FAISS in this tutorial due to its simplicity, ease of use, and fantastic performance. ",
        "id": "808773667c0cd9e41d66d0e5b324f0dc"
    },
    {
        "text": " Notebook compatibility With rapidly changing libraries such as langchain , examples can become outdated rather quickly and will no longer work. For the purposes of demonstration, here are the critical dependencies that are recommended to use to effectively run this notebook: Package Version langchain 0.1.16 lanchain-community 0.0.33 langchain-openai 0.0.8 openai 1.12.0 tiktoken 0.6.0 mlflow 2.12.1 faiss-cpu 1.7.4 If you attempt to execute this notebook with different versions, it may function correctly, but it is recommended to use the precise versions above to ensure that your code executes properly.  Installing Requirements Before proceeding with the tutorial, ensure that you have FAISS and Beautiful Soup installed via pip . The version specifiers for other packages are guaranteed to work with this notebook. Other versions of these packages may not function correctly due to breaking changes their APIs. pip install beautifulsoup4 faiss-cpu == 1 .7.4 langchain == 0 .1.16 langchain-community == 0 .0.33 langchain-openai == 0 .0.8 openai == 1 .12.0 tiktoken == 0 .6.0 NOTE: If you\u00e2\u0080\u0099d like to run this using your GPU, you can install faiss-gpu instead. [1]: import os import shutil import tempfile import requests from bs4 import BeautifulSoup from langchain.chains import RetrievalQA from langchain.document_loaders import TextLoader from langchain.text_splitter import CharacterTextSplitter from langchain.vectorstores import FAISS from langchain_openai import OpenAI , OpenAIEmbeddings import mlflow assert \"OPENAI_API_KEY\" in os . environ , \"Please set the OPENAI_API_KEY environment variable.\" NOTE: If you\u00e2\u0080\u0099d like to use Azure OpenAI with LangChain, you need to install ``openai>=1.10.0`` and ``langchain-openai>=0.0.6``, as well as to specify the following credentials and parameters: [ ]: from langchain_openai import AzureOpenAI , AzureOpenAIEmbeddings # Set this to `azure` os . environ [ \"OPENAI_API_TYPE\" ] = \"azure\" # The API version you want to use: set this to `2023-05-15` for the released version. os . environ [ \"OPENAI_API_VERSION\" ] = \"2023-05-15\" assert ( \"AZURE_OPENAI_ENDPOINT\" in os . environ ), \"Please set the AZURE_OPENAI_ENDPOINT environment variable. It is the base URL for your Azure OpenAI resource. You can find this in the Azure portal under your Azure OpenAI resource.\" assert ( \"OPENAI_API_KEY\" in os . environ ), \"Please set the OPENAI_API_KEY environment variable. It is the API key for your Azure OpenAI resource. You can find this in the Azure portal under your Azure OpenAI resource.\" azure_openai_llm = AzureOpenAI ( deployment_name = \"<your-deployment-name>\" , model_name = \"gpt-4o-mini\" , ) azure_openai_embeddings = AzureOpenAIEmbeddings ( azure_deployment = \"<your-deployment-name>\" , )  Scraping Federal Documents for RAG Processing In this section of the tutorial, we will demonstrate how to scrape content from federal document webpages for use in our RAG system. We\u00e2\u0080\u0099ll be focusing on extracting transcripts from specific sections of webpages, which will then be used to feed our Retrieval Augmented Generation (RAG) model. This process is crucial for providing the RAG system with relevant external data. ",
        "id": "64fccf8922f0486c3b5ee0c007f60b33"
    },
    {
        "text": " Function Overview The function fetch_federal_document is designed to scrape and return the transcript of specific federal documents. It takes two arguments: url (the webpage URL) and div_class (the class of the div element containing the transcript). The function handles web requests, parses HTML content, and extracts the desired transcript text. This step is integral to building a RAG system that relies on external, context-specific data. By effectively fetching and processing this data, we can enrich our model\u00e2\u0080\u0099s responses with accurate information directly sourced from authoritative documents. NOTE : In a real-world scenario, you would have your specific text data located on disk somewhere (either locally or on your cloud provider) and the process of loading the embedded data into a vector search database would be entirely external to this active fetching displayed below. We\u00e2\u0080\u0099re simply showing the entire process here for demonstration purposes to show the entire end-to-end workflow for interfacing with a RAG model. [2]: def fetch_federal_document ( url , div_class ): # noqa: D417 \"\"\" Scrapes the transcript of the Act Establishing Yellowstone National Park from the given URL. Args: url (str): URL of the webpage to scrape. Returns: str: The transcript text of the Act. \"\"\" # Sending a request to the URL response = requests . get ( url ) if response . status_code == 200 : # Parsing the HTML content of the page soup = BeautifulSoup ( response . text , \"html.parser\" ) # Finding the transcript section by its HTML structure transcript_section = soup . find ( \"div\" , class_ = div_class ) if transcript_section : transcript_text = transcript_section . get_text ( separator = \" \\n \" , strip = True ) return transcript_text else : return \"Transcript section not found.\" else : return f \"Failed to retrieve the webpage. Status code: { response . status_code } \" ",
        "id": "9787ce52ceb31082d6d7be22220af805"
    },
    {
        "text": " Document Fetching and FAISS Database Creation In this next part, we focus on two key processes: Document Fetching : We use fetch_and_save_documents to retrieve documents from specified URLs. This function takes a list of URLs and a file path as inputs. Each document fetched from the URLs is appended to a single file at the given path. FAISS Database Creation : create_faiss_database is responsible for creating a FAISS database from the documents saved in the previous step. The function leverages TextLoader to load the text, CharacterTextSplitter for document splitting, and OpenAIEmbeddings for generating embeddings. The resulting FAISS database, which facilitates efficient similarity searches, is saved to a specified directory and returned for further use. These functions streamline the process of gathering relevant documents and setting up a FAISS database, essential for implementing advanced Retrieval-Augmented Generation (RAG) applications in MLflow. By modularizing these steps, we ensure code reusability and maintainability. [3]: def fetch_and_save_documents ( url_list , doc_path ): \"\"\" Fetches documents from given URLs and saves them to a specified file path. Args: url_list (list): List of URLs to fetch documents from. doc_path (str): Path to the file where documents will be saved. \"\"\" for url in url_list : document = fetch_federal_document ( url , \"col-sm-9\" ) with open ( doc_path , \"a\" ) as file : file . write ( document ) def create_faiss_database ( document_path , database_save_directory , chunk_size = 500 , chunk_overlap = 10 ): \"\"\" Creates and saves a FAISS database using documents from the specified file. Args: document_path (str): Path to the file containing documents. database_save_directory (str): Directory where the FAISS database will be saved. chunk_size (int, optional): Size of each document chunk. Default is 500. chunk_overlap (int, optional): Overlap between consecutive chunks. Default is 10. Returns: FAISS database instance. \"\"\" # Load documents from the specified file document_loader = TextLoader ( document_path ) raw_documents = document_loader . load () # Split documents into smaller chunks with specified size and overlap document_splitter = CharacterTextSplitter ( chunk_size = chunk_size , chunk_overlap = chunk_overlap ) document_chunks = document_splitter . split_documents ( raw_documents ) # Generate embeddings for each document chunk embedding_generator = OpenAIEmbeddings () faiss_database = FAISS . from_documents ( document_chunks , embedding_generator ) # Save the FAISS database to the specified directory faiss_database . save_local ( database_save_directory ) return faiss_database ",
        "id": "63170d9833a2c0514fcab1ebe862545e"
    },
    {
        "text": " Setting Up the Working Environment and FAISS Database This section of the tutorial deals with the setup for our Retrieval-Augmented Generation (RAG) application. We\u00e2\u0080\u0099ll establish the working environment and create the necessary FAISS database: Temporary Directory Creation : A temporary directory is created using tempfile.mkdtemp() . This directory serves as a workspace for storing our documents and the FAISS database. Document Path and FAISS Index Directory : Paths for storing the fetched documents and FAISS database are defined within this temporary directory. Document Fetching : We have a list of URLs ( url_listings ) containing the documents we need to fetch. fetch_and_save_documents function is used to retrieve and save the documents from these URLs into a single file located at doc_path . FAISS Database Creation : The create_faiss_database function is then called to create a FAISS database from the saved documents, using the default chunk_size and chunk_overlap values. This database ( vector_db ) is crucial for the RAG process, as it enables efficient similarity searches on the loaded documents. By the end of this process, we have all documents consolidated in a single location and a FAISS database ready to be used for retrieval purposes in our MLflow-enabled RAG application. [4]: temporary_directory = tempfile . mkdtemp () doc_path = os . path . join ( temporary_directory , \"docs.txt\" ) persist_dir = os . path . join ( temporary_directory , \"faiss_index\" ) url_listings = [ \"https://www.archives.gov/milestone-documents/act-establishing-yellowstone-national-park#transcript\" , \"https://www.archives.gov/milestone-documents/sherman-anti-trust-act#transcript\" , ] fetch_and_save_documents ( url_listings , doc_path ) vector_db = create_faiss_database ( doc_path , persist_dir ) ",
        "id": "da909ed2b2498f87b68e3dcacd74e263"
    },
    {
        "text": " Establishing RetrievalQA Chain and Logging with MLflow In this final setup phase, we focus on creating the RetrievalQA chain and integrating it with MLflow: Initializing the RetrievalQA Chain : The RetrievalQA chain is initialized using the OpenAI language model ( llm ) and the retriever from our previously created FAISS database ( vector_db.as_retriever() ). This chain will use the OpenAI model for generating responses and the FAISS retriever for document-based information retrieval. Loader Function for Retrieval : A load_retriever function is defined to load the retriever from the FAISS database saved in the specified directory. This function is crucial for reloading the retriever when the model is used later. Logging the Model with MLflow : The RetrievalQA chain is logged using mlflow.langchain.log_model . This process includes specifying the artifact_path , the loader_fn for the retriever, and the persist_dir where the FAISS database is stored. Logging the model with MLflow ensures it is tracked and can be easily retrieved for future use. Through these steps, we successfully integrate a complex RAG application with MLflow, showcasing its capability to handle advanced NLP tasks. [5]: mlflow . set_experiment ( \"Legal RAG\" ) retrievalQA = RetrievalQA . from_llm ( llm = OpenAI (), retriever = vector_db . as_retriever ()) # Log the retrievalQA chain def load_retriever ( persist_directory ): embeddings = OpenAIEmbeddings () vectorstore = FAISS . load_local ( persist_directory , embeddings , allow_dangerous_deserialization = True , # This is required to load the index from MLflow ) return vectorstore . as_retriever () with mlflow . start_run () as run : model_info = mlflow . langchain . log_model ( retrievalQA , artifact_path = \"retrieval_qa\" , loader_fn = load_retriever , persist_dir = persist_dir , ) IMPORTANT : In order to load a stored vectorstore instance such as our FAISS instance above, we need to specify within the load function the argument allow_dangeous_deserialization to True in order for the load to succeed. This is due to a safety warning that was introduced in langchain for loading objects that have been serialized using pickle or cloudpickle . While this issue of remote code execution is not a risk with using MLflow, as the serialization and\ndeserialization happens entirely via API and within your environment, the argument must be set in order to prevent an Exception from being thrown at load time.  Our RAG application in the MLflow UI  Testing our RAG Model Now that we have the model stored in MLflow, we can load it back as a pyfunc and see how well it answers a few critically important questions about these acts of Congress in America. [6]: loaded_model = mlflow . pyfunc . load_model ( model_info . model_uri ) [7]: def print_formatted_response ( response_list , max_line_length = 80 ): \"\"\" Formats and prints responses with a maximum line length for better readability. Args: response_list (list): A list of strings representing responses. max_line_length (int): Maximum number of characters in a line. Defaults to 80. \"\"\" for response in response_list : words = response . split () line = \"\" for word in words : if len ( line ) + len ( word ) + 1 <= max_line_length : line += word + \" \" else : print ( line ) line = word + \" \" print ( line )  Let\u00e2\u0080\u0099s make sure that this thing works Let\u00e2\u0080\u0099s try out our Retriever model by sending it a fairly simple but purposefully vague question. [8]: answer1 = loaded_model . predict ([{ \"query\" : \"What does the document say about trespassers?\" }]) print_formatted_response ( answer1 ) The document states that all persons who shall locate or settle upon or occupy\nthe land reserved for the public park, except as provided, shall be considered\ntrespassers and removed from the park.  Understanding the RetrievalQA Response With this model, our approach combines text retrieval from a database with language model generation to answer specific queries. ",
        "id": "e872f0ab7aef3645d7e8896ac8eac3da"
    },
    {
        "text": " How It Works: RetrievalQA Model : This model, a part of the LangChain suite, is designed to first retrieve relevant information from a predefined database and then use a language model to generate a response based on this information. Database Integration : In this example, we\u00e2\u0080\u0099ve created a FAISS database from historical documents, such as the Act Establishing Yellowstone National Park. This database is used by the RetrievalQA model to find relevant sections of text. Query Processing : When we execute loaded_model.predict([{\"query\": \"What does the document say about trespassers?\"}]) , the model first searches the database for parts of the document that are most relevant to the query about trespassers.  Why Is This Response Different? Context-Specific Answers : Unlike a direct query to GPT-3.5, which might generate an answer based on its training data without specific context, the RetrievalQA model provides a response directly derived from the specific documents in the database. Accurate and Relevant : The response is more accurate and contextually relevant because it\u00e2\u0080\u0099s based on the actual content of the specific document being queried. No Generalization : There\u00e2\u0080\u0099s less generalization or assumption in the response. The RetrievalQA model is not \u00e2\u0080\u009cguessing\u00e2\u0080\u009d based on its training; it\u00e2\u0080\u0099s providing information directly sourced from the document.  Key Takeaway: This methodology demonstrates how MLflow and LangChain facilitate complex RAG use cases, where direct interaction with historical or specific texts yields more precise answers than generic language model predictions. The tutorial highlights how leveraging RAG can be particularly useful in scenarios where responses need to be grounded in specific texts or documents, showcasing a powerful blend of retrieval and generation capabilities.  Analyzing the Bridle-Path Query Response This section of the tutorial showcases an interesting aspect of the RetrievalQA model\u00e2\u0080\u0099s capabilities, particularly in handling queries that involve both specific information retrieval and additional context generation.  Query and Response Breakdown: Query : The user asks, \u00e2\u0080\u009cWhat is a bridle-path and can I use one at Yellowstone?\u00e2\u0080\u009d Response : The model responds by explaining what a bridle-path is and confirms that bridle-paths can be used at Yellowstone based on the act.  Understanding the Response Dynamics: Combining Document Data with LLM Context : The query about bridle-paths isn\u00e2\u0080\u0099t directly answered in the act establishing Yellowstone. The model uses its language understanding capabilities to provide a definition of a bridle-path. It then merges this information with the context it retrieves from the FAISS database about the act, particularly regarding the construction of roads and bridle-paths in the park. Enhanced Contextual Understanding : The RetrievalQA model demonstrates an ability to supplement direct information from the database with additional context through its language model. This approach provides a more comprehensive answer that aligns with the user\u00e2\u0080\u0099s query, showing a blend of document-specific data and general knowledge. Why This Is Notable : Unlike a standard LLM response, the RetrievalQA model doesn\u00e2\u0080\u0099t solely rely on its training data for general responses. It effectively integrates specific document information with broader contextual understanding, offering a more nuanced answer. ",
        "id": "c512c40fe2af8b472584f5bc1b6e3fc8"
    },
    {
        "text": " Key Takeaway: This example highlights how MLflow and LangChain, through the RetrievalQA model, facilitate a sophisticated response mechanism. The model not only retrieves relevant document information but also intelligently fills in gaps with its own language understanding capabilities. Such a response mechanism is particularly useful when dealing with queries that require both specific document references and additional contextual information, showcasing the advanced capabilities of RAG in practical applications. [9]: answer2 = loaded_model . predict ( [{ \"query\" : \"What is a bridle-path and can I use one at Yellowstone?\" }] ) print_formatted_response ( answer2 ) A bridle-path is a narrow path or trail designed for horseback riding. Yes, you\ncan use a bridle-path at Yellowstone as it is designated for the enjoyment and\nbenefit of the people visiting the park. However, it may be subject to certain\nregulations and restrictions set by the Secretary of the Interior.  A most serious question In this section of our tutorial, we delve into a whimsically ridiculous query and how our model tackles it with a blend of accuracy and a hint of humor.  Query Overview: The Query : \u00e2\u0080\u009cCan I buy Yellowstone from the Federal Government to set up a buffalo-themed day spa?\u00e2\u0080\u009d The Response : The model, with a straight face, responds, \u00e2\u0080\u009cNo, you cannot buy Yellowstone from the Federal Government to set up a buffalo-themed day spa.\u00e2\u0080\u009d  A Peek into the Model\u00e2\u0080\u0099s Thought Process: Direct and No-Nonsense Response : Despite the query\u00e2\u0080\u0099s comedic undertone, the model gives a direct and clear-cut response. It\u00e2\u0080\u0099s like the model is saying, \u00e2\u0080\u009cNice try, but no, you can\u00e2\u0080\u0099t do that.\u00e2\u0080\u009d This highlights the model\u00e2\u0080\u0099s ability to remain factual, even when faced with a question that\u00e2\u0080\u0099s clearly more humorous than serious. Understanding Legal Boundaries : The response respects the legal and regulatory sanctity of national parks. It seems our model takes the protection of national treasures like Yellowstone pretty seriously! The model\u00e2\u0080\u0099s training on legal and general knowledge assists in delivering a response that\u00e2\u0080\u0099s accurate, albeit the question being a facetious one. Contrast with Traditional LLM Responses : A traditional LLM might have given a more generic answer. In contrast, our model, equipped with context-specific data, promptly debunks the whimsical idea of buying a national park for a spa. ",
        "id": "717c22bae8bb232c4ca4b25b6e31c121"
    },
    {
        "text": " A Dash of Humor in Learning: The query, while absurd, serves as an amusing example of the model\u00e2\u0080\u0099s capability to provide contextually relevant answers to even the most far-fetched questions. It\u00e2\u0080\u0099s a reminder that learning can be both informative and entertaining. In this case, the model plays the role of a straight-faced comedian, addressing a wildly imaginative business proposal with a firm yet comical \u00e2\u0080\u009cNo.\u00e2\u0080\u009d So, while you can\u00e2\u0080\u0099t buy Yellowstone for your buffalo-themed spa dreams, you can certainly enjoy the park\u00e2\u0080\u0099s natural beauty\u00e2\u0080\u00a6 just as a visitor, not as a spa owner! [10]: answer3 = loaded_model . predict ( [ { \"query\" : \"Can I buy Yellowstone from the Federal Government to set up a buffalo-themed day spa?\" } ] ) print_formatted_response ( answer3 ) No, you cannot buy Yellowstone from the Federal Government to set up a\nbuffalo-themed day spa. The land near the headwaters of the Yellowstone River\nhas been reserved and withdrawn from settlement, occupancy, or sale under the\nlaws of the United States and dedicated and set apart as a public park for the\nbenefit and enjoyment of the people. The Secretary of the Interior has control\nover the park and is responsible for making and publishing rules and\nregulations for its management. Leases for building purposes are only granted\nfor small parcels of ground for the accommodation of visitors, and the proceeds\nare used for the management of the park and the construction of roads and\nbridle-paths. Additionally, the wanton destruction of fish and game within the\npark is prohibited. Furthermore, the Act to protect trade and commerce against\nunlawful restraints and monopolies (approved July 2, 1890) states that any\ncontract, combination, or conspiracy in restraint of trade or commerce in any\nTerritory or the District of Columbia is illegal. Thus, buying land to set up a\nbuffalo-themed day spa would likely be considered a violation of this act.  Maintaining Composure: Answering Another Whimsical Query In this part of our tutorial, we explore another amusing question about leasing land in Yellowstone for a buffalo-themed day spa. Let\u00e2\u0080\u0099s see how our model, with unflappable composure, responds to this quirky inquiry.  Query and Response: The Query : \u00e2\u0080\u009cCan I lease a small parcel of land from the Federal Government for a small buffalo-themed day spa for visitors to the park?\u00e2\u0080\u009d The Response : \u00e2\u0080\u009cNo, you cannot lease a small parcel of land from the Federal Government for a small buffalo-themed day spa for visitors to the park\u00e2\u0080\u00a6\u00e2\u0080\u009d  Insights into the Model\u00e2\u0080\u0099s Response: Factual and Unwavering : Despite the continued outlandish line of questioning, our model remains as cool as a cucumber. It patiently explains the limitations and actual purposes of leasing land in Yellowstone. The response cites Section 2 of the act, adding legal precision to its rebuttal. A Lawyer\u00e2\u0080\u0099s Patience Tested? : Imagine if this question was posed to an actual lawyer. By now, they might be rubbing their temples! But our model is unfazed and continues to provide factual answers. This showcases the model\u00e2\u0080\u0099s ability to handle repetitive and unusual queries without losing its \u00e2\u0080\u0098cool\u00e2\u0080\u0099. In conclusion, while our model firmly closes the door on the buffalo-themed day spa dreams, it does so with informative grace, demonstrating its ability to stay on course no matter how imaginative the queries get. [11]: answer4 = loaded_model . predict ( [ { \"query\" : \"Can I lease a small parcel of land from the Federal Government for a small \" \"buffalo-themed day spa for visitors to the park?\" } ] ) print_formatted_response ( answer4 ) No, according to the context provided, the Secretary of the Interior may grant\nleases for building purposes for terms not exceeding ten years, but only for\nthe accommodation of visitors. It does not specifically mention a\nbuffalo-themed day spa as a possible use for the leased land. ",
        "id": "9adb8c98fde1bfb80da79c1c4f1d50b7"
    },
    {
        "text": " Another Attempt at the Buffalo-Themed Day Spa Dream Once more, we pose an imaginative question to our model, this time adding a hotel to the buffalo-themed day spa scenario. The reason for this modification is to evaluate whether the RAG application can discern a nuanced element of the intentionally vague wording of the two acts that we\u00e2\u0080\u0099ve loaded. Let\u00e2\u0080\u0099s see the response to determine if it can resolve both bits of information!  Quick Takeaway: The model, sticking to its informative nature, clarifies the leasing aspects based on the Act\u00e2\u0080\u0099s provisions. It interestingly connects the query to another act related to trade and commerce, showing its ability to cross-reference related legal documents. This response demonstrates the model\u00e2\u0080\u0099s capacity to provide detailed, relevant information, even when faced with quirky and hypothetical scenarios. [12]: answer5 = loaded_model . predict ( [ { \"query\" : \"Can I lease a small parcel of land from the Federal Government for a small \" \"buffalo-themed day spa and hotel for visitors to stay in and relax at while visiting the park?\" } ] ) print_formatted_response ( answer5 ) No, it is not possible to lease land from the Federal Government for a\ncommercial purpose within the designated park area. The Secretary of the\nInterior may grant leases for building purposes for terms not exceeding ten\nyears, but only for small parcels of ground for the accommodation of visitors.\nAdditionally, all proceeds from leases and other revenues must be used for the\nmanagement of the park and the construction of roads and bridle-paths.  Well, what can I do then?  Takeaway: The response reassuringly confirms that one can enjoy Yellowstone\u00e2\u0080\u0099s natural beauty, provided park rules and regulations are respected. This illustrates the model\u00e2\u0080\u0099s ability to provide straightforward, practical advice in response to simple, real-world questions. It clearly has the context of the original act and is able to infer what is permissible (enjoying the reserved land). [13]: answer6 = loaded_model . predict ( [{ \"query\" : \"Can I just go to the park and peacefully enjoy the natural splendor?\" }] ) print_formatted_response ( answer6 ) Yes, according to the context, the park was set apart as a public park or\npleasuring-ground for the benefit and enjoyment of the people. However, the\npark is under the exclusive control of the Secretary of the Interior who has\nthe authority to make and publish rules and regulations for the care and\nmanagement of the park. Therefore, it is important to follow any rules or\nregulations set by the Secretary to ensure the preservation and protection of\nthe park for future enjoyment. ",
        "id": "dad853a7ff937c687b181b7fa3d06ada"
    },
    {
        "text": " Evaluating the RetrievalQA Model\u00e2\u0080\u0099s Legal Context Integration This section of the tutorial showcases the RetrievalQA model\u00e2\u0080\u0099s sophisticated ability to integrate and interpret context from multiple legal documents. The model is challenged with a query that requires synthesizing information from distinct sources. This test is particularly interesting for its demonstration of the model\u00e2\u0080\u0099s proficiency in: Contextual Integration : The model adeptly pulls in relevant legal details from different documents, illustrating its capacity to navigate through multiple sources of information. Legal Interpretation : It interprets the legal implications related to the query, highlighting the model\u00e2\u0080\u0099s understanding of complex legal language and concepts. Cross-Document Inference : The model\u00e2\u0080\u0099s ability to discern and extract the most pertinent information from a pool of multiple documents is a testament to its advanced capabilities in multi-document scenarios. This evaluation provides a clear example of the model\u00e2\u0080\u0099s potential in handling intricate queries that necessitate a deep and nuanced understanding of diverse data sources. [14]: answer7 = loaded_model . predict ( [ { \"query\" : \"Can I start a buffalo themed day spa outside of Yellowstone National Park and stifle any competition?\" } ] ) print_formatted_response ( answer7 ) No, according to the context, any attempt to monopolize trade or commerce in\nthe Yellowstone area or in any territory of the United States is considered\nillegal. Additionally, the park is under the exclusive control of the Secretary\nof the Interior, who may grant leases for building purposes but is expected to\nprevent the destruction of natural resources and the exploitation of fish and\ngame for profit. Therefore, it would not be possible to start a buffalo themed\nday spa outside of Yellowstone National Park and stifle competition without\nviolating the law and risking consequences from the Secretary of the Interior. Cleanup: Removing Temporary Directory : - After we\u00e2\u0080\u0099re done asking our Retriever Model a bunch of silly questions, the temporary directory created earlier is cleaned up using shutil.rmtree . [15]: # Clean up our temporary directory that we created with our FAISS instance shutil . rmtree ( temporary_directory ) ",
        "id": "c2728d25837f2a6f330c5473ff66c01b"
    },
    {
        "text": " Conclusion: Mastering RAG with MLflow In this tutorial, we explored the depths of Retrieval Augmented Generation (RAG) applications, enabled and simplified by MLflow. Here\u00e2\u0080\u0099s a recap of our journey and the key takeaways: Ease of Developing RAG Applications : We learned how MLflow facilitates the development of RAG applications by streamlining the process of integrating large language models with external data sources. Our hands-on experience demonstrated the process of fetching, processing, and embedding documents into a FAISS database, all managed within the MLflow framework. Advanced Query Handling : Through our tests, we observed how the MLflow-wrapped LangChain RAG model adeptly handled complex queries, drawing from multiple documents to provide context-rich and accurate responses. This showcased the potential of RAG models in processing and understanding queries that require multi-source data integration. MLflow\u00e2\u0080\u0099s Role in Deployment and Management : MLflow\u00e2\u0080\u0099s robustness was evident in how it simplifies the logging, deployment, and management of complex models. Its ability to track experiments, manage artifacts, and ease the deployment process highlights its indispensability in the machine learning lifecycle. Practical Application Insights : Our queries, while humorous at times, served to illustrate the practical capabilities of RAG models. From legal interpretations to hypothetical scenarios, we saw how these models could be applied in real-world situations, providing insightful and contextually relevant responses. The Future of RAG and MLflow : This tutorial underscored the potential of combining RAG with MLflow\u00e2\u0080\u0099s streamlined management capabilities. As RAG continues to evolve, MLflow stands out as a crucial tool for harnessing its power, making advanced NLP applications more accessible and efficient. In summary, our journey through this tutorial has not only equipped us with the knowledge to develop and deploy RAG applications effectively but also opened our eyes to the vast possibilities that lie ahead in the realm of advanced NLP, all made more attainable through MLflow.  What\u00e2\u0080\u0099s next? If you\u00e2\u0080\u0099d like to learn more about how MLflow and LangChain integrate, see the other advanced tutorials for MLflow\u00e2\u0080\u0099s LangChain flavor . Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "87880e10a68f0bd14aa09aeba42293b2"
    },
    {
        "text": "LangChain within MLflow (Experimental) 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor MLflow OpenAI Flavor MLflow Sentence-Transformers Flavor MLflow LangChain Flavor Why use MLflow with LangChain? Automatic Logging Supported Elements in MLflow LangChain Integration Overview of Chains, Agents, and Retrievers Getting Started with the MLflow LangChain Flavor - Tutorials and Guides Detailed Documentation FAQ MLflow LlamaIndex Flavor MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow LangChain Flavor LangChain within MLflow (Experimental)  LangChain within MLflow (Experimental) Attention The langchain flavor is currently under active development and is marked as Experimental. Public APIs are evolving, and new features are being added to enhance its functionality.  Overview LangChain is a Python framework for creating applications powered by language models. It offers unique features for developing context-aware\napplications that utilize language models for reasoning and generating responses. This integration with MLflow streamlines the development and\ndeployment of complex NLP applications.  LangChain\u00e2\u0080\u0099s Technical Essence Context-Aware Applications : LangChain specializes in connecting language models to various sources of context, enabling them to produce more relevant and accurate outputs. Reasoning Capabilities : It uses the power of language models to reason about the given context and take appropriate actions based on it. Flexible Chain Composition : The LangChain Expression Language (LCEL) allows for easy construction of complex chains from basic components, supporting functionalities like streaming, parallelism, and logging.  Building Chains with LangChain Basic Components : LangChain facilitates chaining together components like prompt templates, models, and output parsers to create complex workflows. Example - Joke Generator :\n- A basic chain can take a topic and generate a joke using a combination of a prompt template, a ChatOpenAI model, and an output parser.\n- The components are chained using the | operator, similar to a Unix pipe, allowing the output of one component to feed into the next. Advanced Use Cases :\n- LangChain also supports more complex setups, like Retrieval-Augmented Generation (RAG) chains, which can add context when responding to questions. ",
        "id": "a2295f9325d31548769febc251d19795"
    },
    {
        "text": " Integration with MLflow Simplified Logging and Loading : MLflow\u00e2\u0080\u0099s langchain flavor provides functions like log_model() and load_model() , enabling easy logging and retrieval of LangChain models within the MLflow ecosystem. Simplified Deployment : LangChain models logged in MLflow can be interpreted as generic Python functions, simplifying their deployment and use in diverse applications. With dependency management incorporated directly into your logged model, you can deploy your application knowing that the environment that you used to train the model is what will be used to serve it. Versatile Model Interaction : The integration allows developers to leverage LangChain\u00e2\u0080\u0099s unique features in conjunction with MLflow\u00e2\u0080\u0099s robust model tracking and management capabilities. Autologging : MLflow\u00e2\u0080\u0099s langchain flavor provides autologging of LangChain models, which automatically logs artifacts, metrics and models for inference. The langchain model flavor enables logging of LangChain models in MLflow format via\nthe mlflow.langchain.save_model() and mlflow.langchain.log_model() functions. Use of these\nfunctions also adds the python_function flavor to the MLflow Models that they produce, allowing the model to be\ninterpreted as a generic Python function for inference via mlflow.pyfunc.load_model() . You can also use the mlflow.langchain.load_model() function to load a saved or logged MLflow\nModel with the langchain flavor as a dictionary of the model\u00e2\u0080\u0099s attributes.  Basic Example: Logging a LangChain LLMChain in MLflow import os from langchain.chains import LLMChain from langchain.llms import OpenAI from langchain.prompts import PromptTemplate import mlflow # Ensure the OpenAI API key is set in the environment assert \"OPENAI_API_KEY\" in os . environ , \"Please set the OPENAI_API_KEY environment variable.\" # Initialize the OpenAI model and the prompt template llm = OpenAI ( temperature = 0.9 ) prompt = PromptTemplate ( input_variables = [ \"product\" ], template = \"What is a good name for a company that makes {product} ?\" , ) # Create the LLMChain with the specified model and prompt chain = LLMChain ( llm = llm , prompt = prompt ) # Log the LangChain LLMChain in an MLflow run with mlflow . start_run (): logged_model = mlflow . langchain . log_model ( chain , \"langchain_model\" ) # Load the logged model using MLflow's Python function flavor loaded_model = mlflow . pyfunc . load_model ( logged_model . model_uri ) # Predict using the loaded model print ( loaded_model . predict ([{ \"product\" : \"colorful socks\" }])) The output of the example is shown below:  Output [ \" \\n\\n Colorful Cozy Creations.\" ]  What the Simple LLMChain Example Showcases Integration Flexibility : The example highlights how LangChain\u00e2\u0080\u0099s LLMChain, consisting of an OpenAI model and a custom prompt template, can be easily logged in MLflow. Simplified Model Management : Through MLflow\u00e2\u0080\u0099s langchain flavor, the chain is logged, enabling version control, tracking, and easy retrieval. Ease of Deployment : The logged LangChain model is loaded using MLflow\u00e2\u0080\u0099s pyfunc module, illustrating the straightforward deployment process for LangChain models within MLflow. Practical Application : The final prediction step demonstrates the model\u00e2\u0080\u0099s functionality in a real-world scenario, generating a company name based on a given product.  Logging a LangChain Agent with MLflow  What is an Agent? Agents in LangChain leverage language models to dynamically determine and execute a sequence of actions, contrasting with the hardcoded sequences in chains.\nTo learn more about Agents and see additional examples within LangChain, you can read the LangChain docs on Agents .  Key Components of Agents  Agent The core chain driving decision-making, utilizing a language model and a prompt. Receives inputs like tool descriptions, user objectives, and previously executed steps. Outputs the next action set (AgentActions) or the final response (AgentFinish).  Tools Functions invoked by agents to fulfill tasks. Essential to provide appropriate tools and accurately describe them for effective use. ",
        "id": "2ff93844455aead1fe41e8e8a34dd059"
    },
    {
        "text": " Toolkits Collections of tools tailored for specific tasks. LangChain offers a range of built-in toolkits and supports custom toolkit creation.  AgentExecutor The runtime environment executing agent decisions. Handles complexities such as tool errors and agent output parsing. Ensures comprehensive logging and observability.  Additional Agent Runtimes Beyond AgentExecutor, LangChain supports experimental runtimes like Plan-and-execute Agent, Baby AGI, and Auto GPT. Custom runtime logic creation is also facilitated.  An Example of Logging an LangChain Agent This example illustrates the process of logging a LangChain Agent in MLflow, highlighting the integration of LangChain\u00e2\u0080\u0099s complex agent functionalities with MLflow\u00e2\u0080\u0099s robust model management. import os from langchain.agents import AgentType , initialize_agent , load_tools from langchain.llms import OpenAI import mlflow # Note: Ensure that the package 'google-search-results' is installed via pypi to run this example # and that you have a accounts with SerpAPI and OpenAI to use their APIs. # Ensuring necessary API keys are set assert \"OPENAI_API_KEY\" in os . environ , \"Please set the OPENAI_API_KEY environment variable.\" assert \"SERPAPI_API_KEY\" in os . environ , \"Please set the SERPAPI_API_KEY environment variable.\" # Load the language model for agent control llm = OpenAI ( temperature = 0 ) # Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in. tools = load_tools ([ \"serpapi\" , \"llm-math\" ], llm = llm ) # Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use. agent = initialize_agent ( tools , llm , agent = AgentType . ZERO_SHOT_REACT_DESCRIPTION , verbose = True ) # Log the agent in an MLflow run with mlflow . start_run (): logged_model = mlflow . langchain . log_model ( agent , \"langchain_model\" ) # Load the logged agent model for prediction loaded_model = mlflow . pyfunc . load_model ( logged_model . model_uri ) # Generate an inference result using the loaded model question = \"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\" answer = loaded_model . predict ([{ \"input\" : question }]) print ( answer ) The output of the example above is shown below:  Output [ \"1.1044000282035853\" ]  What the Simple Agent Example Showcases Complex Agent Logging : Demonstrates how LangChain\u00e2\u0080\u0099s sophisticated agent, which utilizes multiple tools and a language model, can be logged in MLflow. Integration of Advanced Tools : Showcases the use of additional tools like \u00e2\u0080\u0098serpapi\u00e2\u0080\u0099 and \u00e2\u0080\u0098llm-math\u00e2\u0080\u0099 with a LangChain agent, emphasizing the framework\u00e2\u0080\u0099s capability to integrate complex functionalities. Agent Initialization and Usage : Details the initialization process of a LangChain agent with specific tools and model settings, and how it can be used to perform complex queries. Efficient Model Management and Deployment : Illustrates the ease with which complex LangChain agents can be managed and deployed using MLflow, from logging to prediction.  Real-Time Streaming Outputs with LangChain and GenAI LLMs Note Stream responses via the predict_stream API are only available in MLflow versions >= 2.12.2. Previous versions of MLflow do not support streaming responses.  Overview of Streaming Output Capabilities LangChain integration within MLflow enables real-time streaming outputs from various GenAI language models (LLMs) that support such functionality.\nThis feature is essential for applications that require immediate, incremental responses, facilitating dynamic interactions such as conversational\nagents or live content generation.  Supported Streaming Models LangChain is designed to work seamlessly with any LLM that offers streaming output capabilities. This includes certain models from providers\nlike OpenAI (e.g., specific versions of ChatGPT), as well as other LLMs from different vendors that support similar functionalities. ",
        "id": "1abec04723dd0e17fe7236c36e2dec2b"
    },
    {
        "text": " Using predict_stream for Streaming Outputs The predict_stream method within the MLflow pyfunc LangChain flavor is designed to handle synchronous inputs and provide outputs in a streaming manner. This method is particularly\nuseful for maintaining an engaging user experience by delivering parts of the model\u00e2\u0080\u0099s response as they become available, rather than waiting for the\nentire completion of the response generation.  Example Usage The following example demonstrates setting up and using the predict_stream function with a LangChain model managed in MLflow, highlighting\nthe real-time response generation: from langchain.chains import LLMChain from langchain.prompts import PromptTemplate from langchain_openai import OpenAI import mlflow template_instructions = \"Provide brief answers to technical questions about {topic} and do not answer non-technical questions.\" prompt = PromptTemplate ( input_variables = [ \"topic\" ], template = template_instructions , ) chain = LLMChain ( llm = OpenAI ( temperature = 0.05 ), prompt = prompt ) with mlflow . start_run (): model_info = mlflow . langchain . log_model ( chain , \"tech_chain\" ) # Assuming the model is already logged in MLflow and loaded loaded_model = mlflow . pyfunc . load_model ( model_uri = model_info . model_uri ) # Simulate a single synchronous input input_data = \"Hello, can you explain streaming outputs?\" # Generate responses in a streaming fashion response_stream = loaded_model . predict_stream ( input_data ) for response_part in response_stream : print ( \"Streaming Response Part:\" , response_part ) # Each part of the response is handled as soon as it is generated  Advanced Integration with Callbacks LangChain\u00e2\u0080\u0099s architecture also supports the use of callbacks within the streaming output context. These callbacks can be used to enhance\nfunctionality by allowing actions to be triggered during the streaming process, such as logging intermediate responses or modifying them before delivery. Note Most uses of callback handlers involve logging of traces involved in the various calls to services and tools within a Chain or Retriever. For purposes\nof simplicity, a simple stdout callback handler is shown below. Real-world callback handlers must be subclasses of the BaseCallbackHandler class\nfrom LangChain. from langchain_core.callbacks import StdOutCallbackHandler handler = StdOutCallbackHandler () # Attach callback to enhance the streaming process response_stream = loaded_model . predict_stream ( input_data , callback_handlers = [ handler ]) for enhanced_response in response_stream : print ( \"Enhanced Streaming Response:\" , enhanced_response ) These examples and explanations show how developers can utilize the real-time streaming output capabilities of LangChain models within MLflow,\nenabling the creation of highly responsive and interactive applications.  Enhanced Management of RetrievalQA Chains with MLflow LangChain\u00e2\u0080\u0099s integration with MLflow introduces a more efficient way to manage and utilize the RetrievalQA chains, a key aspect of LangChain\u00e2\u0080\u0099s capabilities.\nThese chains adeptly combine data retrieval with question-answering processes, leveraging the strength of language models.  Key Insights into RetrievalQA Chains RetrievalQA Chain Functionality : These chains represent a sophisticated LangChain feature where information retrieval is seamlessly blended with language\nmodel-based question answering. They excel in scenarios requiring the language\nmodel to consult specific data or documents for accurate responses. Role of the Retrieval Object : At the core of RetrievalQA chains lies the retriever object, tasked with sourcing relevant documents or data in response\nto queries. ",
        "id": "88319bc263b40a554178b7d9450d52df"
    },
    {
        "text": " Detailed Overview of the RAG Process Document Loaders : Facilitate loading documents from a diverse array of sources, boasting over 100 loaders and integrations. Document Transformers : Prepare documents for retrieval by transforming and segmenting them into manageable parts. Text Embedding Models : Generate semantic embeddings of texts, enhancing the relevance and efficiency of data retrieval. Vector Stores : Specialized databases that store and facilitate the search of text embeddings. Retrievers : Employ various retrieval techniques, ranging from simple semantic searches to more sophisticated methods like the Parent Document Retriever and\nEnsemble Retriever.  Clarifying Vector Database Management with MLflow Traditional LangChain Serialization : LangChain typically requires manual management for the serialization of retriever objects, including handling of the vector database. MLflow\u00e2\u0080\u0099s Simplification : The langchain flavor in MLflow substantially simplifies this process. It automates serialization, managing the contents of\nthe persist_dir and the pickling of the loader_fn function.  Key MLflow Components and VectorDB Logging persist_dir : The directory where the retriever object, including the vector database, is stored. loader_fn : The function for loading the retriever object from its storage location.  Important Considerations VectorDB Logging : MLflow, through its langchain flavor, does manage the vector database as part of the retriever object. However, the vector\ndatabase itself is not explicitly logged as a separate entity in MLflow. Runtime VectorDB Maintenance : It\u00e2\u0080\u0099s essential to maintain consistency in the vector database between the training and runtime environments.\nWhile MLflow manages the serialization of the retriever object, ensuring that the same vector database is accessible during runtime remains crucial\nfor consistent performance.  An Example of logging a LangChain RetrievalQA Chain import os import tempfile from langchain.chains import RetrievalQA from langchain.document_loaders import TextLoader from langchain.embeddings.openai import OpenAIEmbeddings from langchain.llms import OpenAI from langchain.text_splitter import CharacterTextSplitter from langchain.vectorstores import FAISS import mlflow assert \"OPENAI_API_KEY\" in os . environ , \"Please set the OPENAI_API_KEY environment variable.\" with tempfile . TemporaryDirectory () as temp_dir : persist_dir = os . path . join ( temp_dir , \"faiss_index\" ) # Create the vector db, persist the db to a local fs folder loader = TextLoader ( \"tests/langchain/state_of_the_union.txt\" ) documents = loader . load () text_splitter = CharacterTextSplitter ( chunk_size = 1000 , chunk_overlap = 0 ) docs = text_splitter . split_documents ( documents ) embeddings = OpenAIEmbeddings () db = FAISS . from_documents ( docs , embeddings ) db . save_local ( persist_dir ) # Create the RetrievalQA chain retrievalQA = RetrievalQA . from_llm ( llm = OpenAI (), retriever = db . as_retriever ()) # Log the retrievalQA chain def load_retriever ( persist_directory ): embeddings = OpenAIEmbeddings () vectorstore = FAISS . load_local ( persist_directory , embeddings ) return vectorstore . as_retriever () with mlflow . start_run () as run : logged_model = mlflow . langchain . log_model ( retrievalQA , artifact_path = \"retrieval_qa\" , loader_fn = load_retriever , persist_dir = persist_dir , ) # Load the retrievalQA chain loaded_model = mlflow . pyfunc . load_model ( logged_model . model_uri ) print ( loaded_model . predict ([{ \"query\" : \"What did the president say about Ketanji Brown Jackson\" }])) The output of the example above is shown below:  Output (truncated) [ \" The president said...\" ]   Logging and Evaluating a LangChain Retriever in MLflow The langchain flavor in MLflow extends its functionalities to include the logging and individual evaluation of retriever objects. This capability is particularly valuable for assessing the quality of documents retrieved by a retriever without needing to process them through a large language model (LLM). ",
        "id": "6f9e2c7ddc7c5901e890d9c0c8e0b1f9"
    },
    {
        "text": " Purpose of Logging Individual Retrievers Independent Evaluation : Allows for the assessment of a retriever\u00e2\u0080\u0099s performance in fetching relevant documents, independent of their subsequent use in LLMs. Quality Assurance : Facilitates the evaluation of the retriever\u00e2\u0080\u0099s effectiveness in sourcing accurate and contextually appropriate documents.  Requirements for Logging Retrievers in MLflow persist_dir : Specifies where the retriever object is stored. loader_fn : Details the function used to load the retriever object from its storage location. These requirements align with those for logging RetrievalQA chains, ensuring consistency in the process.  An example of logging a LangChain Retriever import os import tempfile from langchain.document_loaders import TextLoader from langchain.embeddings.openai import OpenAIEmbeddings from langchain.text_splitter import CharacterTextSplitter from langchain.vectorstores import FAISS import mlflow assert \"OPENAI_API_KEY\" in os . environ , \"Please set the OPENAI_API_KEY environment variable.\" with tempfile . TemporaryDirectory () as temp_dir : persist_dir = os . path . join ( temp_dir , \"faiss_index\" ) # Create the vector database and persist it to a local filesystem folder loader = TextLoader ( \"tests/langchain/state_of_the_union.txt\" ) documents = loader . load () text_splitter = CharacterTextSplitter ( chunk_size = 1000 , chunk_overlap = 0 ) docs = text_splitter . split_documents ( documents ) embeddings = OpenAIEmbeddings () db = FAISS . from_documents ( docs , embeddings ) db . save_local ( persist_dir ) # Define a loader function to recall the retriever from the persisted vectorstore def load_retriever ( persist_directory ): embeddings = OpenAIEmbeddings () vectorstore = FAISS . load_local ( persist_directory , embeddings ) return vectorstore . as_retriever () # Log the retriever with the loader function with mlflow . start_run () as run : logged_model = mlflow . langchain . log_model ( db . as_retriever (), artifact_path = \"retriever\" , loader_fn = load_retriever , persist_dir = persist_dir , ) # Load the retriever chain loaded_model = mlflow . pyfunc . load_model ( logged_model . model_uri ) print ( loaded_model . predict ([{ \"query\" : \"What did the president say about Ketanji Brown Jackson\" }])) The output of the example above is shown below:  Output (truncated) [ [ { \"page_content\" : \"Tonight. I call...\" , \"metadata\" : { \"source\" : \"/state.txt\" }, }, { \"page_content\" : \"A former top...\" , \"metadata\" : { \"source\" : \"/state.txt\" }, }, ] ]  MLflow Langchain Autologging Please refer to the MLflow Langchain Autologging documentation for more details on how to enable autologging for Langchain models. Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "5c98b564b75a66d844a87a7dded38b82"
    },
    {
        "text": "Building a Tool-calling Agent with LlamaIndex Workflow and MLflow 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor MLflow OpenAI Flavor MLflow Sentence-Transformers Flavor MLflow LangChain Flavor MLflow LlamaIndex Flavor Introduction Why use LlamaIndex with MLflow? Getting Started Concepts Usage FAQ MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow LlamaIndex Flavor Building a Tool-calling Agent with LlamaIndex Workflow and MLflow  Building a Tool-calling Agent with LlamaIndex Workflow and MLflow Welcome to this interactive tutorial designed to introduce you to LlamaIndex Workflow and its integration with MLflow. This tutorial is structured as a notebook to provide a hands-on, practical learning experience with Workflow , LlamaIndex\u00e2\u0080\u0099s novel approach to design LLM applications, and managing the development process with MLflow.  What you will learn By the end of this tutorial you will have: Created an MVP agentic application with tool calling functionality in a LlamaIndex Workflow. Observed the agent actions with MLflow Tracing. Logged that workflow to the MLflow Experiment. Loaded the model back and performed inference. Explored the MLflow UI to learn about logged artifacts. Download this Notebook  Installation MLflow\u00e2\u0080\u0099s integration with LlamaIndex\u00e2\u0080\u0099s Workflow API is available in MLflow >= 2.17.0 and LlamaIndex (core) >= 0.11.16. After installing the packages, you may need to restart the Python kernel to correctly load modules. [ ]: % pip install mlflow>=2.17.0 llama-index>=0.11.16 -qqqU # Workflow util is required for rendering Workflow as HTML % pip install llama-index-utils-workflow -qqqU  Choose your favorite LLM By default, LlamaIndex uses OpenAI as the source for LLms and embedding models. If you are signing up with different LLM providers or using a local model, configure them for use by using the Settings object.  Option 1: OpenAI (default) LlamaIndex by default uses OpenAI APIs for LLMs and embeddings models. To proceed with this setting, you just need to set the API key in the environment variable. [1]: import os os . environ [ \"OPENAI_API_KEY\" ] = \"<YOUR_OPENAI_API_KEY>\"  Option 2: Other Hosted LLMs If you want to use other hosted LLMs, Download the integration package for the model provider of your choice. Set up required environment variables as specified in the integration documentation. Instantiate the LLM instance and set it to the global Settings object. The following cells show an example for using Databricks hosted LLMs (Llama3.1 70B instruct). [ ]: % pip install llama-index-llms-databricks [ ]: import os os . environ [ \"DATABRICKS_TOKEN\" ] = \"<YOUR_DATABRICKS_API_TOKEN>\" os . environ [ \"DATABRICKS_SERVING_ENDPOINT\" ] = \"https://YOUR_DATABRICKS_HOST/serving-endpoints/\" [ ]: from llama_index.core import Settings from llama_index.llms.databricks import Databricks llm = Databricks ( model = \"databricks-meta-llama-3-1-70b-instruct\" ) Settings . llm = llm  Option 3: Local LLM LlamaIndex also support locally hosted LLMs. Please refer to the Starter Tutorial (Local Models) for how to set them up.  Create an MLflow Experiemnt Skip this step if you are running this tutorial on a Databricks Notebook. An MLflow experiment is automatically set up when you created any notebook. [ ]: import mlflow mlflow . set_experiment ( \"MLflow LlamaIndex Workflow Tutorial\" ) ",
        "id": "0061d96678d4effa6b295c673929e981"
    },
    {
        "text": " Define tools The agents access with various functions and resources via tool objects. In this example, we define the simplest possible math tools add and multiply based on Python functions. For a real-world application, you can create arbitrary tools such as vector search retrieval, web search, or even calling another agent as a tool. Please refer to the Tools documentation for more details. Please ignore the ### [USE IN MODEL] comment at the beginning of some cells like below. This will be used in later steps in this tutorial! [3]: # [USE IN MODEL] from llama_index.core.tools import FunctionTool def add ( x : int , y : int ) -> int : \"\"\"Useful function to add two numbers.\"\"\" return x + y def multiply ( x : int , y : int ) -> int : \"\"\"Useful function to multiply two numbers.\"\"\" return x * y tools = [ FunctionTool . from_defaults ( add ), FunctionTool . from_defaults ( multiply ), ]  Define Workflow  Workflow Primer LlamaIndex Workflow is an event-driven orchestration framework. At its core, a workflow consists of two fundamental components: Steps and Events . Steps : Units of execution within the workflow. Steps are defined as methods marked with the @step decorator in a class that implements the Workflow base class. Events : Custom objects that trigger steps. Two special events, StartEvent and EndEvent , are reserved for dispatch at the beginning and end of the workflow. Each step specifies its input and output events through its function signature. @step async def my_step ( self , event : StartEvent ) -> FooEvent : # This method triggers when a StartEvent is emitted at the workflow's start, # and then dispatches a FooEvent. Based on each step\u00e2\u0080\u0099s signature and defined events, LlamaIndex automatically constructs the workflow\u00e2\u0080\u0099s execution flow. You may notice that the my_step function is defined as an async function. LlamaIndex Workflow makes asynchronous operations a first-class feature, enabling easy parallel execution and scalable workflows. Another essential component of the workflow is the Context object. This global registry, accessible from any step, allows shared information to be defined without the need to pass it through multiple events. ",
        "id": "e13c4ad14a375a7ce9a6dd1af8a701e0"
    },
    {
        "text": " Define a ReAct Agent as a Workflow The Workflow definition below models a ReAct Agent that utilizes the simple math tools we defined. [4]: # [USE IN MODEL] # Event definitions from llama_index.core.llms import ChatMessage , ChatResponse from llama_index.core.tools import ToolOutput , ToolSelection from llama_index.core.workflow import Event class PrepEvent ( Event ): \"\"\"An event to handle new messages and prepare the chat history\"\"\" class LLMInputEvent ( Event ): \"\"\"An event to prmopt the LLM with the react prompt (chat history)\"\"\" input : list [ ChatMessage ] class LLMOutputEvent ( Event ): \"\"\"An event represents LLM generation\"\"\" response : ChatResponse class ToolCallEvent ( Event ): \"\"\"An event to trigger tool calls, if any\"\"\" tool_calls : list [ ToolSelection ] class ToolOutputEvent ( Event ): \"\"\"An event to handle the results of tool calls, if any\"\"\" output : ToolOutput [15]: # [USE IN MODEL] # Workflow definition from llama_index.core import Settings from llama_index.core.agent.react import ReActChatFormatter , ReActOutputParser from llama_index.core.agent.react.types import ActionReasoningStep , ObservationReasoningStep from llama_index.core.memory import ChatMemoryBuffer from llama_index.core.workflow import ( Context , StartEvent , StopEvent , Workflow , step , ) class ReActAgent ( Workflow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . tools = tools # Store the chat history in memory so the agent can handle multiple interactions with users. self . memory = ChatMemoryBuffer . from_defaults ( llm = Settings . llm ) @step async def new_user_msg ( self , ctx : Context , ev : StartEvent ) -> PrepEvent : \"\"\"Start workflow with the new user messsage\"\"\" # StartEvent carries whatever keys passed to the workflow's run() method as attributes. user_input = ev . input user_msg = ChatMessage ( role = \"user\" , content = user_input ) self . memory . put ( user_msg ) # We store the executed reasoning steps in the context. Clear it at the start. await ctx . set ( \"steps\" , []) return PrepEvent () @step async def prepare_llm_prompt ( self , ctx : Context , ev : PrepEvent ) -> LLMInputEvent : \"\"\"Prepares the react prompt, using the chat history, tools, and current reasoning (if any)\"\"\" steps = await ctx . get ( \"steps\" , default = []) chat_history = self . memory . get () # Construct an LLM from the chat history, tools, and current reasoning, using the # built-in prompt template. llm_input = ReActChatFormatter () . format ( self . tools , chat_history , current_reasoning = steps ) return LLMInputEvent ( input = llm_input ) @step async def invoke_llm ( self , ev : LLMInputEvent ) -> LLMOutputEvent : \"\"\"Call the LLM with the react prompt\"\"\" response = await Settings . llm . achat ( ev . input ) retur",
        "id": "35213c43b95e61e900108ce866ab8c04"
    },
    {
        "text": "ctChatFormatter () . format ( self . tools , chat_history , current_reasoning = steps ) return LLMInputEvent ( input = llm_input ) @step async def invoke_llm ( self , ev : LLMInputEvent ) -> LLMOutputEvent : \"\"\"Call the LLM with the react prompt\"\"\" response = await Settings . llm . achat ( ev . input ) return LLMOutputEvent ( response = response ) @step async def handle_llm_response ( self , ctx : Context , ev : LLMOutputEvent ) -> ToolCallEvent | PrepEvent | StopEvent : \"\"\" Parse the LLM response to extract any tool calls requested. If theere is no tool call, we can stop and emit a StopEvent. Otherwise, we emit a ToolCallEvent to handle tool calls. \"\"\" try : step = ReActOutputParser () . parse ( ev . response . message . content ) ( await ctx . get ( \"steps\" , default = [])) . append ( step ) if step . is_done : # No additional tool call is required. Ending the workflow by emitting StopEvent. return StopEvent ( result = step . response ) elif isinstance ( step , ActionReasoningStep ): # Tool calls are returned from LLM, trigger the tool call event. return ToolCallEvent ( tool_calls = [ ToolSelection ( tool_id = \"fake\" , tool_name = step . action , tool_kwargs = step . action_input , ) ] ) except Exception as e : error_step = ObservationReasoningStep ( observation = f \"There was an error in parsing my reasoning: { e } \" ) ( await ctx . get ( \"steps\" , default = [])) . append ( error_step ) # if no tool calls or final response, iterate again return PrepEvent () @step async def handle_tool_calls ( self , ctx : Context , ev : ToolCallEvent ) -> PrepEvent : \"\"\" Safely calls tools with error handling, adding the tool outputs to the current reasoning. Then, by emitting a PrepEvent, we loop around for another round of ReAct prompting and parsing. \"\"\" tool_calls = ev . tool_calls tools_by_name = { tool . metadata . get_name (): tool for tool in self . tools } # call tools -- safely! for tool_call in tool_calls : if tool := tools_by_name . get ( tool_call . tool_name ): try : tool_output = tool ( ** tool_call . tool_kwargs ) step = ObservationReasoningStep ( observation = tool_output . content ) except Exception as e : step = ObservationReasoningStep ( observation = f \"Error calling tool { tool . metadata . get_name () } : { e } \" ) else : step = ObservationReasoningStep ( observation = f \"Tool { tool_call . tool_name } does not exist\" ) ( await ctx . get ( \"steps\" , default = [])) . append ( step ) # prep the next iteration return PrepEvent () ",
        "id": "73bac91b2a43c1f995769f202f96bb1f"
    },
    {
        "text": " Check the Workflow Visually Before instantiating the agent object, let\u00e2\u0080\u0099s pause and validate if the workflow is constructed as we expect. To check that, we can render the graphical representation of the workflow by using the draw_all_possible_flows utility function. (Note: If the rendered HTML is blank, it might be due to the safety feature in Jupyter. In that case, you can trust the notebook by !jupyter trust llama_index_workflow_tutorial.ipynb . See Jupyter documentation for more details.) [ ]: from IPython.display import HTML from llama_index.utils.workflow import draw_all_possible_flows draw_all_possible_flows ( ReActAgent , filename = \"workflow.html\" ) with open ( \"workflow.html\" ) as file : html_content = file . read () HTML ( html_content ) [17]: # [USE IN MODEL] agent = ReActAgent ( timeout = 180 )  Run the Workflow (with Trace) Now your workflow is all set! But before running that, let\u00e2\u0080\u0099s not forget to turn on MLflow Tracing , so you get observability into each step during the agent run, and record it for the review later. Mlflow supports automatic tracing for LlamaIndex Workflow. To enable it, you just need to call the mlflow.llama_index.autolog() function. [12]: import mlflow mlflow . llama_index . autolog () [18]: # Run the workflow await agent . run ( input = \"What is (123 + 456) * 789?\" ) [18]: 'The result of (123 + 456) * 789 is 579,027.'  Review the Trace The generated traces are automatically recorded to your MLflow Experiment. Open a terminal, run mlflow ui --port 5000 within the current directory (and keep it running). Navigate to http://127.0.0.1:5000 in your browser. Open the experiment \u00e2\u0080\u009cMLflow LlamaIndex Workflow Tutorial\u00e2\u0080\u009d. Navigate to the \u00e2\u0080\u009cTrace\u00e2\u0080\u009d tab below the experiment name header. The Trace records the individual steps inside the workflow execution with its inputs, outputs, and additional metadata such as latency. Let\u00e2\u0080\u0099s do a quick exercise to find the following information on the Trace UI. Token count used for the first LLM invocation <p>You can find token counts for LLm call in the <strong>Attribtues</strong> section of the LLM call span, inside the <code>usage</code> field.</p> Input numbers for the \u00e2\u0080\u009cadd\u00e2\u0080\u009d tool call. You can find input numbers x=123 and y=456 in the Inputs field of the span named FunctionTool.call. That span is located under the ReActAgent.handle_tool_calls step span.  Log the Workflow to an MLflow Experiment Now that you\u00e2\u0080\u0099ve built your first ReAct Agent using LlamaIndex Workflow, it\u00e2\u0080\u0099s essential to iteratively refine and optimize for better performance. An MLflow Experiment is the ideal place to record and manage these improvements ",
        "id": "2f67d98458ca2bfddbb30f07f3af8e3d"
    },
    {
        "text": " Prepare a Model script MLflow supports logging LlamaIndex workflows using the Models from Code method, allowing models to be defined and logged directly from a standalone Python script. This approach bypasses the need for risky and brittle serialization methods like pickle , using code as the single source of truth for the model definition. Combined with MLflow\u00e2\u0080\u0099s environment-freezing capability, this provides a reliable way to persist the model. For more details, see the MLflow documentation . You could manually create a separate Python file by copying the code from this notebook. However, for convenience, we define a utility function to generate a model script automatically from this notebook\u00e2\u0080\u0099s content in one step. Running the cell below will create this script in the current directory, ready for MLflow logging. [22]: def generate_model_script ( output_path , notebook_path = \"llama_index_workflow_tutorial.ipynb\" ): \"\"\" A utility function to generate a ready-to-log .py script that contains necessary library imports and model definitions. Args: output_path: The path to write the .py file to. notebook_path: The path to the tutorial notebook. \"\"\" import nbformat with open ( notebook_path , encoding = \"utf-8\" ) as f : notebook = nbformat . read ( f , as_version = 4 ) # Filter cells that are code cells and contain the specified marker merged_code = ( \" \\n\\n \" . join ( [ cell . source for cell in notebook . cells if cell . cell_type == \"code\" and cell . source . startswith ( \"# [USE IN MODEL]\" ) ] ) + \" \\n\\n import mlflow \\n\\n mlflow.models.set_model(agent)\" ) # Write to the output .py file with open ( output_path , \"w\" , encoding = \"utf-8\" ) as f : f . write ( merged_code ) print ( f \"Model code saved to { output_path } \" ) # Pass `notebook_path` argument if you changed the notebook name generate_model_script ( output_path = \"react_agent.py\" ) Model code saved to react_agent.py  Logging the Model [ ]: import mlflow with mlflow . start_run ( run_name = \"react-agent-workflow\" ): model_info = mlflow . llama_index . log_model ( \"react_agent.py\" , artifact_path = \"model\" , # Logging with an input example help MLflow to record dependency and signature information accurately. input_example = { \"input\" : \"What is (123 + 456) * 789?\" }, )  Explore the MLflow UI Let\u00e2\u0080\u0099s open the MLflow UI again to see which information is being tracked in the experiment. Access the MLflow UI like we did for reviewing traces. Open the experiment \u00e2\u0080\u009cMLflow LlamaIndex Workflow Tutorial\u00e2\u0080\u009d. The Runs tab in the experiment should contain a run named \u00e2\u0080\u009creact-agent-workflow\u00e2\u0080\u009d. Open it. On the run page, navigate to the \"Artifacts\" tab. The artifacts tab shows various files saved by MLflow in the Run. See the below image and open the annotated files to check which information is stored in each file.  Load the Model Back for Inference With all necessary metadata logged to MLflow, you can load the model in a different notebook or deploy it for inference without concerns about environment inconsistencies. Let\u00e2\u0080\u0099s do a quick exercise to demonstrate how this helps in reproducing experiment results. To simulate a different environment, we\u00e2\u0080\u0099ll remove the llm configuration from the global Settings object. [24]: from llama_index.core.llms import MockLLM Settings . llm = MockLLM ( max_tokens = 1 ) await agent . run ( input = \"What is (123 + 456) * 789?\" ) [24]: 'text' Since the dummy LLM is configured, the workflow could not generate the correct output but just returns \u00e2\u0080\u009ctext\u00e2\u0080\u009d. Now try loading the model back from the MLflow Experiment by calling mlflow.llama_index.load_model() API and run the workflow again. [ ]: loaded_model = mlflow . llama_index . load_model ( \"runs:/f8e0a0d2dd5546d5ac93ce126358c444/model\" ) await loaded_model . run ( input = \"What is (123 + 456) * 789?\" ) '(123 + 456) * 789 = 456831' This time, the output is computed correctly, because MLflow automatically restores the original LLM setting at the time of logging. ",
        "id": "a98ca8f9a604dae613e8fdcb14fcc609"
    },
    {
        "text": " Learning More Congratulations! \u00f0\u009f\u008e\u0089 You\u00e2\u0080\u0099ve successfully learned how to build a tool-calling agent using LlamaIndex Workflow and MLflow. Continue your journey with these advanced resources: Improve Workflow Quality : Evaluate your workflow to enhance performance with MLflow LLM Evaluation . Deploy Your Model : Deploy your MLflow model to a serving endpoint with MLflow Deployment . Explore More Examples : Discover additional examples of LlamaIndex Workflow in the official documentation . Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "f74d1498bb9c398d684999d8dfb1e9d8"
    },
    {
        "text": "Introduction to Using LlamaIndex with MLflow 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor MLflow OpenAI Flavor MLflow Sentence-Transformers Flavor MLflow LangChain Flavor MLflow LlamaIndex Flavor Introduction Why use LlamaIndex with MLflow? Getting Started Concepts Usage FAQ MLflow DSPy Flavor Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow LlamaIndex Flavor Introduction to Using LlamaIndex with MLflow  Introduction to Using LlamaIndex with MLflow Welcome to this interactive tutorial designed to introduce you to LlamaIndex and its integration with MLflow. This tutorial is structured as a notebook to provide a hands-on, practical learning experience with the simplest and most core features of LlamaIndex. Download this Notebook  What you will learn By the end of this tutorial you will have: Created an MVP VectorStoreIndex in LlamaIndex. Logged that index to the MLflow tracking server. Registered that index to the MLflow model registry. Loaded the model and performed inference. Explored the MLflow UI to learn about logged artifacts. These basics will familiarize you with the LlamaIndex user journey in MLlfow.  Setup First, we must ensure we have the required dependecies and environment variables. By default, LlamaIndex uses OpenAI as the source for LLMs and embeding models, so we\u00e2\u0080\u0099ll do the same. Let\u00e2\u0080\u0099s start by installing the requisite libraries and providing an OpenAI API key. [1]: % pip install mlflow>=2.15 llama-index>=0.10.44 -q Note: you may need to restart the kernel to use updated packages. [2]: import os from getpass import getpass from llama_index.core import Document , VectorStoreIndex from llama_index.core.llms import ChatMessage import mlflow os . environ [ \"OPENAI_API_KEY\" ] = getpass ( \"Enter your OpenAI API key: \" ) [21]: assert \"OPENAI_API_KEY\" in os . environ , \"Please set the OPENAI_API_KEY environment variable.\" ",
        "id": "d584d270775ac9d09790e90d0bdfaebf"
    },
    {
        "text": " Create a Index Vector store indexes are one of the core components in LlamaIndex. They contain embedding vectors of ingested document chunks (and sometimes the document chunks as well). These vectors enable various types of inference, such as query engines, chat engines, and retrievers, each serving different purposes in LlamaIndex. Query Engine: Usage: Perform straightforward queries to retrieve relevant information based on a user\u00e2\u0080\u0099s question. Scenario: Ideal for fetching concise answers or documents matching specific queries, similar to a search engine. Chat Engine: Usage: Engage in conversational AI tasks that require maintaining context and history over multiple interactions. Scenario: Suitable for interactive applications like customer support bots or virtual assistants, where conversation context is important. Retriever: Usage: Retrieve documents or text segments that are semantically similar to a given input. Scenario: Useful in retrieval-augmented generation (RAG) systems to fetch relevant context or background information, enhancing the quality of generated responses in tasks like summarization or question answering. By leveraging these different types of inference, LlamaIndex allows you to build robust AI applications tailored to various use cases, enhancing interaction between users and large language models. [4]: print ( \"------------- Example Document used to Enrich LLM Context -------------\" ) llama_index_example_document = Document . example () print ( llama_index_example_document ) index = VectorStoreIndex . from_documents ([ llama_index_example_document ]) print ( \" \\n ------------- Example Query Engine -------------\" ) query_response = index . as_query_engine () . query ( \"What is llama_index?\" ) print ( query_response ) print ( \" \\n ------------- Example Chat Engine  -------------\" ) chat_response = index . as_chat_engine () . chat ( \"What is llama_index?\" , chat_history = [ ChatMessage ( role = \"system\" , content = \"You are an expert on RAG!\" )], ) print ( chat_response ) print ( \" \\n ------------- Example Retriever   -------------\" ) retriever_response = index . as_retriever () . retrieve ( \"What is llama_index?\" ) print ( retriever_response ) ------------- Example Document used to Enrich LLM Context -------------\nDoc ID: e4c638ce-6757-482e-baed-096574550602\nText: Context LLMs are a phenomenal piece of technology for knowledge\ngeneration and reasoning. They are pre-trained on large amounts of\npublicly available data. How do we best augment LLMs with our own\nprivate data? We need a comprehensive toolkit to help perform this\ndata augmentation for LLMs.  Proposed Solution That's where LlamaIndex\ncomes in. Ll...\n\n------------- Example Query Engine -------------\nLlamaIndex is a \"data framework\" designed to assist in building LLM apps by offering tools such as data connectors for various data sources, ways to structure data for easy use with LLMs, an advanced retrieval/query interface, and integrations with different application frameworks. It caters to both beginner and advanced users, providing a high-level API for simple data ingestion and querying, as well as lower-level APIs for customization and extension of different modules to suit individual needs.\n\n------------- Example Chat Engine  -------",
        "id": "b0a7316aebca0c8e8e2d803d477a7d0f"
    },
    {
        "text": "LMs, an advanced retrieval/query interface, and integrations with different application frameworks. It caters to both beginner and advanced users, providing a high-level API for simple data ingestion and querying, as well as lower-level APIs for customization and extension of different modules to suit individual needs.\n\n------------- Example Chat Engine  -------------\nLlamaIndex is a data framework designed to assist in building LLM apps by providing tools such as data connectors for various data sources, ways to structure data for easy use with LLMs, an advanced retrieval/query interface, and integrations with different application frameworks. It caters to both beginner and advanced users with a high-level API for easy data ingestion and querying, as well as lower-level APIs for customization and extension of different modules to suit specific needs.\n\n------------- Example Retriever   -------------\n[NodeWithScore(node=TextNode(id_='d18bb1f1-466a-443d-98d9-6217bf71ee5a', embedding=None, metadata={'filename': 'README.md', 'category': 'codebase'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e4c638ce-6757-482e-baed-096574550602', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filename': 'README.md', 'category': 'codebase'}, hash='3183371414f6a23e9a61e11b45ec45f808b148f9973166cfed62226e3505eb05')}, text='Context\\nLLMs are a phenomenal piece of technology for knowledge generation and reasoning.\\nThey are pre-trained on large amounts of publicly available data.\\nHow do we best augment LLMs with our own private data?\\nWe need a comprehensive toolkit to help perform this data augmentation for LLMs.\\n\\nProposed Solution\\nThat\\'s where LlamaIndex comes in. LlamaIndex is a \"data framework\" to help\\nyou build LLM  apps. It provides the following tools:\\n\\nOffers data connectors to ingest your existing data sources and data formats\\n(APIs, PDFs, docs, SQL, etc.)\\nProvides ways to structure your data (indices, graphs) so that this data can be\\neasily used with LLMs.\\nProvides an advanced retrieval/query interface over your data:\\nFeed in any LLM input prompt, get back retrieved context and knowledge-augmented output.\\nAllows easy integrations with your outer application framework\\n(e.g. with LangChain, Flask, Docker, ChatGPT, anything else).\\nLlamaIndex provides tools for both beginner users and advanced users.\\nOur high-level API allows beginner users to use LlamaIndex to ingest and\\nquery their data in 5 lines of code. Our lower-level APIs allow advanced users to\\ncustomize and extend any module (data connectors, indices, retrievers, query engines,\\nreranking modules), to fit their needs.', mimetype='text/plain', start_char_idx=1, end_char_idx=1279, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.850998849877966)] ",
        "id": "7ac23384af0cbe0be7c47c69746dd8b1"
    },
    {
        "text": " Log the Index with MLflow The below code logs a LlamaIndex model with MLflow, allowing you to persist and manage it across different environments. By using MLflow, you can track, version, and reproduce your model reliably. The script logs parameters, an example input, and registers the model under a specific name. The model_uri provides a unique identifier for retrieving the model later. This persistence is essential for ensuring consistency and reproducibility in development, testing, and production. Managing the\nmodel with MLflow simplifies loading, deployment, and sharing, maintaining an organized workflow. Key Parameters engine_type : defines the pyfunc and spark_udf inference type input_example : defines the the input signature and infers the output signature via a prediction registered_model_name : defines the name of the model in the MLflow model registry [5]: mlflow . llama_index . autolog () # This is for enabling tracing with mlflow . start_run () as run : mlflow . llama_index . log_model ( index , artifact_path = \"llama_index\" , engine_type = \"query\" , # Defines the pyfunc and spark_udf inference type input_example = \"hi\" , # Infers signature registered_model_name = \"my_llama_index_vector_store\" , # Stores an instance in the model registry ) run_id = run . info . run_id model_uri = f \"runs:/ { run_id } /llama_index\" print ( f \"Unique identifier for the model location for loading: { model_uri } \" ) 2024/07/24 17:58:27 INFO mlflow.llama_index.serialize_objects: API key(s) will be removed from the global Settings object during serialization to protect against key leakage. At inference time, the key(s) must be passed as environment variables.\n/Users/michael.berk/opt/anaconda3/envs/mlflow-dev/lib/python3.8/site-packages/_distutils_hack/__init__.py:26: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\nSuccessfully registered model 'my_llama_index_vector_store'.\nCreated version '1' of model 'my_llama_index_vector_store'. Unique identifier for the model location for loading: runs:/036936a7ac964f0cb6ab99fa908d6421/llama_index ",
        "id": "0215955feee1c0888be60da2e8a23af7"
    },
    {
        "text": " Load the Index and Perform Inference The below code demonstrates three core types of inference that can be done with the loaded model. Load and Perform Inference via LlamaIndex: This method loads the model using mlflow.llama_index.load_model and performs direct querying, chat, or retrieval. It is ideal when you want to leverage the full capabilities of the underlying llama index object. Load and Perform Inference via MLflow PyFunc: This method loads the model using mlflow.pyfunc.load_model , enabling model predictions in a generic PyFunc format, with the engine type specified at logging time. It is useful for evaluating the model with mlflow.evaluate or deploying the model for serving. Load and Perform Inference via MLflow Spark UDF: This method uses mlflow.pyfunc.spark_udf to load the model as a Spark UDF, facilitating distributed inference across large datasets in a Spark DataFrame. It is ideal for handling large-scale data processing and, like with PyFunc inference, only supports the engine type defined when logging. [8]: print ( \" \\n ------------- Inference via Llama Index   -------------\" ) index = mlflow . llama_index . load_model ( model_uri ) query_response = index . as_query_engine () . query ( \"hi\" ) print ( query_response ) print ( \" \\n ------------- Inference via MLflow PyFunc -------------\" ) index = mlflow . pyfunc . load_model ( model_uri ) query_response = index . predict ( \"hi\" ) print ( query_response ) 2024/07/24 18:02:21 WARNING mlflow.tracing.processor.mlflow: Creating a trace within the default experiment with id '0'. It is strongly recommended to not use the default experiment to log traces due to ambiguous search results and probable performance issues over time due to directory table listing performance degradation with high volumes of directories within a specific path. To avoid performance and disambiguation issues, set the experiment for your environment using `mlflow.set_experiment()` API. ------------- Inference via Llama Index   ------------- 2024/07/24 18:02:22 WARNING mlflow.tracing.processor.mlflow: Creating a trace within the default experiment with id '0'. It is strongly recommended to not use the default experiment to log traces due to ambiguous search results and probable performance issues over time due to directory table listing performance degradation with high volumes of directories within a specific path. To avoid performance and disambiguation issues, set the experiment for your environment using `mlflow.set_experiment()` API. Hello! How can I assist you today?\n\n------------- Inference via MLflow PyFunc -------------\nHello! How can I assist you today? [6]: # Optional: Spark UDF inference show_spark_udf_inference = False if show_spark_udf_inference : print ( \" \\n ------------- Inference via MLflow Spark UDF -------------\" ) from pyspark.sql import SparkSession spark = SparkSession . builder . getOrCreate () udf = mlflow . pyfunc . spark_udf ( spark , model_uri , result_type = \"string\" ) df = spark . createDataFrame ([( \"hi\" ,), ( \"hello\" ,)], [ \"text\" ]) df . withColumn ( \"response\" , udf ( \"text\" )) . toPandas () ",
        "id": "8e49b2174a032de1f43fd8028e8052f3"
    },
    {
        "text": " Explore the MLflow UI Finally, let\u00e2\u0080\u0099s explore what\u00e2\u0080\u0099s happening under the hood. To open the MLflow UI, run the following cell. Note that you can also run this in a new CLI window at the same directory that contains your mlruns folder, which by default will be this notebook\u00e2\u0080\u0099s directory. [7]: import os import subprocess from IPython.display import IFrame # Start the MLflow UI in a background process mlflow_ui_command = [ \"mlflow\" , \"ui\" , \"--port\" , \"5000\" ] subprocess . Popen ( mlflow_ui_command , stdout = subprocess . PIPE , stderr = subprocess . PIPE , preexec_fn = os . setsid ) [7]: <subprocess.Popen at 0x7fbe09399ee0> [ ]: # Wait for the MLflow server to start then run the following command # Note that cached results don't render, so you need to run this to see the UI IFrame ( src = \"http://localhost:5000\" , width = 1000 , height = 600 ) Let\u00e2\u0080\u0099s navigate to the experiments tab in the top left of the screen and click on our most recent run, as shown in the image below. MLflow logs artifacts associated with your model and its environment during the MLflow run. Most of the logged files, such as the conda.yaml , python_env.yml , and requirements.txt are standard to all MLflow logging and facilitate reproducibility between environments. However, there are two sets of artifacts that are specific to LlamaIndex: index : a directory that stores the serialized vector store. For more details, visit LlamaIndex\u00e2\u0080\u0099s serialization docs . settings.json : the serialized llama_index.core.Settings service context. For more details, visit LlamaIndex\u00e2\u0080\u0099s Settings docs By storing these objects, MLflow is able to recreate the environment in which you logged your model. Important: MLflow will not serialize API keys. Those must be present in your model loading environment as environment variables. We also created a record of the model in the model registry. By simply specifying registered_model_name and input_example when logging the model, we get robust signature inference and an instance in the model registry, as shown below. Finally, let\u00e2\u0080\u0099s explore the traces we logged. In the Experiments tab we can click on Tracing to view the logged traces for our two inference calls. Tracing effectively shows a callback-based stacktrace for what ocurred in our inference system. If we click on our first trace, we can see some really cool details about our inputs, outputs, and the duration of each step in the chain.  Customization and Next Steps When working with production systems, typically users leverage a customized service context, which can be done via LlamaIndex\u00e2\u0080\u0099s Settings object. Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "80b5d81f3ff01a65a80ae7761f959827"
    },
    {
        "text": "DSPy Quickstart 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs MLflow Transformers Flavor MLflow OpenAI Flavor MLflow Sentence-Transformers Flavor MLflow LangChain Flavor MLflow LlamaIndex Flavor MLflow DSPy Flavor Introduction Why use DSPy with MLflow? Getting Started Concepts Usage FAQ Explore the Native LLM Flavors LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow DSPy Flavor DSPy Quickstart  DSPy Quickstart DSPy simplifies building language model (LM) pipelines by replacing manual prompt engineering with structured \u00e2\u0080\u009ctext transformation graphs.\u00e2\u0080\u009d These graphs use flexible, learning modules that automate and optimize LM tasks like reasoning, retrieval, and answering complex questions.  How does it work? At a high level, DSPy optimizes prompts, selects the best language model, and can even fine-tune the model using training data. The process follows these three steps, common to most DSPy optimizers : Candidate Generation : DSPy finds all Predict modules in the program and generates variations of instructions and demonstrations (e.g., examples for prompts). This step creates a set of possible candidates for the next stage. Parameter Optimization : DSPy then uses methods like random search, TPE, or Optuna to select the best candidate. Fine-tuning models can also be done at this stage.  This Demo Below we create a simple program that demonstrates the power of DSPy. We will build a text classifier leveraging OpenAI. By the end of this tutorial, we will\u00e2\u0080\u00a6 Define a dspy.Signature and dspy.Module to perform text classification. Leverage dspy.teleprompt.BootstrapFewShotWithRandomSearch to compile our module so it\u00e2\u0080\u0099s better at classifying our text. Analyze internal steps with MLflow Tracing. Log the compiled model with MLflow. Load the logged model and perform inference. [ ]: % pip install -U openai dspy>=2.5.17 mlflow>=2.18.0 zsh:1: 2.5.1 not found\nNote: you may need to restart the kernel to use updated packages.  Setup  Set Up LLM After installing the relevant dependencies, let\u00e2\u0080\u0099s leverage a Databricks foundational model serving endpoint as our LLM of choice. Here, will leverage OpenAI\u00e2\u0080\u0099s gpt-4o-mini model. [ ]: # Set OpenAI API Key to the environment variable. You can also pass the token to dspy.LM() import getpass import os os . environ [ \"OPENAI_API_KEY\" ] = getpass . getpass ( \"Enter your OpenAI Key:\" ) [8]: import dspy # Define your model. We will use OpenAI for simplicity model_name = \"gpt-4o-mini\" # Leverage default authentication inside the Databricks context (notebooks, workflows, etc.) # Note that an OPENAI_API_KEY environment must be present. You can also pass the token to dspy.LM() lm = dspy . LM ( model = f \"openai/ { model_name } \" , max_tokens = 500 , temperature = 0.1 , ) dspy . settings . configure ( lm = lm )  Create MLflow Experiment Create a new MLflow Experiment to track your DSPy models, metrics, parameters, and traces in one place. Although there is already a \u00e2\u0080\u009cdefault\u00e2\u0080\u009d experiment created in your workspace, it is highly recommended to create one for different tasks to organize experiment artifacts. \u00f0\u009f\u0092\u00a1 Skip this step if you are running this tutorial on a Databricks Notebook. An MLflow experiment is automatically set up when you created any notebook. [ ]: import mlflow mlflow . set_experiment ( \"DSPy Quickstart\" ) ",
        "id": "533908799b1fc4355be57841dd3dc5ea"
    },
    {
        "text": " Turn on Auto Tracing with MLflow MLflow Tracing is a powerful observability tool for monitoring and debugging what happens inside your DSPy modules, helping you identify potential bottlenecks or issues quickly. To enable DSPy tracing, you just need to call mlflow.dspy.autolog and that\u00e2\u0080\u0099s it! [ ]: mlflow . dspy . autolog () ",
        "id": "1bdc3db0998287a1e590f974b7e2e437"
    },
    {
        "text": " Set Up Data Next, we will download the Reuters 21578 dataset from Huggingface. We also write a utility to ensure that our train/test split has the same labels. [13]: import numpy as np import pandas as pd from dspy.datasets.dataset import Dataset def read_data_and_subset_to_categories () -> tuple [ pd . DataFrame ]: \"\"\" Read the reuters-21578 dataset. Docs can be found in the url below: https://huggingface.co/datasets/yangwang825/reuters-21578 \"\"\" # Read train/test split file_path = \"hf://datasets/yangwang825/reuters-21578/ {} .json\" train = pd . read_json ( file_path . format ( \"train\" )) test = pd . read_json ( file_path . format ( \"test\" )) # Clean the labels label_map = { 0 : \"acq\" , 1 : \"crude\" , 2 : \"earn\" , 3 : \"grain\" , 4 : \"interest\" , 5 : \"money-fx\" , 6 : \"ship\" , 7 : \"trade\" , } train [ \"label\" ] = train [ \"label\" ] . map ( label_map ) test [ \"label\" ] = test [ \"label\" ] . map ( label_map ) return train , test class CSVDataset ( Dataset ): def __init__ ( self , n_train_per_label : int = 20 , n_test_per_label : int = 10 , * args , ** kwargs ) -> None : super () . __init__ ( * args , ** kwargs ) self . n_train_per_label = n_train_per_label self . n_test_per_label = n_test_per_label self . _create_train_test_split_and_ensure_labels () def _create_train_test_split_and_ensure_labels ( self ) -> None : \"\"\"Perform a train/test split that ensure labels in `dev` are also in `train`.\"\"\" # Read the data train_df , test_df = read_data_and_subset_to_categories () # Sample for each label train_samples_df = pd . concat ( [ group . sample ( n = self . n_train_per_label ) for _ , group in train_df . groupby ( \"label\" )] ) test_samples_df = pd . concat ( [ group . sample ( n = self . n_test_per_label ) for _ , group in test_df . groupby ( \"label\" )] ) # Set DSPy class variables self . _train = train_samples_df . to_dict ( orient = \"records\" ) self . _dev = test_samples_df . to_dict ( orient = \"records\" ) # Limit to a small dataset to showcase the value of bootstrapping dataset = CSVDataset ( n_train_per_label = 3 , n_test_per_label = 1 ) # Create train and test sets containing DSPy # Note that we must specify the expected input value name train_dataset = [ example . with_inputs ( \"text\" ) for example in dataset . train ] test_dataset = [ example . with_inputs ( \"text\" ) for example in dataset . dev ] unique_train_labels = { example . label for example in dataset . train } print ( len ( train_dataset ), len ( test_dataset )) print ( f \"Train labels: { unique_train_labels } \" ) print ( train_dataset [ 0 ]) 24 8\nTrain labels: {'interest', 'acq', 'grain', 'earn', 'money-fx', 'ship', 'crude', 'trade'}\nExample({'label': 'interest', 'text': 'u s urges banks to weigh philippine debt plan the u s is urging reluctant commercial banks to seriously consider accepting a novel philippine proposal for paying its interest bill and believes the innovation is fully consistent with its third world debt strategy a reagan administration official said the official s comments also suggest that debtors pleas for interest rate concessions should be treated much more seriously by the commercial banks in cases where developing nations are carrying out genuine economic reforms in addition he signaled that the banks might want to reconsider the idea of a megabank where third world debt would be pooled and suggested the administration would support such a plan even though it was not formally proposing it at the same time however the official expressed reservations that such a scheme would ever get off the ground the philippine proposal together with argentine suggestions that exit bonds be issued to end the troublesome role of small banks in the debt strategy would help to underpin the flagging role of private banks within the plan the official said in an interview with reuters all of these things would fit within the definition of our initiative as we have asked it and we think any novel and unique approach such as those should be considered said the official who asked not to be named in october washington outlined a debt crisis strategy under which commercial banks and multilateral institutions such as the world bank and the international monetary fund imf were urged to step up lending to major debtors nations in return america called on the debtor countries to enact economic reforms promoting inflatio",
        "id": "6b534110ab8b3d90de236b71799557c2"
    },
    {
        "text": "f our initiative as we have asked it and we think any novel and unique approach such as those should be considered said the official who asked not to be named in october washington outlined a debt crisis strategy under which commercial banks and multilateral institutions such as the world bank and the international monetary fund imf were urged to step up lending to major debtors nations in return america called on the debtor countries to enact economic reforms promoting inflation free economic growth the multilaterals have been performing well the debtors have been performing well said the official but he admitted that the largest third world debtor brazil was clearly an exception the official who played a key role in developing the u s debt strategy and is an administration economic policymaker also said these new ideas would help commercial banks improve their role in resolving the third world debt crisis we called at the very beginning for the bank syndications to find procedures or processes whereby they could operate more effectively the official said among those ideas the official said were suggestions that commercial banks create a megabank which could swap third world debt paper for so called exit bonds for banks like regional american or european institutions such bonds in theory would rid these banks of the need to lend money to their former debtors every time a new money package was assembled and has been suggested by argentina in its current negotiations for a new loan of billion dlrs he emphasised that the megabank was not an administration plan but something some people have suggested other u s officials said japanese commercial banks are examining the creation of a consortium bank to assume third world debt this plan actively under consideration would differ slightly from the one the official described but the official expressed deep misgivings that such a plan would work in the united states if the banks thought that that was a suitable way to go fine i don t think they ever will he pointed out that banks would swap their third world loans for capital in the megabank and might then be reluctant to provide new money to debtors through the new institution meanwhile the official praised the philippine plan under which it would make interest payments on its debt in cash at no more than pct above libor the philippine proposal is very interesting it s quite unique and i don t think it s something that should be categorically rejected out of hand the official said banks which found this level unacceptably low would be offered an alternative of libor payments in cash and a margin above that of one pct in the form of philippine investment notes these tradeable dollar denominated notes would have a six year life and if banks swapped them for cash before maturity the country would guarantee a payment of point over libor until now bankers have criticised these spreads as far too low the talks now in their second week are aimed at stretching out repayments of billion dlrs of debt and granting easier terms on billion of already rescheduled debt the country which has enjoyed strong political support in washington since corazon aquino came to power early last year owes an overall billion dlrs of debt but the official denied the plan amounts to interest rate capitalisation a development until now unacceptable to the banks it s no more interest rate capitalisation than if you have a write down in the spread over libor from what existed before the official said in comments suggesting some ought to be granted the rate concessions they seek some people argue that cutting the spread is debt forgiveness what it really is is narrowing the spread on new money he added he said the u s debt strategy is sufficiently broad as an initiative to include plans like the philippines reuter'}) (input_keys={'text'}) ",
        "id": "04be885e2f4d061880af62f884c2a71a"
    },
    {
        "text": " Set up DSPy Signature and Module Finally, we will define our task: text classification. There are a variety of ways you can provide guidelines to DSPy signature behavior. Currently, DSPy allows users to specify: A high-level goal via the class docstring. A set of input fields, with optional metadata. A set of output fields with optional metadata. DSPy will then leverage this information to inform optimization. In the below example, note that we simply provide the expected labels to output field in the TextClassificationSignature class. From this initial state, we\u00e2\u0080\u0099ll look to use DSPy to learn to improve our classifier accuracy. [15]: class TextClassificationSignature ( dspy . Signature ): text = dspy . InputField () label = dspy . OutputField ( desc = f \"Label of predicted class. Possible labels are { unique_train_labels } \" ) class TextClassifier ( dspy . Module ): def __init__ ( self ): super () . __init__ () self . generate_classification = dspy . Predict ( TextClassificationSignature ) def forward ( self , text : str ): return self . generate_classification ( text = text )  Run it!  Hello World Let\u00e2\u0080\u0099s demonstrate predicting via the DSPy module and associated signature. The program has correctly learned our labels from the signature desc field and generates reasonable predictions. [16]: from copy import copy # Initilize our impact_improvement class text_classifier = copy ( TextClassifier ()) message = \"I am interested in space\" print ( text_classifier ( text = message )) message = \"I enjoy ice skating\" print ( text_classifier ( text = message )) Prediction(\n    label='interest'\n)\nPrediction(\n    label='interest'\n)  Review Traces Open the MLflow UI and select the \"DSPy Quickstart\" experiment. Go to the \"Traces\" tab to view the generated traces. Now, you can observe how DSPy translates your query and interacts with the LLM. This feature is extremely valuable for debugging, iteratively refining components within your system, and monitoring models in production. While the module in this tutorial is relatively simple, the tracing feature becomes even more powerful as your model grows in complexity.  Compilation ",
        "id": "a5cdb0c7564e13dcaf37894ce72a7772"
    },
    {
        "text": " Training To train, we will leverage BootstrapFewShotWithRandomSearch , an optimizer that will take bootstrap samples from our training set and leverage a random search strategy to optimize our predictive accuracy. Note that in the below example, we leverage a simple metric definition of exact match, as defined in validate_classification , but dspy.Metrics can contain complex and LM-based logic to properly evaluate our accuracy. [17]: from dspy.teleprompt import BootstrapFewShotWithRandomSearch def validate_classification ( example , prediction , trace = None ) -> bool : return example . label == prediction . label optimizer = BootstrapFewShotWithRandomSearch ( metric = validate_classification , num_candidate_programs = 5 , max_bootstrapped_demos = 2 , num_threads = 1 , ) compiled_pe = optimizer . compile ( copy ( TextClassifier ()), trainset = train_dataset ) Going to sample between 1 and 2 traces per predictor.\nWill attempt to bootstrap 5 candidate sets.\nAverage Metric: 19 / 24  (79.2): 100%|\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088| 24/24 [00:19<00:00,  1.26it/s]\nNew best score: 79.17 for seed -3\nScores so far: [79.17]\nBest score so far: 79.17\nAverage Metric: 22 / 24  (91.7): 100%|\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088| 24/24 [00:20<00:00,  1.17it/s]\nNew best score: 91.67 for seed -2\nScores so far: [79.17, 91.67]\nBest score so far: 91.67 17%|\u00e2\u0096\u0088\u00e2\u0096\u008b        | 4/24 [00:02<00:13,  1.50it/s] Bootstrapped 2 full traces after 5 examples in round 0.\nAverage Metric: 21 / 24  (87.5): 100%|\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088| 24/24 [00:19<00:00,  1.21it/s]\nScores so far: [79.17, 91.67, 87.5]\nBest score so far: 91.67 12%|\u00e2\u0096\u0088\u00e2\u0096\u008e        | 3/24 [00:02<00:18,  1.13it/s] Bootstrapped 2 full traces after 4 examples in round 0.\nAverage Metric: 22 / 24  (91.7): 100%|\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088| 24/24 [00:29<00:00,  1.23s/it]\nScores so far: [79.17, 91.67, 87.5, 91.67]\nBest score so far: 91.67 4%|\u00e2\u0096\u008d         | 1/24 [00:00<00:18,  1.27it/s] Bootstrapped 1 full traces after 2 examples in round 0.\nAverage Metric: 22 / 24  (91.7): 100%|\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088| 24/24 [00:20<00:00,  1.18it/s]\nScores so far: [79.17, 91.67, 87.5, 91.67, 91.67]\nBest score so far: 91.67 8%|\u00e2\u0096\u008a         | 2/24 [00:01<00:20,  1.10it/s] Bootstrapped 1 full traces after 3 examples in round 0.\nAverage Metric: 22 / 24  (91.7): 100%|\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088| 24/24 [00:22<00:00,  1.06it/s]\nScores so far: [79.17, 91.67, 87.5, 91.67, 91.67, 91.67]\nBest score so far: 91.67 4%|\u00e2\u0096\u008d         | 1/24 [00:01<00:30,  1.31s/it] Bootstrapped 1 full traces after 2 examples in round 0.\nAverage Metric: 23 / 24  (95.8): 100%|\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088| 24/24 [00:25<00:00,  1.04s/it]\nNew best score: 95.83 for seed 3\nScores so far: [79.17, 91.67, 87.5, 91.67, 91.67, 91.67, 95.83]\nBest score so far: 95.83 4%|\u00e2\u0096\u008d         | 1/24 [00:00<00:20,  1.12it/s] Bootstrapped 1 full traces after 2 examples in round 0.\nAverage Metric: 22 / 24  (91.7): 100%|\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088| 24/24 [00:24<00:00,  1.03s/it]\nScores so far: [79.17, 91.67, 87.5, 91.67, 91.67, 91.67, 95.83, 91.67]\nBest score so far: 95.83\n8 candidate programs found. ",
        "id": "8a31ac9932bb484050b596cd560ac543"
    },
    {
        "text": " Compare Pre/Post Compiled Accuracy Finally, let\u00e2\u0080\u0099s explore how well our trained model can predict on unseen test data. [18]: def check_accuracy ( classifier , test_data : pd . DataFrame = test_dataset ) -> float : residuals = [] predictions = [] for example in test_data : prediction = classifier ( text = example [ \"text\" ]) residuals . append ( int ( validate_classification ( example , prediction ))) predictions . append ( prediction ) return residuals , predictions uncompiled_residuals , uncompiled_predictions = check_accuracy ( copy ( TextClassifier ())) print ( f \"Uncompiled accuracy: { np . mean ( uncompiled_residuals ) } \" ) compiled_residuals , compiled_predictions = check_accuracy ( compiled_pe ) print ( f \"Compiled accuracy: { np . mean ( compiled_residuals ) } \" ) Uncompiled accuracy: 0.625\nCompiled accuracy: 0.875 As shown above, our compiled accuracy is non-zero - our base LLM inferred meaning of the classification labels simply via our initial prompt. However, with DSPy training, the prompts, demonstrations, and input/output signatures have been updated to give our model to 88% accuracy on unseen data. That\u00e2\u0080\u0099s a gain of 25 percentage points! Let\u00e2\u0080\u0099s take a look at each prediction in our test set. [19]: for uncompiled_residual , uncompiled_prediction in zip ( uncompiled_residuals , uncompiled_predictions ): is_correct = \"Correct\" if bool ( uncompiled_residual ) else \"Incorrect\" prediction = uncompiled_prediction . label print ( f \" { is_correct } prediction: { ' ' * ( 12 - len ( is_correct )) }{ prediction } \" ) Incorrect prediction:    money-fx\nCorrect prediction:      crude\nCorrect prediction:      money-fx\nCorrect prediction:      earn\nIncorrect prediction:    interest\nCorrect prediction:      grain\nCorrect prediction:      trade\nIncorrect prediction:    trade [20]: for compiled_residual , compiled_prediction in zip ( compiled_residuals , compiled_predictions ): is_correct = \"Correct\" if bool ( compiled_residual ) else \"Incorrect\" prediction = compiled_prediction . label print ( f \" { is_correct } prediction: { ' ' * ( 12 - len ( is_correct )) }{ prediction } \" ) Correct prediction:      interest\nCorrect prediction:      crude\nCorrect prediction:      money-fx\nCorrect prediction:      earn\nCorrect prediction:      acq\nCorrect prediction:      grain\nCorrect prediction:      trade\nIncorrect prediction:    crude ",
        "id": "f1f0360dadcd4cf9517220e0736c0ca4"
    },
    {
        "text": " Log and Load the Model with MLflow Now that we have a compiled model with higher classification accuracy, let\u00e2\u0080\u0099s leverage MLflow to log this model and load it for inference. [21]: import mlflow with mlflow . start_run (): model_info = mlflow . dspy . log_model ( compiled_pe , \"model\" , input_example = \"what is 2 + 2?\" , ) Open the MLflow UI again and check the complied model is recorded to a new MLflow Run. Now you can load the model back for inference using mlflow.dspy.load_model or mlflow.pyfunc.load_model . \u00f0\u009f\u0092\u00a1 MLflow will remember the environment configuration stored in dspy.settings , such as the language model (LM) used during the experiment. This ensures excellent reproducibility for your experiment. [22]: # Define input text print ( \" \\n ==============Input Text============\" ) text = test_dataset [ 0 ][ \"text\" ] print ( f \"Text: { text } \" ) # Inference with original DSPy object print ( \" \\n --------------Original DSPy Prediction------------\" ) print ( compiled_pe ( text = text ) . label ) # Inference with loaded DSPy object print ( \" \\n --------------Loaded DSPy Prediction------------\" ) loaded_model_dspy = mlflow . dspy . load_model ( model_info . model_uri ) print ( loaded_model_dspy ( text = text ) . label ) # Inference with MLflow PyFunc API loaded_model_pyfunc = mlflow . pyfunc . load_model ( model_info . model_uri ) print ( \" \\n --------------PyFunc Prediction------------\" ) print ( loaded_model_pyfunc . predict ( text )[ \"label\" ]) ==============Input Text============\nText: top discount rate at u k bill tender rises to pct\n\n--------------Original DSPy Prediction------------\ninterest\n\n--------------Loaded DSPy Prediction------------\ninterest\n\n--------------PyFunc Prediction------------\ninterest  Next Steps This example demonstrates how DSPy works. Below are some potential extensions for improving this project, both with DSPy and MLflow.  DSPy Use real-world data for the classifier. Experiment with different optimizers. For more in-depth examples, check out the tutorials and documentation .  MLflow Deploy the model using MLflow serving. Use MLflow to experiment with various optimization strategies. Happy coding! Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "8a022eff5e40a478b2260706b5183865"
    },
    {
        "text": "LLM RAG Evaluation with MLflow Example Notebook 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow Retrieval Augmented Generation (RAG) Benefits of RAG Understanding the Power of RAG Explore RAG Tutorials Deploying Advanced LLMs with Custom PyFuncs in MLflow LLM Evaluation Examples Tutorial: Getting Started with ChatModel Tutorial: Custom GenAI Models using ChatModel Build a tool-calling model with mlflow.pyfunc.ChatModel MLflow Trace UI in Jupyter Notebook Demo MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs Retrieval Augmented Generation (RAG) RAG Tutorials LLM RAG Evaluation with MLflow Example Notebook  LLM RAG Evaluation with MLflow Example Notebook Download this Notebook Welcome to this comprehensive tutorial on evaluating Retrieval-Augmented Generation (RAG) systems using MLflow. This tutorial is designed to guide you through the intricacies of assessing various RAG systems, focusing on how they can be effectively integrated and evaluated in a real-world context. Whether you are a data scientist, a machine learning engineer, or simply an enthusiast in the field of AI, this tutorial offers valuable insights and practical knowledge. ",
        "id": "9656437e1cd547089f07b475981d91b8"
    },
    {
        "text": " What You Will Learn: Setting Up the Environment : Learn how to set up your development environment with all the necessary tools and libraries, including MLflow, OpenAI, ChromaDB, LangChain, and more. This section ensures you have everything you need to start working with RAG systems. Understanding RAG Systems : Delve into the concept of Retrieval-Augmented Generation and its significance in modern AI applications. Understand how RAG systems leverage both retrieval and generation capabilities to provide accurate and contextually relevant responses. Securely Managing API Keys with Databricks Secrets : Explore the best practices for securely managing API keys using Databricks Secrets. This part is crucial for ensuring the security and integrity of your application. Deploying and Testing RAG Systems with MLflow : Learn how to create, deploy, and test RAG systems using MLflow. This includes setting up endpoints, deploying models, and querying them to see their responses in action. Evaluating Performance with MLflow : Dive into evaluating the RAG systems using MLflow\u00e2\u0080\u0099s evaluation tools. Understand how to use metrics like relevance and latency to assess the performance of your RAG system. Experimenting with Chunking Strategies : Experiment with different text chunking strategies to optimize the performance of RAG systems. Understand how the size of text chunks affects retrieval accuracy and system responsiveness. Creating and Using Evaluation Datasets : Learn how to create and utilize evaluation datasets (Golden Datasets) to effectively assess the performance of your RAG system. Combining Retrieval and Generation for Question Answering : Gain insights into how retrieval and generation components work together in a RAG system to answer questions based on a given context or documentation. By the end of this tutorial, you will have a thorough understanding of how to evaluate and optimize RAG systems using MLflow. You will be equipped with the knowledge to deploy, test, and refine RAG systems, making them suitable for various practical applications. This tutorial is your stepping stone into the world of advanced AI model evaluation and deployment. [ ]: % pip install mlflow>=2.8.1 % pip install openai % pip install chromadb==0.4.15 % pip install langchain==0.0.348 % pip install tiktoken % pip install 'mlflow[genai]' % pip install databricks-sdk --upgrade [ ]: dbutils . library . restartPython () # noqa: F821 [ ]: import ast import os import chromadb import pandas as pd from langchain.chains import RetrievalQA from langchain.document_loaders import WebBaseLoader from langchain.embeddings.databricks import DatabricksEmbeddings from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings from langchain.llms import Databricks from langchain.text_splitter import CharacterTextSplitter from langchain.vectorstores import Chroma import mlflow import mlflow.deployments from mlflow.deployments import set_deployments_target from mlflow.metrics.genai.metric_definitions import relevance [ ]: # check mlflow version mlflow . __version__ '2.9.1' [ ]: # check chroma version chromadb . __version__ '0.4.18' ",
        "id": "33eb90336d05cf184f0356ce6c176b50"
    },
    {
        "text": " Set-up Databricks Workspace Secrets In order to use the secrets that are defined within this notebook, ensure that they are set via following the guide to Databricks Secrets here . It is highly recommended to utilize the Databricks CLI to set secrets within your workspace for a secure experience. In order to safely store and access your API KEY for Azure OpenAI, ensure that you are setting the following when registering your secret: KEY_NAME : The name that you will be setting for your Azure OpenAI Key SCOPE_NAME : The referenced scope that your secret will reside in, within Databricks Secrets OPENAI_API_KEY : Your Azure OpenAI Key As an example, you would set these keys through a terminal as follows: databricks secrets put-secret \"<SCOPE_NAME>\" \"<KEY_NAME>\" --string-value \"<OPENAI_API_KEY>\" [ ]: # Set your Scope and Key Names that you used when registering your API KEY from the Databricks CLI # Do not put your OpenAI API Key in the notebook! SCOPE_NAME = ... KEY_NAME = ... [ ]: os . environ [ \"OPENAI_API_KEY\" ] = dbutils . secrets . get ( scope = SCOPE_NAME , key = KEY_NAME ) # noqa: F821 os . environ [ \"OPENAI_API_TYPE\" ] = \"azure\" os . environ [ \"OPENAI_API_VERSION\" ] = \"2023-05-15\" # Ensure that you set the name of your OPEN_API_BASE value to the name of your OpenAI instance on Azure os . environ [ \"OPENAI_API_BASE\" ] = \"https://<NAME_OF_YOUR_INSTANCE>.openai.azure.com/\" # replace this! os . environ [ \"OPENAI_DEPLOYMENT_NAME\" ] = \"gpt-4o-mini\" os . environ [ \"OPENAI_ENGINE\" ] = \"gpt-4o-mini\"  Create and Test Endpoint on MLflow for OpenAI [ ]: client = mlflow . deployments . get_deploy_client ( \"databricks\" ) endpoint_name = \"<your-endpoint-name>\" # replace this! client . create_endpoint ( name = endpoint_name , config = { \"served_entities\" : [ { \"name\" : \"test-gpt\" , # Provide a unique identifying name for your deployments endpoint \"external_model\" : { \"name\" : \"gpt-4o-mini\" , \"provider\" : \"openai\" , \"task\" : \"llm/v1/completions\" , \"openai_config\" : { \"openai_api_type\" : \"azure\" , # replace with your own secrets, for reference see https://docs.databricks.com/en/security/secrets/secrets.html \"openai_api_key\" : \"{{secrets/scope/openai_api_key}}\" , \"openai_api_base\" : \"{{secrets/scope/openai_api_base}}\" , \"openai_deployment_name\" : \"gpt-4o-mini\" , \"openai_api_version\" : \"2023-05-15\" , }, }, } ], }, ) [ ]: print ( client . predict ( endpoint = endpoint_name , inputs = { \"prompt\" : \"How is Pi calculated? Be very concise.\" , \"max_tokens\" : 100 , }, ) )  Create RAG POC with LangChain and log with MLflow Use Langchain and Chroma to create a RAG system that answers questions based on the MLflow documentation. [ ]: loader = WebBaseLoader ( [ \"https://mlflow.org/docs/latest/index.html\" , \"https://mlflow.org/docs/latest/tracking/autolog.html\" , \"https://mlflow.org/docs/latest/getting-started/tracking-server-overview/index.html\" , \"https://mlflow.org/docs/latest/python_api/mlflow.deployments.html\" , ] ) documents = loader . load () CHUNK_SIZE = 1000 text_splitter = CharacterTextSplitter ( chunk_size = CHUNK_SIZE , chunk_overlap = 0 ) texts = text_splitter . split_documents ( documents ) llm = Databricks ( endpoint_name = \"<your-endpoint-name>\" , # replace this! extra_params = { \"temperature\" : 0.1 , \"top_p\" : 0.1 , \"max_tokens\" : 500 , }, # parameters used in AI Playground ) # create the embedding function using Databricks Foundation Model APIs embedding_function = DatabricksEmbeddings ( endpoint = \"databricks-bge-large-en\" ) docsearch = Chroma . from_documents ( texts , embedding_function ) qa = RetrievalQA . from_chain_type ( llm = llm , chain_type = \"stuff\" , retriever = docsearch . as_retriever ( fetch_k = 3 ), return_source_documents = True , )  Evaluate the Vector Database and Retrieval using mlflow.evaluate() ",
        "id": "03223fde8ad13869ec2186a85f2f9a8d"
    },
    {
        "text": " Create an eval dataset (Golden Dataset) We can leveraging the power of an LLM to generate synthetic data for testing , offering a creative and efficient alternative. To our readers and customers, we emphasize the importance of crafting a dataset that mirrors the expected inputs and outputs of your RAG application. It\u00e2\u0080\u0099s a journey worth taking for the incredible insights you\u00e2\u0080\u0099ll gain! [ ]: EVALUATION_DATASET_PATH = \"https://raw.githubusercontent.com/mlflow/mlflow/master/examples/llms/RAG/static_evaluation_dataset.csv\" synthetic_eval_data = pd . read_csv ( EVALUATION_DATASET_PATH ) # Load the static evaluation dataset from disk and deserialize the source and retrieved doc ids synthetic_eval_data [ \"source\" ] = synthetic_eval_data [ \"source\" ] . apply ( ast . literal_eval ) synthetic_eval_data [ \"retrieved_doc_ids\" ] = synthetic_eval_data [ \"retrieved_doc_ids\" ] . apply ( ast . literal_eval ) [ ]: display ( synthetic_eval_data )  Evaluating the Embedding Model with MLflow In this part of the tutorial, we focus on evaluating the embedding model\u00e2\u0080\u0099s performance in the context of a retrieval system. The process involves a series of steps to assess how effectively the model can retrieve relevant documents based on given questions.  Creating Evaluation Data We start by defining a set of questions and their corresponding source URLs. This eval_data DataFrame acts as our evaluation dataset, allowing us to test the model\u00e2\u0080\u0099s ability to link questions to the correct source documents.  The evaluate_embedding Function The evaluate_embedding function is designed to assess the performance of a given embedding function. Chunking Strategy : The function begins by splitting a list of documents into chunks using a CharacterTextSplitter . The size of these chunks is crucial, as it can influence the retrieval accuracy. Retriever Initialization : We then use Chroma.from_documents to create a retriever with the specified embedding function. This retriever is responsible for finding documents relevant to a given query. Retrieval Process : The function defines a retriever_model_function that applies the retriever to each question in the evaluation dataset. It retrieves document IDs that the model finds most relevant for each question.  MLflow Evaluation With mlflow.start_run() , we initiate an evaluation run. mlflow.evaluate is then called to evaluate our retriever model function against the evaluation dataset. We use the default evaluator with specified targets to assess the model\u00e2\u0080\u0099s performance. The results of this evaluation, stored in eval_results_of_retriever_df_bge , are displayed, providing insights into the effectiveness of the embedding model in document retrieval. ",
        "id": "45647aacdcd80cce17d10bb2bb8f97ea"
    },
    {
        "text": " Further Evaluation with Metrics Additionally, we perform a more detailed evaluation using various metrics like precision, recall, and NDCG at different \u00e2\u0080\u0098k\u00e2\u0080\u0099 values. These metrics offer a deeper understanding of the model\u00e2\u0080\u0099s retrieval accuracy and ranking effectiveness. This evaluation step is integral to understanding the strengths and weaknesses of our embedding model in a real-world RAG system. By analyzing these results, we can make informed decisions about model adjustments or optimizations to improve overall system performance. [ ]: eval_data = pd . DataFrame ( { \"question\" : [ \"What is MLflow?\" , \"What is Databricks?\" , \"How to serve a model on Databricks?\" , \"How to enable MLflow Autologging for my workspace by default?\" , ], \"source\" : [ [ \"https://mlflow.org/docs/latest/index.html\" ], [ \"https://mlflow.org/docs/latest/getting-started/tracking-server-overview/index.html\" ], [ \"https://mlflow.org/docs/latest/python_api/mlflow.deployments.html\" ], [ \"https://mlflow.org/docs/latest/tracking/autolog.html\" ], ], } ) [ ]: def evaluate_embedding ( embedding_function ): CHUNK_SIZE = 1000 list_of_documents = loader . load () text_splitter = CharacterTextSplitter ( chunk_size = CHUNK_SIZE , chunk_overlap = 0 ) docs = text_splitter . split_documents ( list_of_documents ) retriever = Chroma . from_documents ( docs , embedding_function ) . as_retriever () def retrieve_doc_ids ( question : str ) -> list [ str ]: docs = retriever . get_relevant_documents ( question ) return [ doc . metadata [ \"source\" ] for doc in docs ] def retriever_model_function ( question_df : pd . DataFrame ) -> pd . Series : return question_df [ \"question\" ] . apply ( retrieve_doc_ids ) with mlflow . start_run (): return mlflow . evaluate ( model = retriever_model_function , data = eval_data , model_type = \"retriever\" , targets = \"source\" , evaluators = \"default\" , ) result1 = evaluate_embedding ( DatabricksEmbeddings ( endpoint = \"databricks-bge-large-en\" )) # To validate the results of a different model, comment out the above line and uncomment the below line: # result2 = evaluate_embedding(SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")) eval_results_of_retriever_df_bge = result1 . tables [ \"eval_results_table\" ] # To validate the results of a different model, comment out the above line and uncomment the below line: # eval_results_of_retriever_df_MiniLM = result2.tables[\"eval_results_table\"] display ( eval_results_of_retriever_df_bge )  Evaluate different Top K strategy with MLflow [ ]: with mlflow . start_run () as run : evaluate_results = mlflow . evaluate ( data = eval_results_of_retriever_df_bge , targets = \"source\" , predictions = \"outputs\" , evaluators = \"default\" , extra_metrics = [ mlflow . metrics . precision_at_k ( 1 ), mlflow . metrics . precision_at_k ( 2 ), mlflow . metrics . precision_at_k ( 3 ), mlflow . metrics . recall_at_k ( 1 ), mlflow . metrics . recall_at_k ( 2 ), mlflow . metrics . recall_at_k ( 3 ), mlflow . metrics . ndcg_at_k ( 1 ), mlflow . metrics . ndcg_at_k ( 2 ), mlflow . metrics . ndcg_at_k ( 3 ), ], ) display ( evaluate_results . tables [ \"eval_results_table\" ])  Evaluate the Chunking Strategy with MLflow In the realm of RAG systems, the strategy for dividing text into chunks plays a pivotal role in both retrieval effectiveness and the overall system performance. Let\u00e2\u0080\u0099s delve into why and how we evaluate different chunking strategies:  Importance of Chunking: Influences Retrieval Accuracy : The way text is chunked can significantly affect the retrieval component of RAG systems. Smaller chunks may lead to more focused and relevant document retrieval, while larger chunks might capture broader context. Impacts System\u00e2\u0080\u0099s Responsiveness : The size of text chunks also influences the speed of document retrieval and processing. Smaller chunks can be processed more quickly but may require the system to evaluate more chunks overall. ",
        "id": "59b5084a3d7d3bdf6afc6e86d228474b"
    },
    {
        "text": " Evaluating Different Chunk Sizes: Purpose : By evaluating different chunk sizes, we aim to find an optimal balance between retrieval accuracy and processing efficiency. This involves experimenting with various chunk sizes to see how they impact the system\u00e2\u0080\u0099s performance. Method : We create text chunks of different sizes (e.g., 1000 characters, 2000 characters) and then evaluate how each chunking strategy affects the RAG system. Key aspects to observe include the relevance of retrieved documents and the system\u00e2\u0080\u0099s latency. In this example below, we\u00e2\u0080\u0099re using the default evaluation suite to provide a comprehensive adjudication of the quality of the responses to retrieved document contents to determine what the impact to the quality of the returned references are, allowing us to explore and tune the chunk size in order to arrive at a configuration that best handles our suite of test questions. Note that the embedding model has changed in this next code block. Above, we were using DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\") , while now we\u00e2\u0080\u0099re evaluating the performance of SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\") [ ]: def evaluate_chunk_size ( chunk_size ): list_of_documents = loader . load () text_splitter = CharacterTextSplitter ( chunk_size = chunk_size , chunk_overlap = 0 ) docs = text_splitter . split_documents ( list_of_documents ) embedding_function = SentenceTransformerEmbeddings ( model_name = \"all-MiniLM-L6-v2\" ) retriever = Chroma . from_documents ( docs , embedding_function ) . as_retriever () def retrieve_doc_ids ( question : str ) -> list [ str ]: docs = retriever . get_relevant_documents ( question ) return [ doc . metadata [ \"source\" ] for doc in docs ] def retriever_model_function ( question_df : pd . DataFrame ) -> pd . Series : return question_df [ \"question\" ] . apply ( retrieve_doc_ids ) with mlflow . start_run (): return mlflow . evaluate ( model = retriever_model_function , data = eval_data , model_type = \"retriever\" , targets = \"source\" , evaluators = \"default\" , ) result1 = evaluate_chunk_size ( 1000 ) result2 = evaluate_chunk_size ( 2000 ) display ( result1 . tables [ \"eval_results_table\" ]) display ( result2 . tables [ \"eval_results_table\" ])  Evaluate the RAG system using mlflow.evaluate() In this section, we\u00e2\u0080\u0099ll delve into evaluating the Retrieval-Augmented Generation (RAG) systems using mlflow.evaluate() . This evaluation is crucial for assessing the effectiveness and efficiency of RAG systems in question-answering contexts. We focus on two key metrics: relevance_metric and latency .  Relevance Metric: What It Measures : The relevance_metric quantifies how relevant the RAG system\u00e2\u0080\u0099s answers are to the input questions. This metric is critical for understanding the accuracy and contextual appropriateness of the system\u00e2\u0080\u0099s responses. Why It\u00e2\u0080\u0099s Important : In question-answering systems, relevance is paramount. The ability of a RAG system to provide accurate and contextually correct answers determines its utility and effectiveness in real-world applications, such as information retrieval and customer support. Tutorial Context : Within our tutorial, we utilize the relevance_metric to evaluate the quality of answers provided by the RAG system. It serves as a quantitative measure of the system\u00e2\u0080\u0099s content accuracy, reflecting its capability to generate useful and precise responses. ",
        "id": "d4993d7f84c0c52b9e459d95f781ae08"
    },
    {
        "text": " Latency: What It Measures : The latency metric captures the response time of the RAG system. It measures the duration taken by the system to generate an answer after receiving a query. Why It\u00e2\u0080\u0099s Important : Response time is a critical factor in user experience. In interactive systems, lower latency leads to a more efficient and satisfying user experience. High latency, conversely, can be detrimental to user satisfaction. Tutorial Context : In this tutorial, we assess the system\u00e2\u0080\u0099s efficiency in terms of response time through the latency metric. This evaluation is vital for understanding the system\u00e2\u0080\u0099s performance in a production environment, where timely responses are as important as their accuracy. To start with evaluating, we\u00e2\u0080\u0099ll create a simple function that runs each input through the RAG chain [ ]: def model ( input_df ): return input_df [ \"questions\" ] . map ( qa ) . tolist ()  Create an evaluation dataset (Golden Dataset) [ ]: eval_df = pd . DataFrame ( { \"questions\" : [ \"What is MLflow?\" , \"What is Databricks?\" , \"How to serve a model on Databricks?\" , \"How to enable MLflow Autologging for my workspace by default?\" , ], } ) display ( eval_df )  Evaluate using LLM as a Judge and Basic Metrics In this concluding section of the tutorial, we perform a final evaluation of our RAG system using MLflow\u00e2\u0080\u0099s powerful evaluation tools. This evaluation is crucial for assessing the performance and efficiency of the question-answering model.  Key Steps in the Evaluation: Setting the Deployment Target : The deployment target is set to Databricks, enabling us to retrieve all endpoints in the Databricks Workspace. This is essential for accessing our deployed models. Relevance Metric Setup : We initialize the relevance metric using a model hosted on Databricks. This metric assesses how relevant the answers generated by our RAG system are in response to the input questions. Running the Evaluation : An MLflow run is initiated, and mlflow.evaluate() is called to evaluate our RAG model against the prepared evaluation dataset. The model is evaluated as a \u00e2\u0080\u009cquestion-answering\u00e2\u0080\u009d system using default evaluators. Additional metrics, including the relevance_metric and latency , are specified. These metrics provide insights into the relevance of the answers and the response time of the model. The evaluator_config maps the input questions and context, ensuring the correct evaluation of the RAG system. Results and Metrics Display : The results of the evaluation, including key metrics, are displayed in a table format, providing a clear and structured view of the model\u00e2\u0080\u0099s performance based on relevance and latency. This comprehensive evaluation step is vital for understanding the effectiveness and efficiency of our RAG system. By assessing both the relevance of the answers and the latency of the responses, we gain a holistic view of the model\u00e2\u0080\u0099s performance, guiding any further optimization or deployment decisions. [ ]: set_deployments_target ( \"databricks\" ) # To retrieve all endpoint in your Databricks Workspace relevance_metric = relevance ( model = \"endpoints:/databricks-llama-2-70b-chat\" ) # You can also use any model you have hosted on Databricks, models from the Marketplace or models in the Foundation model API with mlflow . start_run (): results = mlflow . evaluate ( model , eval_df , model_type = \"question-answering\" , evaluators = \"default\" , predictions = \"result\" , extra_metrics = [ relevance_metric , mlflow . metrics . latency ()], evaluator_config = { \"col_mapping\" : { \"inputs\" : \"questions\" , \"context\" : \"source_documents\" , } }, ) print ( results . metrics ) display ( results . tables [ \"eval_results_table\" ]) Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "b5fe3539c93971e702129eef0b2be5c1"
    },
    {
        "text": "Question Generation For Retrieval Evaluation 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow Retrieval Augmented Generation (RAG) Benefits of RAG Understanding the Power of RAG Explore RAG Tutorials Deploying Advanced LLMs with Custom PyFuncs in MLflow LLM Evaluation Examples Tutorial: Getting Started with ChatModel Tutorial: Custom GenAI Models using ChatModel Build a tool-calling model with mlflow.pyfunc.ChatModel MLflow Trace UI in Jupyter Notebook Demo MLflow Tracing MLflow AI Gateway for LLMs LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs Retrieval Augmented Generation (RAG) RAG Tutorials Question Generation For Retrieval Evaluation  Question Generation For Retrieval Evaluation Download this Notebook MLflow provides an advanced framework for constructing Retrieval-Augmented Generation (RAG) models. RAG is a cutting edge approach that combines the strengths of retrieval models (a model that chooses and ranks relevant chunks of a document based on the user\u00e2\u0080\u0099s question) and generative models. It effectively merges the capabilities of searching and generating text to provide responses that are contextually relevant and coherent, allowing the generated text to make reference to existing documents.\nRAG leverges the retriever to find context documents, and this novel approach has revolutionized various NLP tasks. Naturally, we want to be able to evaluate this retriever system for the RAG model to compare and judge its performance. To evaluate a retriever system, we would first need a test set of questions on the documents. These questions need to be diverse, relevant, and coherent. Manually generating questions may be challenging because it first requires you to understand the documents, and spend lots of time coming up with questions for them. We want to make this process simpler by utilizing an LLM to generate questions for this test set. This tutorial will walk through how to generate the questions and how to analyze the diversity and relevance of the questions. ",
        "id": "722c017d0603216e27454e85eb70c651"
    },
    {
        "text": " Step 1: Install and Load Packages We also define some utility functions to cache the LLM responses to save cost. You can skip reading the implementation details in the next cell. [ ]: % pip install beautifulsoup4 langchain openai pandas seaborn scikit-learn [1]: import json import os # For cost-saving, create a cache for the LLM responses import threading # For data analysis and visualization import matplotlib.pyplot as plt import numpy as np import openai import pandas as pd # For scraping import requests import seaborn as sns from bs4 import BeautifulSoup from langchain.embeddings import OpenAIEmbeddings from langchain.text_splitter import CharacterTextSplitter from sklearn.decomposition import PCA from sklearn.manifold import TSNE class Cache : def __init__ ( self , persist_path , cache_loading_fn ): \"\"\" The cache_loading_fn should be a function that takes arbitrary serializable arguments and returns a serilaizable value. value = cache_loading_fn(**kwargs) For example, for openai.chat.completions.create(...), the cache_loading_fn should be: def cache_loading_fn(**kwargs): result = openai.chat.completions.create(**kwargs) return result.to_dict_recursive() \"\"\" self . _cache = self . _get_or_create_cache_dict ( persist_path ) self . _persist_path = persist_path self . _cache_loading_fn = cache_loading_fn self . _cache_lock = threading . Lock () @classmethod def _get_or_create_cache_dict ( cls , persist_path ): if os . path . exists ( persist_path ): # File exists, load it as a JSON string into a dict with open ( persist_path ) as f : cache = json . load ( f ) else : # File does not exist, create an empty dict cache = {} return cache def _save_to_file ( self ): with open ( self . _persist_path , \"w\" ) as file : json . dump ( self . _cache , file ) def _update_cache ( self , key , value ): with self . _cache_lock : self . _cache [ key ] = value self . _save_to_file () def get_from_cache_or_load_cache ( self , ** kwargs ): key = json . dumps ( kwargs ) with self . _cache_lock : value = self . _cache . get ( key , None ) if value is None : value = self . _cache_loading_fn ( ** kwargs ) self . _update_cache ( key , value ) else : print ( \"Loaded from cache\" ) return value def chat_completion_create_fn ( ** kwargs ): result = openai . chat . completions . create ( ** kwargs ) return result . to_dict_recursive () def cached_openai_ChatCompletion_create ( ** kwargs ): cache = kwargs . pop ( \"cache\" ) return cache . get_from_cache_or_load_cache ( ** kwargs ) def embeddings_embed_documents_fn ( ** kwargs ): chunk = kwargs . get ( \"chunk\" ) return embeddings . embed_documents ([ chunk ]) def cached_langchain_openai_embeddings ( ** kwargs ): cache = kwargs . pop ( \"cache\" ) return cache . get_from_cache_or_load_cache ( ** kwargs )  Step 2: Set OpenAI Key The question generation system can be done using any LLM. We chose to use OpenAI here, so we will need their API key. [ ]: openai . api_key = \"<redacted>\" os . environ [ \"OPENAI_API_KEY\" ] = openai . api_key [2]: # Other configurations # Choose a seed for reproducible results SEED = 2023 # For cost-saving purposes, choose a path to persist the responses for LLM calls CACHE_PATH = \"_cache.json\" EMBEDDINGS_CACHE_PATH = \"_embeddings_cache.json\" # To avoid re-running the scraping process, choose a path to save the scrapped docs SCRAPPED_DATA_PATH = \"mlflow_docs_scraped.csv\" # Choose a path to save the generated dataset OUTPUT_DF_PATH = \"question_answer_source.csv\" [3]: cache = Cache ( CACHE_PATH , chat_completion_create_fn ) embeddings_cache = Cache ( EMBEDDINGS_CACHE_PATH , embeddings_embed_documents_fn )  Step 3: Decide on Chunk Size [4]: CHUNK_SIZE = 1500  Step 4: Prepare Document Data ",
        "id": "5b153ae8f0f2e85e1ffe2ff5f75b9d97"
    },
    {
        "text": " Scrape the documents from the MLflow website [ ]: page = requests . get ( \"https://mlflow.org/docs/latest/index.html\" ) soup = BeautifulSoup ( page . content , \"html.parser\" ) mainLocation = \"https://mlflow.org/docs/latest/\" header = { \"User-Agent\" : \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11\" , \"Accept-Language\" : \"en-US,en;q=0.8\" , \"Connection\" : \"keep-alive\" , } data = [] for a_link in soup . find_all ( \"a\" ): document_url = mainLocation + a_link [ \"href\" ] page = requests . get ( document_url , headers = header ) soup = BeautifulSoup ( page . content , \"html.parser\" ) file_to_store = a_link . get ( \"href\" ) if soup . find ( \"div\" , { \"class\" : \"rst-content\" }): data . append ( [ file_to_store , soup . find ( \"div\" , { \"class\" : \"rst-content\" }) . text . replace ( \" \\n \" , \" \" ), ] ) df = pd . DataFrame ( data , columns = [ \"source\" , \"text\" ]) [5]: df . to_csv ( SCRAPPED_DATA_PATH , index = False ) df = pd . read_csv ( SCRAPPED_DATA_PATH ) ",
        "id": "b831acd99de6c4e5e67cef9f10513e47"
    },
    {
        "text": " Select a subset of the documents and split them into chunks [6]: # For demonstration purposes, let's pick 5 popular MLflow documantation pages from the dataset mask = df [ \"source\" ] . isin ( { \"tracking.html\" , \"models.html\" , \"model-registry.html\" , \"search-runs.html\" , \"projects.html\" , } ) sub_df = df [ mask ] # Split documents into chunks text_splitter = CharacterTextSplitter ( chunk_size = CHUNK_SIZE , separator = \" \" ) def get_chunks ( input_row ): new_rows = [] chunks = text_splitter . split_text ( input_row [ \"text\" ]) for i , chunk in enumerate ( chunks ): new_rows . append ({ \"chunk\" : chunk , \"source\" : input_row [ \"source\" ], \"chunk_index\" : i }) return new_rows expanded_df = pd . DataFrame ( columns = [ \"chunk\" , \"source\" , \"chunk_index\" ]) for index , row in sub_df . iterrows (): new_rows = get_chunks ( row ) expanded_df = pd . concat ([ expanded_df , pd . DataFrame ( new_rows )], ignore_index = True ) expanded_df . head ( 3 ) [6]: chunk source chunk_index 0 Documentation MLflow Tracking MLflow Tracking ... tracking.html 0 1 Tags Concepts MLflow Tracking is organized aro... tracking.html 1 2 runs into experiments, which group together ru... tracking.html 2 [7]: # For cost-saving purposes, let's pick the first 3 chunks from each doc # To generate questions with more chunks, change the start index and end index in iloc[] start , end = 0 , 3 filtered_df = ( expanded_df . groupby ( \"source\" ) . apply ( lambda x : x . iloc [ start : end ]) . reset_index ( drop = True ) ) filtered_df . head ( 3 ) [7]: chunk source chunk_index 0 Documentation MLflow Model Registry MLflow Mod... model-registry.html 0 1 logged, this model can then be registered with... model-registry.html 1 2 associate with registered models and model ver... model-registry.html 2 [ ]: filtered_df [ \"chunk\" ][ 0 ] 'Documentation MLflow Model Registry MLflow Model Registry The MLflow Model Registry component is a centralized model store, set of APIs, and UI, to collaboratively manage the full lifecycle of an MLflow Model. It provides model lineage (which MLflow experiment and run produced the model), model versioning, model aliasing, model tagging, and annotations. Table of Contents Concepts Model Registry Workflows UI Workflow Register a Model Find Registered Models Deploy and Organize Models API Workflow Adding an MLflow Model to the Model Registry Deploy and Organize Models with Aliases and Tags Fetching an MLflow Model from the Model Registry Serving an MLflow Model from Model Registry Promoting an MLflow Model across environments Adding or Updating an MLflow Model Descriptions Renaming an MLflow Model Listing and Searching MLflow Models Deleting MLflow Models Registering a Model Saved Outside MLflow Registering an Unsupported Machine Learning Model Transitioning an MLflow Model\u00e2\u0080\u0099s Stage Archiving an MLflow Model Concepts The Model Registry introduces a few concepts that describe and facilitate the full lifecycle of an MLflow Model. ModelAn MLflow Model is created from an experiment or run that is logged with one of the model flavor\u00e2\u0080\u0099s mlflow.<model_flavor>.log_model() methods. Once logged, this model can then be registered with the Model Registry. Registered ModelAn MLflow Model can be registered with the Model Registry. A registered model has a unique name, contains versions,' ",
        "id": "dafade7f8cfe7042a4012e83b2659a41"
    },
    {
        "text": " Step 5: Generate questions The prompt below instructs the LLM to generate a question for each given chunk, and also generate an answer to the question to make it easier to do human validation. Also, return the results in a structured format. This example uses OpenAI\u00e2\u0080\u0099s gpt-4o-mini model to generate the questions, you can replace it with the LLM that works best for your use case. [ ]: def get_raw_response ( content ): prompt = f \"\"\"Please generate a question asking for the key information in the given paragraph. Also answer the questions using the information in the given paragraph. Please ask the specific question instead of the general question, like 'What is the key information in the given paragraph?'. Please generate the answer using as much information as possible. If you are unable to answer it, please generate the answer as 'I don't know.' The answer should be informative and should be more than 3 sentences. Paragraph: { content } Please call the submit_function function to submit the generated question and answer. \"\"\" messages = [{ \"role\" : \"user\" , \"content\" : prompt }] submit_function = { \"name\" : \"submit_function\" , \"description\" : \"Call this function to submit the generated question and answer.\" , \"parameters\" : { \"type\" : \"object\" , \"properties\" : { \"question\" : { \"type\" : \"string\" , \"description\" : \"The question asking for the key information in the given paragraph.\" , }, \"answer\" : { \"type\" : \"string\" , \"description\" : \"The answer to the question using the information in the given paragraph.\" , }, }, \"required\" : [ \"question\" , \"answer\" ], }, } return cached_openai_ChatCompletion_create ( messages = messages , model = \"gpt-4o-mini\" , functions = [ submit_function ], function_call = \"auto\" , temperature = 0.0 , seed = SEED , cache = cache , ) def generate_question_answer ( content ): if content is None or len ( content ) == 0 : return \"\" , \"N/A\" response = get_raw_response ( content ) try : func_args = json . loads ( response [ \"choices\" ][ 0 ][ \"message\" ][ \"function_call\" ][ \"arguments\" ]) question = func_args [ \"question\" ] answer = func_args [ \"answer\" ] return question , answer except Exception as e : return str ( e ), \"N/A\" [ ]: queries = [] [ ]: get_raw_response ( filtered_df [ \"chunk\" ][ 0 ]) {'id': 'chatcmpl-8NPsIJQZYDP4aqiWEUlUyLakv3lyR',\n 'object': 'chat.completion',\n 'created': 1700591698,\n 'model': 'gpt-3.5-turbo-0613',\n 'choices': [{'index': 0,\n   'message': {'role': 'assistant',\n    'content': None,\n    'function_call': {'name': 'submit_function',\n     'arguments': '{\\n  \"question\": \"What is the purpose of the MLflow Model Registry?\",\\n  \"answer\": \"The purpose of the MLflow Model Registry is to provide a centralized model store, set of APIs, and UI to collaboratively manage the full lifecycle of an MLflow Model. It allows for model lineage, versioning, aliasing, tagging, and annotations.\"\\n}'}},\n   'finish_reason': 'function_call'}],\n 'usage': {'prompt_tokens': 490, 'completion_tokens': 81, 'total_tokens': 571}} [ ]: # The requests sometimes get ratelimited, you can re-execute this cell without losing the existing results. n = len ( filtered_df ) for i , row in filtered_df . iterrows (): chunk = row [ \"chunk\" ] question , answer = generate_question_answer ( chunk ) print ( f \" { i + 1 } / { n } : { question } \" ) queries . append ( { \"question\" : question , \"answer\" : answer , \"chunk\" : chunk , \"chunk_id\" : row [ \"chunk_index\" ], \"source\" : row [ \"source\" ],",
        "id": "08333ca6d549f110137cc2aea9a66d49"
    },
    {
        "text": "cell without losing the existing results. n = len ( filtered_df ) for i , row in filtered_df . iterrows (): chunk = row [ \"chunk\" ] question , answer = generate_question_answer ( chunk ) print ( f \" { i + 1 } / { n } : { question } \" ) queries . append ( { \"question\" : question , \"answer\" : answer , \"chunk\" : chunk , \"chunk_id\" : row [ \"chunk_index\" ], \"source\" : row [ \"source\" ], } ) Loaded from cache\n1/15: What is the purpose of the MLflow Model Registry?\nLoaded from cache\n2/15: What are the key features of a registered model in the Model Registry?\nLoaded from cache\n3/15: What can you do with tags in MLflow?\nLoaded from cache\n4/15: What is the purpose of an MLflow Model?\nLoaded from cache\n5/15: What are the flavors defined in the MLmodel file for the mlflow.sklearn library?\nLoaded from cache\n6/15: What are the fields that can be contained in the MLmodel YAML format?\nLoaded from cache\n7/15: What is an MLflow Project?\nLoaded from cache\n8/15: What can you specify for the entry points in a MLproject file?\nLoaded from cache\n9/15: What are the project environments supported by MLflow?\nLoaded from cache\n10/15: What does the MLflow UI and API support in terms of searching runs?\nLoaded from cache\n11/15: What are the key information in the given paragraph?\n12/15: What are some examples of entity names that contain special characters?\n13/15: What is the purpose of MLflow Tracking?\n14/15: What information does each run record in MLflow Tracking?\n15/15: How can you create an experiment in MLflow? Sometimes, the LLM may fail to generate a question. We can examine the data above to see whether there are any errors. If so, remove the error records. [ ]: result_df = pd . DataFrame ( queries ) result_df = result_df [ result_df [ \"answer\" ] != \"N/A\" ] [10]: def add_to_output_df ( result_df = pd . DataFrame ({})): \"\"\" This function adds the records in result_df to the existing records saved at OUTPUT_DF_PATH, remove the duplicate rows and save the new collection of records back to OUTPUT_DF_PATH. \"\"\" if os . path . exists ( OUTPUT_DF_PATH ): all_result_df = pd . read_csv ( OUTPUT_DF_PATH ) else : all_result_df = pd . DataFrame ({}) all_result_df = ( pd . concat ([ all_result_df , result_df ], ignore_index = True ) . drop_duplicates () . sort_values ( by = [ \"source\" , \"chunk_id\" ]) . reset_index ( drop = True ) ) all_result_df . to_csv ( OUTPUT_DF_PATH , index = False ) return all_result_df [11]: all_result_df = add_to_output_df ( result_df ) [12]: all_result_df . head ( 3 ) [12]: question answer chunk chunk_id source 0 What is the purpose of the MLflow Model Registry? The purpose of the MLflow Model Registry is to... Documentation MLflow Model Registry MLflow Mod... 0 model-registry.html 1 What is the purpose of registering a model wit... The purpose of registering a model with the Mo... logged, this model can then be registered with... 1 model-registry.html 2 What can you do with registered models and mod... With registered models and model versions, you... associate with registered models and model ver... 2 model-registry.html ",
        "id": "c7c5a6402df5b4e5acede51d34ee19ce"
    },
    {
        "text": " Quality Analysis of Questions Generated (Optional) If you would like to compare quality of questions generated across different prompts, we can analyze the quality of questions manually and in aggregate. We want to evaluate questions along two dimensions - their diversity and relevance. https://github.com/mlflow/mlflow/blob/master/examples/llms/question_generation/question_answer_source.csv is a pre-generated dataset with 56 questions. You can download it and specify the path with OUTPUT_DF_PATH , and load it to run the rest of the notebook if you want to jump to this section. Note: There isn\u00e2\u0080\u0099t a well-defined way to analyze the quality of generated questions, so this is just one approach you can take to gain insight into how diverse and relevant your generated questions are. [14]: all_result_df = add_to_output_df ()  Evaluating Diversity of Questions Diversity of questions is important because we want questions to cover the majority of the document content. In addition, we want to be able to evaluate the retriever with different forms of questioning. We want to be able to have harder questions and easier questions. All of these are not straightforward to analyze, and we decided to analyze its through question length and latent space embeddings.  Length Length gives a sense of how diverse the questions are. Some questions may be wordy while others are straight to the point. It also allows us to identify problems with the question generated. [ ]: # Length questions = all_result_df [ \"question\" ] . to_list () question_len = pd . DataFrame ([ len ( q ) for q in questions ], columns = [ \"length\" ]) question_len . hist ( bins = 5 ) plt . title ( \"Histogram of Question Lengths\" ) plt . xlabel ( \"Question Length\" ) plt . ylabel ( \"Frequency\" ) plt . show () In addition to visual representation, we also want to look at more concrete percentile values. [ ]: # Calculating percentile values p10 = int ( question_len [ \"length\" ] . quantile ( 0.10 )) p90 = int ( question_len [ \"length\" ] . quantile ( 0.90 )) print ( \"p10-p90 range is\" , p90 - p10 ) p10-p90 range is 46 There are also a couple queries that are longer than normal. However, these seem fine. [ ]: [ q for q in questions if len ( q ) > 100 ] ['What is a common configuration for lowering the total memory pressure for pytorch models within transformers pipelines?',\n 'How can you get all active runs from experiments IDs 3, 4, and 17 that used a CNN model with 10 layers and had a prediction accuracy of 94.5% or higher?',\n 'What interfaces does the MLflow client use to record MLflow entities and artifacts when running MLflow on a local machine with a SQLAlchemy-compatible database?'] ",
        "id": "f60c3f24e47d951d980c857d685cb335"
    },
    {
        "text": " Latent Space Latent space embeddings contain semantic information about the question. This can be used to evaluate the diversity and the difference between two questions semantically. To do so, we will need to map the high dimensional space to a lower dimensional space. We utilize PCA and TSNE to map the embeddings into a 2-dimensional space for visualization. We append 5 benchmark queries to help visualize how diverse the questions are. The first four of these questions are semantically similar and all asking about MLflow, while the last is different and asks about RAG. [ ]: benchmark_questions = [ \"What is MLflow?\" , \"What is MLflow about?\" , \"What is MLflow Tracking?\" , \"What is MLflow Evaluation?\" , \"Why is RAG so popular?\" , ] questions_to_embed = questions + benchmark_questions We apply PCA to reduce the embedding dimensions to 10 before applying TSNE to reduce it to 2 dimensions, as recommended by sklearn due to the computational complexity of TSNE. [ ]: # Apply embeddings embeddings = OpenAIEmbeddings () question_embeddings = embeddings . embed_documents ( questions_to_embed ) # PCA on embeddings to reduce to 10-dim pca = PCA ( n_components = 10 ) question_embeddings_reduced = pca . fit_transform ( question_embeddings ) # TSNE on embeddings to reduce to 2-dim tsne = TSNE ( n_components = 2 , random_state = SEED ) lower_dim_embeddings = tsne . fit_transform ( question_embeddings_reduced ) Now that we have 2-dimensional embeddings representing the semantics of the question, we can visualize it with a scatter plot, differentiating the generated questions and the benchmark questions. [ ]: labels = np . concatenate ( [ np . full ( len ( lower_dim_embeddings ) - len ( benchmark_questions ), \"generated\" ), np . full ( len ( benchmark_questions ), \"benchmark\" ), ] ) data = pd . DataFrame ( { \"x\" : lower_dim_embeddings [:, 0 ], \"y\" : lower_dim_embeddings [:, 1 ], \"label\" : labels } ) sns . scatterplot ( data = data , x = \"x\" , y = \"y\" , hue = \"label\" ) Observe that within the orange points on the scatter plot, there is one point that is further than the others. That is the unique benchmark question about RAG. This plot gives a sense of the diversity of the questions generated.  Evaluate Document Relevance Another important axis to consider is how relevant the questions are to the document we provided. We want to understand whether the questions generated by the LLM is actually referring to our provided text, or whether it is hallucinating irrelevant questions. We will evaluate relevance by first manually checking certain questions against their document chunk. Then, we define a measure of relevance to analyze it quantitatively.  Manual Checking of Document Relevance Manual qualitative check of whether the questions are relevant to the document. [ ]: all_result_df . sample ( 3 ) question answer chunk chunk_id source 27 What is an MLflow Project? An MLflow Project is a format for packaging da... Documentation MLflow Projects MLflow Projects ... 0 projects.html 54 What information does autologging capture when... Autologging captures the following information... when launching short-lived MLflow runs that re... 21 tracking.html 38 What is the syntax for searching runs using th... The syntax for searching runs using the MLflow... Documentation Search Runs Search Runs The MLfl... 0 search-runs.html ",
        "id": "930ec96593b37c202e2426bd463f8ef7"
    },
    {
        "text": " Embeddings Cosine Similarity The embedding of the chunk and query is placed in the same latent space, and the retriever model would extract similar chunk embeddings to a query embedding. Hence, relevance for the retriever is defined by the distance of embeddings in this latent space. Cosine similarity is a measure of vector similarity, and can be used to determine the distance of embeddings between the chunk and the query. It is a distance metric that approaches 1 when the question and chunk are similar, and becomes 0 when they are different. We can use the cosine similarity score directly to measure the relevancy. [ ]: embedded_queries = all_result_df . copy () embedded_queries [ \"chunk_emb\" ] = all_result_df [ \"chunk\" ] . apply ( lambda x : np . squeeze ( cached_langchain_openai_embeddings ( chunk = x , cache = embeddings_cache )) ) embedded_queries [ \"question_emb\" ] = all_result_df [ \"question\" ] . apply ( lambda x : np . squeeze ( cached_langchain_openai_embeddings ( chunk = x , cache = embeddings_cache )) ) [ ]: def cossim ( x , y ): return np . dot ( x , y ) / ( np . linalg . norm ( x ) * np . linalg . norm ( y )) embedded_queries [ \"cossim\" ] = embedded_queries . apply ( lambda row : cossim ( row [ \"question_emb\" ], row [ \"chunk_emb\" ]), axis = 1 ) After we score each question by its relative relevancy, we can evaluate the generated questions as a whole. [ ]: scores = embedded_queries [ \"cossim\" ] . to_list () plt . hist ( scores , bins = 5 ) (array([ 1.,  8., 15., 20., 12.]),\n array([0.72730601, 0.76292693, 0.79854785, 0.83416876, 0.86978968,\n        0.9054106 ]),\n <BarContainer object of 5 artists>) There are a couple lower scores. Let\u00e2\u0080\u0099s take a look at them. [ ]: mask = embedded_queries [ \"cossim\" ] < 0.75 lower_cossim = embedded_queries [ mask ] for i , row in lower_cossim . iterrows (): print ( f \"Question: { i } \" ) print ( row [ \"question\" ]) print ( \"Chunk:\" ) print ( row [ \"chunk\" ]) print ( \"cossim:\" ) print ( row [ \"cossim\" ]) Question: 45\nWhat is the purpose of the 'experimentIds' variable in the given paragraph?\nChunk:\nAPI. List<Long> experimentIds = Arrays.asList(\"1\", \"2\", \"4\", \"8\"); List<RunInfo> searchResult = client.searchRuns(experimentIds, \"metrics.accuracy_score < 99.90\"); Previous Next \u00c2\u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.\ncossim:\n0.7273060141018568 Manual inspection of these less relevant questions reveals that some chunks are less informative or mainly consists of code, hence the generated question might be less useful. You can choose to filter these as desired. Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "b3070b0c9d24cc2bcd960faf5b1aa8c5"
    },
    {
        "text": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# Retriever Evaluation with MLflow\"\n   ]\n  },\n  {\n   \"cell_type\": \"raw\",\n   \"metadata\": {},\n   \"source\": [\n    \" Download this Notebook \"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {},\n     \"inputWidgets\": {},\n     \"nuid\": \"f8938de9-7fae-41cd-ad6b-7ee26c288eab\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"source\": [\n    \"In MLflow 2.8.0, we introduced a new model type \\\"retriever\\\" to the `mlflow.evaluate()` API. It helps you to evaluate the retriever in a RAG application. It contains two built-in metrics `precision_at_k` and `recall_at_k`. In MLflow 2.9.0, `ndcg_at_k` is available.\\n\",\n    \"\\n\",\n    \"This notebook illustrates how to use `mlflow.evaluate()` to evaluate the retriever in a RAG application. It has the following steps:\\n\",\n    \"\\n\",\n    \"* Step 1: Install and Load Packages\\n\",\n    \"* Step 2: Evaluation Dataset Preparation\\n\",\n    \"* Step 3: Calling `mlflow.evaluate()`\\n\",\n    \"* Step 4: Result Analysis and Visualization\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Step 1: Install and Load Packages\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {\n      \"byteLimit\": 2048000,\n      \"rowLimit\": 10000\n     },\n     \"inputWidgets\": {},\n     \"nuid\": \"5bf12edb-2498-4edd-aeff-b4844451850f\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%pip install mlflow==2.9.0 langchain==0.0.339 openai faiss-cpu gensim nltk pyLDAvis tiktoken\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 1,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {\n      \"byteLimit\": 2048000,\n      \"rowLimit\": 10000\n     },\n     \"inputWidgets\": {},\n     \"nuid\": \"414eb948-7f7a-411b-8308-facadb0bdde8\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"import ast\\n\",\n    \"import os\\n\",\n    \"import pprint\\n\",\n    \"\\n\",\n    \"import pandas as pd\\n\",\n    \"from langchain.docstore.document import Document\\n\",\n    \"from langchain.embeddings.openai import OpenAIEmbeddings\\n\",\n    \"from langchain.text_splitter import CharacterTextSplitter\\n\",\n    \"from langchain.vectorstores import FAISS\\n\",\n    \"\\n\",\n    \"import mlflow\\n\",\n    \"\\n\",\n    \"os.environ[\\\"OPENAI_API_KEY\\\"] = \\\" \\\"\\n\",\n    \"\\n\",\n    \"CHUNK_SIZE = 1000\\n\",\n    \"\\n\",\n    \"# Assume running from https://github.com/mlflow/mlflow/blob/master/examples/llms/rag\\n\",\n    \"OUTPUT_DF_PATH = \\\"question_answer_source.csv\\\"\\n\",\n    \"SCRAPPED_DOCS_PATH = \\\"mlflow_docs_scraped.csv\\\"\\n\",\n    \"EVALUATION_DATASET_PATH = \\\"static_evaluation_dataset.csv\\\"\\n\",\n    \"DB_PERSIST_DIR = \\\"faiss_index\\\"\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {},\n     \"inputWidgets\": {},\n     \"nuid\": \"eebcf0d9-6634-47d9-808d-e79c5a50fbbf\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"source\": [\n    \"## Step 2: Evaluation Dataset Preparation\\n\",\n    \"The evaluation dataset should contain three columns: questions, ground truth doc IDs, retrieved relevant doc IDs. A \\\"doc ID\\\" is a unique string identifier of the documents in you RAG application. For example, it could be the URL of a documentation web page, or the file path of a PDF document.\\n\",\n    \"\\n\",\n    \"If you have a list of questions that you would like to evaluate, please see 1.1 Manual Preparation. If you do not have a question list yet, please see 1.2 Generate the Evaluation Dataset.\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {},\n     \"inputWidgets\": {},\n     \"nuid\": \"f8a690cc-7672-4f24-8518-8faabfc9afea\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"source\": [\n    \"### Manual Preparation\\n\",\n    \"\\n\",\n    \"When evaluating a retriever, it's recommended t",
        "id": "44bced3dc10e2d6be299359053d7c9fa"
    },
    {
        "text": "ike to evaluate, please see 1.1 Manual Preparation. If you do not have a question list yet, please see 1.2 Generate the Evaluation Dataset.\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {},\n     \"inputWidgets\": {},\n     \"nuid\": \"f8a690cc-7672-4f24-8518-8faabfc9afea\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"source\": [\n    \"### Manual Preparation\\n\",\n    \"\\n\",\n    \"When evaluating a retriever, it's recommended to save the retrieved document IDs into a static dataset represented by a Pandas Dataframe or an MLflow Pandas Dataset containing the input queries, retrieved relevant document IDs, and the ground-truth document IDs for the evaluation.\\n\",\n    \"\\n\",\n    \"#### Concepts\\n\",\n    \"\\n\",\n    \"A \\\"document ID\\\" is a string that identifies a document.\\n\",\n    \"\\n\",\n    \"A list of \\\"retrieved relevant document IDs\\\" are the output of the retriever for a specific input query and a `k` value.\\n\",\n    \"\\n\",\n    \"A list of \\\"ground-truth document IDs\\\" are the labeled relevant documents for a specific input query.\\n\",\n    \"\\n\",\n    \"#### Expected Data Format\\n\",\n    \"\\n\",\n    \"For each row, the retrieved relevant document IDs and the ground-truth relevant document IDs should be provided as a tuple of document ID strings.\\n\",\n    \"\\n\",\n    \"The column name of the retrieved relevant document IDs should be specified by the `predictions` parameter, and the column name of the ground-truth relevant document IDs should be specified by the `targets` parameter.\\n\",\n    \"\\n\",\n    \"Here is a simple example dataset that illustrates the expected data format. The doc IDs are the paths of the documentation pages.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {\n      \"byteLimit\": 2048000,\n      \"rowLimit\": 10000\n     },\n     \"inputWidgets\": {},\n     \"nuid\": \"1a61b1b2-582e-49d5-864d-b58d2b6c3392\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"data = pd.DataFrame(\\n\",\n    \"    {\\n\",\n    \"        \\\"questions\\\": [\\n\",\n    \"            \\\"What is MLflow?\\\",\\n\",\n    \"            \\\"What is Databricks?\\\",\\n\",\n    \"            \\\"How to serve a model on Databricks?\\\",\\n\",\n    \"            \\\"How to enable MLflow Autologging for my workspace by default?\\\",\\n\",\n    \"        ],\\n\",\n    \"        \\\"retrieved_context\\\": [\\n\",\n    \"            [\\n\",\n    \"                \\\"mlflow/index.html\\\",\\n\",\n    \"                \\\"mlflow/quick-start.html\\\",\\n\",\n    \"            ],\\n\",\n    \"            [\\n\",\n    \"                \\\"introduction/index.html\\\",\\n\",\n    \"                \\\"getting-started/overview.html\\\",\\n\",\n    \"            ],\\n\",\n    \"            [\\n\",\n    \"                \\\"machine-learning/model-serving/index.html\\\",\\n\",\n    \"                \\\"machine-learning/model-serving/model-serving-intro.html\\\",\\n\",\n    \"            ],\\n\",\n    \"            [],\\n\",\n    \"        ],\\n\",\n    \"        \\\"ground_truth_context\\\": [\\n\",\n    \"            [\\\"mlflow/index.html\\\"],\\n\",\n    \"            [\\\"introduction/index.html\\\"],\\n\",\n    \"            [\\n\",\n    \"                \\\"machine-learning/model-serving/index.html\\\",\\n\",\n    \"                \\\"machine-learning/model-serving/llm-optimized-model-serving.html\\\",\\n\",\n    \"            ],\\n\",\n    \"            [\\\"mlflow/databricks-autologging.html\\\"],\\n\",\n    \"        ],\\n\",\n    \"    }\\n\",\n    \")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {},\n     \"inputWidgets\": {},\n     \"nuid\": \"f740b47c-71ee-4633-944c-172887ff5081\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"source\": [\n    \"### Generate the Evaluation Dataset\\n\",\n    \"There are two steps to generate the evaluation dataset: generate questions with ground truth doc IDs and retrieve relevant doc IDs. \"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {},\n     \"inputWidge",
        "id": "ce691b5d0274eaa2bbb234a4a3b172da"
    },
    {
        "text": "v1+cell\": {\n     \"cellMetadata\": {},\n     \"inputWidgets\": {},\n     \"nuid\": \"f740b47c-71ee-4633-944c-172887ff5081\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"source\": [\n    \"### Generate the Evaluation Dataset\\n\",\n    \"There are two steps to generate the evaluation dataset: generate questions with ground truth doc IDs and retrieve relevant doc IDs. \"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {},\n     \"inputWidgets\": {},\n     \"nuid\": \"f6beddae-85e2-44e7-8ec6-7ca2f02bc16b\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"source\": [\n    \"\\n\",\n    \"#### Generate Questions with Ground Truth Doc IDs\\n\",\n    \"If you don't have a list of questions to evaluate, you can generate them using LLMs. The [Question Generation Notebook](https://mlflow.org/docs/latest/llms/rag/notebooks/question-generation-retrieval-evaluation.html) provides an example way to do it. Here is the result of running that notebook.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 5,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {\n      \"byteLimit\": 2048000,\n      \"rowLimit\": 10000\n     },\n     \"inputWidgets\": {},\n     \"nuid\": \"98bf55c7-3e58-4fff-bc0e-1af58d64839f\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"generated_df = pd.read_csv(OUTPUT_DF_PATH)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 7,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {\n      \"byteLimit\": 2048000,\n      \"rowLimit\": 10000\n     },\n     \"inputWidgets\": {},\n     \"nuid\": \"17baa097-457f-46df-9e25-56061972785f\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/html\": [\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" question \\n\",\n       \" answer \\n\",\n       \" chunk \\n\",\n       \" chunk_id \\n\",\n       \" source \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" 0 \\n\",\n       \" What is the purpose of the MLflow Model Registry? \\n\",\n       \" The purpose of the MLflow Model Registry is to... \\n\",\n       \" Documentation MLflow Model Registry MLflow Mod... \\n\",\n       \" 0 \\n\",\n       \" model-registry.html \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" 1 \\n\",\n       \" What is the purpose of registering a model wit... \\n\",\n       \" The purpose of registering a model with the Mo... \\n\",\n       \" logged, this model can then be registered with... \\n\",\n       \" 1 \\n\",\n       \" model-registry.html \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" 2 \\n\",\n       \" What can you do with registered models and mod... \\n\",\n       \" With registered models and model versions, you... \\n\",\n       \" associate with registered models and model ver... \\n\",\n       \" 2 \\n\",\n       \" model-registry.html \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \"\n      ],\n      \"text/plain\": [\n       \"                                            question  \\\\\\n\",\n       \"0  What is the purpose of the MLflow Model Registry?   \\n\",\n       \"1  What is the purpose of registering a model wit...   \\n\",\n       \"2  What can you do with registered models and mod...   \\n\",\n       \"\\n\",\n       \"                                              answer  \\\\\\n\",\n       \"0  The purpose of the MLflow Model Registry is to...   \\n\",\n       \"1  The purpose of registering a model with the Mo...   \\n\",\n       \"2  With registered models and model versions, you...   \\n\",\n       \"\\n\",\n       \"                                               chunk  chunk_id  \\\\\\n\",\n       \"0  Documentation MLflow Model Registry MLflow Mod...         0   \\n\",\n       \"1  logged, this model can then be registered with...         1   \\n\",\n       \"2  associate with registered models and model ver...         2   \\n\",\n       \"\\n\",\n       \"                source  \\n\",\n       \"0  model-registry.html  \\n\",\n       \"1  model-registry.html  \\n\",\n       \"2  model-registry.html  \"\n      ]\n     },\n     \"execution_count\": 7,\n     \"metadata\":",
        "id": "92df14beedd583556c1023c3a3aa3eea"
    },
    {
        "text": "                                     chunk  chunk_id  \\\\\\n\",\n       \"0  Documentation MLflow Model Registry MLflow Mod...         0   \\n\",\n       \"1  logged, this model can then be registered with...         1   \\n\",\n       \"2  associate with registered models and model ver...         2   \\n\",\n       \"\\n\",\n       \"                source  \\n\",\n       \"0  model-registry.html  \\n\",\n       \"1  model-registry.html  \\n\",\n       \"2  model-registry.html  \"\n      ]\n     },\n     \"execution_count\": 7,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"generated_df.head(3)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 8,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {\n      \"byteLimit\": 2048000,\n      \"rowLimit\": 10000\n     },\n     \"inputWidgets\": {},\n     \"nuid\": \"93165dc5-aff9-46f9-83ab-e6dbfcbbc32b\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/html\": [\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" question \\n\",\n       \" source \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" 0 \\n\",\n       \" What is the purpose of the MLflow Model Registry? \\n\",\n       \" [model-registry.html] \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" 1 \\n\",\n       \" What is the purpose of registering a model wit... \\n\",\n       \" [model-registry.html] \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" 2 \\n\",\n       \" What can you do with registered models and mod... \\n\",\n       \" [model-registry.html] \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \"\n      ],\n      \"text/plain\": [\n       \"                                            question                 source\\n\",\n       \"0  What is the purpose of the MLflow Model Registry?  [model-registry.html]\\n\",\n       \"1  What is the purpose of registering a model wit...  [model-registry.html]\\n\",\n       \"2  What can you do with registered models and mod...  [model-registry.html]\"\n      ]\n     },\n     \"execution_count\": 8,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"# Prepare dataframe `data` with the required format\\n\",\n    \"data = pd.DataFrame({})\\n\",\n    \"data[\\\"question\\\"] = generated_df[\\\"question\\\"].copy(deep=True)\\n\",\n    \"data[\\\"source\\\"] = generated_df[\\\"source\\\"].apply(lambda x: [x])\\n\",\n    \"data.head(3)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {},\n     \"inputWidgets\": {},\n     \"nuid\": \"3eabe651-28be-45bb-94ad-58e6bc582137\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"source\": [\n    \"#### Retrieve Relevant Doc IDs\\n\",\n    \"\\n\",\n    \"Once we have a list of questions with ground truth doc IDs from 1.1, we can collect the retrieved relevant doc IDs. In this tutorial, we use a LangChain retriever. You can plug in your own retriever as needed.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {},\n     \"inputWidgets\": {},\n     \"nuid\": \"9817f671-f2fd-4b2e-abe9-3bc9afd9ce3c\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"source\": [\n    \"First, we build a FAISS retriever from the docs saved at https://github.com/mlflow/mlflow/blob/master/examples/llms/question_generation/mlflow_docs_scraped.csv. See the [Question Generation Notebook](https://mlflow.org/docs/latest/llms/rag/notebooks/question-generation-retrieval-evaluation.html) for how to create this csv file.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 10,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {\n      \"byteLimit\": 2048000,\n      \"rowLimit\": 10000\n     },\n     \"inputWidgets\": {},\n     \"nuid\": \"178b45b4-11f9-47ca-9564-c8caa32d2504\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"embeddings = OpenAIEmbeddings()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+c",
        "id": "10570c1d89e5113419401f17a5b9b2d5"
    },
    {
        "text": "cell_type\": \"code\",\n   \"execution_count\": 10,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {\n      \"byteLimit\": 2048000,\n      \"rowLimit\": 10000\n     },\n     \"inputWidgets\": {},\n     \"nuid\": \"178b45b4-11f9-47ca-9564-c8caa32d2504\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"embeddings = OpenAIEmbeddings()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {\n      \"byteLimit\": 2048000,\n      \"rowLimit\": 10000\n     },\n     \"inputWidgets\": {},\n     \"nuid\": \"e5a113bb-11b8-4d1a-a21b-b59b523f3525\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"scrapped_df = pd.read_csv(SCRAPPED_DOCS_PATH)\\n\",\n    \"list_of_documents = [\\n\",\n    \"    Document(page_content=row[\\\"text\\\"], metadata={\\\"source\\\": row[\\\"source\\\"]})\\n\",\n    \"    for i, row in scrapped_df.iterrows()\\n\",\n    \"]\\n\",\n    \"text_splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=0)\\n\",\n    \"docs = text_splitter.split_documents(list_of_documents)\\n\",\n    \"db = FAISS.from_documents(docs, embeddings)\\n\",\n    \"\\n\",\n    \"# Save the db to local disk\\n\",\n    \"db.save_local(DB_PERSIST_DIR)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 11,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {\n      \"byteLimit\": 2048000,\n      \"rowLimit\": 10000\n     },\n     \"inputWidgets\": {},\n     \"nuid\": \"bace7c63-e3d5-42f3-bf6a-00ef1842baae\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Load the db from local disk\\n\",\n    \"db = FAISS.load_local(DB_PERSIST_DIR, embeddings)\\n\",\n    \"retriever = db.as_retriever()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 13,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {\n      \"byteLimit\": 2048000,\n      \"rowLimit\": 10000\n     },\n     \"inputWidgets\": {},\n     \"nuid\": \"c06bcb3c-58c8-454c-bf5b-e29ec227991f\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/plain\": [\n       \"4\"\n      ]\n     },\n     \"execution_count\": 13,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"# Test the retriever with a query\\n\",\n    \"retrieved_docs = retriever.get_relevant_documents(\\n\",\n    \"    \\\"What is the purpose of the MLflow Model Registry?\\\"\\n\",\n    \")\\n\",\n    \"len(retrieved_docs)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {},\n     \"inputWidgets\": {},\n     \"nuid\": \"2ec7b458-c248-4ec0-9d85-0e447d6b4ecd\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"source\": [\n    \"After building a retriever, we define a function that takes a question string as input and returns a list of relevant doc ID strings.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 14,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {\n      \"byteLimit\": 2048000,\n      \"rowLimit\": 10000\n     },\n     \"inputWidgets\": {},\n     \"nuid\": \"bc688e4b-3389-4804-b7bf-159bce4f9db8\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Define a function to return a list of retrieved doc ids\\n\",\n    \"def retrieve_doc_ids(question: str) -> list[str]:\\n\",\n    \"    docs = retriever.get_relevant_documents(question)\\n\",\n    \"    return [doc.metadata[\\\"source\\\"] for doc in docs]\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {},\n     \"inputWidgets\": {},\n     \"nuid\": \"330a3336-6ca2-455f-a1ae-5bd842a4d2bb\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"source\": [\n    \"We can store the retrieved doc IDs in the dataframe as a column \\\"retrieved_doc_ids\\\".\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 17,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\":",
        "id": "89376c52d346c5b104ef48b849864819"
    },
    {
        "text": "  ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {},\n     \"inputWidgets\": {},\n     \"nuid\": \"330a3336-6ca2-455f-a1ae-5bd842a4d2bb\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"source\": [\n    \"We can store the retrieved doc IDs in the dataframe as a column \\\"retrieved_doc_ids\\\".\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 17,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {\n      \"byteLimit\": 2048000,\n      \"rowLimit\": 10000\n     },\n     \"inputWidgets\": {},\n     \"nuid\": \"f96ec69b-bea3-4023-8cd3-6bee1e327ff0\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/html\": [\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" question \\n\",\n       \" source \\n\",\n       \" retrieved_doc_ids \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" 0 \\n\",\n       \" What is the purpose of the MLflow Model Registry? \\n\",\n       \" [model-registry.html] \\n\",\n       \" [model-registry.html, introduction/index.html,... \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" 1 \\n\",\n       \" What is the purpose of registering a model wit... \\n\",\n       \" [model-registry.html] \\n\",\n       \" [model-registry.html, models.html, introductio... \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" 2 \\n\",\n       \" What can you do with registered models and mod... \\n\",\n       \" [model-registry.html] \\n\",\n       \" [model-registry.html, models.html, deployment/... \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \"\n      ],\n      \"text/plain\": [\n       \"                                            question                 source  \\\\\\n\",\n       \"0  What is the purpose of the MLflow Model Registry?  [model-registry.html]   \\n\",\n       \"1  What is the purpose of registering a model wit...  [model-registry.html]   \\n\",\n       \"2  What can you do with registered models and mod...  [model-registry.html]   \\n\",\n       \"\\n\",\n       \"                                   retrieved_doc_ids  \\n\",\n       \"0  [model-registry.html, introduction/index.html,...  \\n\",\n       \"1  [model-registry.html, models.html, introductio...  \\n\",\n       \"2  [model-registry.html, models.html, deployment/...  \"\n      ]\n     },\n     \"execution_count\": 17,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"data[\\\"retrieved_doc_ids\\\"] = data[\\\"question\\\"].apply(retrieve_doc_ids)\\n\",\n    \"data.head(3)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {\n      \"byteLimit\": 2048000,\n      \"rowLimit\": 10000\n     },\n     \"inputWidgets\": {},\n     \"nuid\": \"5e5c4cd1-38c3-4709-8d41-6e319fb8a924\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Persist the static evaluation dataset to disk\\n\",\n    \"data.to_csv(EVALUATION_DATASET_PATH, index=False)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 16,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {\n      \"byteLimit\": 2048000,\n      \"rowLimit\": 10000\n     },\n     \"inputWidgets\": {},\n     \"nuid\": \"deabd8f0-44cf-409f-a27b-e82dd4d99940\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/html\": [\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" question \\n\",\n       \" source \\n\",\n       \" retrieved_doc_ids \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" 0 \\n\",\n       \" What is the purpose of the MLflow Model Registry? \\n\",\n       \" [model-registry.html] \\n\",\n       \" [model-registry.html, introduction/index.html,... \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" 1 \\n\",\n       \" What is the purpose of registering a model wit... \\n\",\n       \" [model-registry.html] \\n\",\n       \" [model-registry.html, models.html, introductio... \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" 2 \\n\",\n       \" What can you do wit",
        "id": "964444593b459853e6e87fb908039213"
    },
    {
        "text": "   \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" 0 \\n\",\n       \" What is the purpose of the MLflow Model Registry? \\n\",\n       \" [model-registry.html] \\n\",\n       \" [model-registry.html, introduction/index.html,... \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" 1 \\n\",\n       \" What is the purpose of registering a model wit... \\n\",\n       \" [model-registry.html] \\n\",\n       \" [model-registry.html, models.html, introductio... \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" 2 \\n\",\n       \" What can you do with registered models and mod... \\n\",\n       \" [model-registry.html] \\n\",\n       \" [model-registry.html, models.html, deployment/... \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \"\n      ],\n      \"text/plain\": [\n       \"                                            question                 source  \\\\\\n\",\n       \"0  What is the purpose of the MLflow Model Registry?  [model-registry.html]   \\n\",\n       \"1  What is the purpose of registering a model wit...  [model-registry.html]   \\n\",\n       \"2  What can you do with registered models and mod...  [model-registry.html]   \\n\",\n       \"\\n\",\n       \"                                   retrieved_doc_ids  \\n\",\n       \"0  [model-registry.html, introduction/index.html,...  \\n\",\n       \"1  [model-registry.html, models.html, introductio...  \\n\",\n       \"2  [model-registry.html, models.html, deployment/...  \"\n      ]\n     },\n     \"execution_count\": 16,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"# Load the static evaluation dataset from disk and deserialize the source and retrieved doc ids\\n\",\n    \"data = pd.read_csv(EVALUATION_DATASET_PATH)\\n\",\n    \"data[\\\"source\\\"] = data[\\\"source\\\"].apply(ast.literal_eval)\\n\",\n    \"data[\\\"retrieved_doc_ids\\\"] = data[\\\"retrieved_doc_ids\\\"].apply(ast.literal_eval)\\n\",\n    \"data.head(3)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {},\n     \"inputWidgets\": {},\n     \"nuid\": \"c62b5306-9c0d-4ce8-8c4a-23cb1ecc7f66\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"source\": [\n    \"## Step 3: Calling `mlflow.evaluate()`\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {},\n     \"inputWidgets\": {},\n     \"nuid\": \"bdeebdcc-b4e7-4f9d-8fdc-366f9c13ed20\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"source\": [\n    \"### Metrics Definition\\n\",\n    \"\\n\",\n    \"There are three built-in metrics provided for the retriever model type. Click the metric name below to see the metrics definitions.\\n\",\n    \"\\n\",\n    \"1.  [mlflow.metrics.precision_at_k(k)](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.precision_at_k)\\n\",\n    \"1.  [mlflow.metrics.recall_at_k(k)](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.recall_at_k)\\n\",\n    \"1.  [mlflow.metrics.ndcg_at_k(k)](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.ndcg_at_k) \\n\",\n    \"\\n\",\n    \"All metrics compute a score between 0 and 1 for each row representing the corresponding metric of the retriever model at the given `k` value.\\n\",\n    \"\\n\",\n    \"The `k` parameter should be a positive integer representing the number of retrieved documents\\n\",\n    \"to evaluate for each row. `k` defaults to 3.\\n\",\n    \"\\n\",\n    \"When the model type is `\\\"retriever\\\"`, these metrics will be calculated automatically with the\\n\",\n    \"default `k` value of 3.\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {},\n     \"inputWidgets\": {},\n     \"nuid\": \"25a32237-b2ef-4f4a-9e5a-4537e7e43012\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"source\": [\n    \"### Basic usage\\n\",\n    \"\\n\",\n    \"There are two supported ways to specify the retriever's output:\\n\",\n    \"\\n\",\n    \"* Case 1: Save the retriever's output to a static evaluation dataset\\n\",\n    \"* Case 2: Wrap the retriever in a function\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null",
        "id": "5d260316a8524865ac4a8f014308967d"
    },
    {
        "text": "ion/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {},\n     \"inputWidgets\": {},\n     \"nuid\": \"25a32237-b2ef-4f4a-9e5a-4537e7e43012\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"source\": [\n    \"### Basic usage\\n\",\n    \"\\n\",\n    \"There are two supported ways to specify the retriever's output:\\n\",\n    \"\\n\",\n    \"* Case 1: Save the retriever's output to a static evaluation dataset\\n\",\n    \"* Case 2: Wrap the retriever in a function\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {\n      \"byteLimit\": 2048000,\n      \"rowLimit\": 10000\n     },\n     \"inputWidgets\": {},\n     \"nuid\": \"0390728a-a6cf-4c84-867a-0c6832114471\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"outputs\": [\n    {\n     \"name\": \"stderr\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"2023/11/22 14:39:59 WARNING mlflow.data.pandas_dataset: Failed to infer schema for Pandas dataset. Exception: Unable to map 'object' type to MLflow DataType. object can be mapped iff all values have identical data type which is one of (string, (bytes or byterray),  int, float).\\n\",\n      \"2023/11/22 14:39:59 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\\n\",\n      \"2023/11/22 14:39:59 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\\n\",\n      \"2023/11/22 14:39:59 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: precision_at_3\\n\",\n      \"2023/11/22 14:39:59 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: recall_at_3\\n\",\n      \"2023/11/22 14:39:59 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: ndcg_at_3\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"# Case 1: Evaluating a static evaluation dataset\\n\",\n    \"with mlflow.start_run() as run:\\n\",\n    \"    evaluate_results = mlflow.evaluate(\\n\",\n    \"        data=data,\\n\",\n    \"        model_type=\\\"retriever\\\",\\n\",\n    \"        targets=\\\"source\\\",\\n\",\n    \"        predictions=\\\"retrieved_doc_ids\\\",\\n\",\n    \"        evaluators=\\\"default\\\",\\n\",\n    \"    )\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 18,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {\n      \"byteLimit\": 2048000,\n      \"rowLimit\": 10000\n     },\n     \"inputWidgets\": {},\n     \"nuid\": \"70aa6719-f69d-4fda-8a67-ac4e0d8ea6d8\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/html\": [\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" question \\n\",\n       \" source \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" 0 \\n\",\n       \" What is the purpose of the MLflow Model Registry? \\n\",\n       \" [model-registry.html] \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" 1 \\n\",\n       \" What is the purpose of registering a model wit... \\n\",\n       \" [model-registry.html] \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" 2 \\n\",\n       \" What can you do with registered models and mod... \\n\",\n       \" [model-registry.html] \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \"\n      ],\n      \"text/plain\": [\n       \"                                            question                 source\\n\",\n       \"0  What is the purpose of the MLflow Model Registry?  [model-registry.html]\\n\",\n       \"1  What is the purpose of registering a model wit...  [model-registry.html]\\n\",\n       \"2  What can you do with registered models and mod...  [model-registry.html]\"\n      ]\n     },\n     \"execution_count\": 18,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"question_source_df = data[[\\\"question\\\", \\\"source\\\"]]\\n\",\n    \"question_source_df.head(3)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {\n      \"byteLimit\": 2048000,\n      \"rowLimit\": 10000\n     },\n     \"inputWidgets\": {},\n     \"nuid\": \"00672280-3dfc-4c00-9ae2-bea50732ef8b\",\n     \"showT",
        "id": "5f1e2b00406b3bf8c187986283004c5f"
    },
    {
        "text": " },\n     \"execution_count\": 18,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"question_source_df = data[[\\\"question\\\", \\\"source\\\"]]\\n\",\n    \"question_source_df.head(3)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {\n      \"byteLimit\": 2048000,\n      \"rowLimit\": 10000\n     },\n     \"inputWidgets\": {},\n     \"nuid\": \"00672280-3dfc-4c00-9ae2-bea50732ef8b\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"outputs\": [\n    {\n     \"name\": \"stderr\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"2023/11/22 14:09:12 WARNING mlflow.data.pandas_dataset: Failed to infer schema for Pandas dataset. Exception: Unable to map 'object' type to MLflow DataType. object can be mapped iff all values have identical data type which is one of (string, (bytes or byterray),  int, float).\\n\",\n      \"2023/11/22 14:09:12 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\\n\",\n      \"2023/11/22 14:09:12 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\\n\",\n      \"2023/11/22 14:09:24 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\\n\",\n      \"2023/11/22 14:09:24 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: precision_at_3\\n\",\n      \"2023/11/22 14:09:24 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: recall_at_3\\n\",\n      \"2023/11/22 14:09:24 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: ndcg_at_3\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"# Case 2: Evaluating a function\\n\",\n    \"def retriever_model_function(question_df: pd.DataFrame) -> pd.Series:\\n\",\n    \"    return question_df[\\\"question\\\"].apply(retrieve_doc_ids)\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"with mlflow.start_run() as run:\\n\",\n    \"    evaluate_results = mlflow.evaluate(\\n\",\n    \"        model=retriever_model_function,\\n\",\n    \"        data=question_source_df,\\n\",\n    \"        model_type=\\\"retriever\\\",\\n\",\n    \"        targets=\\\"source\\\",\\n\",\n    \"        evaluators=\\\"default\\\",\\n\",\n    \"    )\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {\n      \"byteLimit\": 2048000,\n      \"rowLimit\": 10000\n     },\n     \"inputWidgets\": {},\n     \"nuid\": \"cb24318b-6149-4703-ad06-731c8a75866f\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"{   'ndcg_at_3/mean': 0.7530888125490431,\\n\",\n      \"    'ndcg_at_3/p90': 1.0,\\n\",\n      \"    'ndcg_at_3/variance': 0.1209151911325433,\\n\",\n      \"    'precision_at_3/mean': 0.26785714285714285,\\n\",\n      \"    'precision_at_3/p90': 0.3333333333333333,\\n\",\n      \"    'precision_at_3/variance': 0.017538265306122448,\\n\",\n      \"    'recall_at_3/mean': 0.8035714285714286,\\n\",\n      \"    'recall_at_3/p90': 1.0,\\n\",\n      \"    'recall_at_3/variance': 0.15784438775510204}\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"pp = pprint.PrettyPrinter(indent=4)\\n\",\n    \"pp.pprint(evaluate_results.metrics)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {},\n     \"inputWidgets\": {},\n     \"nuid\": \"7a9b83c5-1544-4b0e-81f6-7abc1fafa258\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"source\": [\n    \"### Try different k values\\n\",\n    \"To use another `k` value, use the `evaluator_config` parameter\\n\",\n    \"in the `mlflow.evaluate()` API as follows: `evaluator_config={\\\"retriever_k\\\": }`.\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"```python\\n\",\n    \"# Case 1: Specifying the model type\\n\",\n    \"evaluate_results = mlflow.evaluate(\\n\",\n    \"    data=data,\\n\",\n    \"    model_type=\\\"retriever\\\",\\n\",\n    \"    targets=\\\"ground_truth_context\\\",\\n\",\n    \"    predictions=\\\"retrieved_context\\\",\\n\",\n    \"    evaluators=\\\"default\\\",\\n\",\n    \"    evaluator_config={\\\"retriever_k\\\": 5}\\n\",\n    \"  )\\n\",\n    \"```\\n",
        "id": "6761d5d3877fcda535f83cf22e6801fc"
    },
    {
        "text": "config` parameter\\n\",\n    \"in the `mlflow.evaluate()` API as follows: `evaluator_config={\\\"retriever_k\\\": }`.\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"```python\\n\",\n    \"# Case 1: Specifying the model type\\n\",\n    \"evaluate_results = mlflow.evaluate(\\n\",\n    \"    data=data,\\n\",\n    \"    model_type=\\\"retriever\\\",\\n\",\n    \"    targets=\\\"ground_truth_context\\\",\\n\",\n    \"    predictions=\\\"retrieved_context\\\",\\n\",\n    \"    evaluators=\\\"default\\\",\\n\",\n    \"    evaluator_config={\\\"retriever_k\\\": 5}\\n\",\n    \"  )\\n\",\n    \"```\\n\",\n    \"\\n\",\n    \"Alternatively, you can directly specify the desired metrics\\n\",\n    \"in the `extra_metrics` parameter of the `mlflow.evaluate()` API without specifying a model\\n\",\n    \"type. In this case, the `k` value specified in the `evaluator_config` parameter will be\\n\",\n    \"ignored.\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"```python\\n\",\n    \"# Case 2: Specifying the extra_metrics\\n\",\n    \"evaluate_results = mlflow.evaluate(\\n\",\n    \"    data=data,\\n\",\n    \"    targets=\\\"ground_truth_context\\\",\\n\",\n    \"    predictions=\\\"retrieved_context\\\",\\n\",\n    \"    extra_metrics=[\\n\",\n    \"      mlflow.metrics.precision_at_k(4),\\n\",\n    \"      mlflow.metrics.precision_at_k(5)\\n\",\n    \"    ],\\n\",\n    \"  )\\n\",\n    \"```\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {\n      \"byteLimit\": 2048000,\n      \"rowLimit\": 10000\n     },\n     \"inputWidgets\": {},\n     \"nuid\": \"4b7174aa-0aa2-497d-aaa5-842121fcf270\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"outputs\": [\n    {\n     \"name\": \"stderr\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"2023/11/22 14:40:22 WARNING mlflow.data.pandas_dataset: Failed to infer schema for Pandas dataset. Exception: Unable to map 'object' type to MLflow DataType. object can be mapped iff all values have identical data type which is one of (string, (bytes or byterray),  int, float).\\n\",\n      \"2023/11/22 14:40:22 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\\n\",\n      \"2023/11/22 14:40:22 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\\n\",\n      \"2023/11/22 14:40:22 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: precision_at_1\\n\",\n      \"2023/11/22 14:40:22 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: precision_at_2\\n\",\n      \"2023/11/22 14:40:22 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: precision_at_3\\n\",\n      \"2023/11/22 14:40:22 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: recall_at_1\\n\",\n      \"2023/11/22 14:40:22 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: recall_at_2\\n\",\n      \"2023/11/22 14:40:22 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: recall_at_3\\n\",\n      \"2023/11/22 14:40:22 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: ndcg_at_1\\n\",\n      \"2023/11/22 14:40:22 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: ndcg_at_2\\n\",\n      \"2023/11/22 14:40:22 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: ndcg_at_3\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"with mlflow.start_run() as run:\\n\",\n    \"    evaluate_results = mlflow.evaluate(\\n\",\n    \"        data=data,\\n\",\n    \"        targets=\\\"source\\\",\\n\",\n    \"        predictions=\\\"retrieved_doc_ids\\\",\\n\",\n    \"        evaluators=\\\"default\\\",\\n\",\n    \"        extra_metrics=[\\n\",\n    \"            mlflow.metrics.precision_at_k(1),\\n\",\n    \"            mlflow.metrics.precision_at_k(2),\\n\",\n    \"            mlflow.metrics.precision_at_k(3),\\n\",\n    \"            mlflow.metrics.recall_at_k(1),\\n\",\n    \"            mlflow.metrics.recall_at_k(2),\\n\",\n    \"            mlflow.metrics.recall_at_k(3),\\n\",\n    \"            mlflow.metrics.ndcg_at_k(1),\\n\",\n    \"            mlflow.metrics.ndcg_at_k(2),\\n\",\n    \"            mlflow.metrics.ndcg_at_k(3),\\n\",\n    \"        ],\\n\",\n    \"    )\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"appli",
        "id": "bba22bd6b5061e55dd374fbf60143086"
    },
    {
        "text": "cs.precision_at_k(2),\\n\",\n    \"            mlflow.metrics.precision_at_k(3),\\n\",\n    \"            mlflow.metrics.recall_at_k(1),\\n\",\n    \"            mlflow.metrics.recall_at_k(2),\\n\",\n    \"            mlflow.metrics.recall_at_k(3),\\n\",\n    \"            mlflow.metrics.ndcg_at_k(1),\\n\",\n    \"            mlflow.metrics.ndcg_at_k(2),\\n\",\n    \"            mlflow.metrics.ndcg_at_k(3),\\n\",\n    \"        ],\\n\",\n    \"    )\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {\n      \"byteLimit\": 2048000,\n      \"rowLimit\": 10000\n     },\n     \"inputWidgets\": {},\n     \"nuid\": \"d57c201b-3718-43af-b8c2-ef22bfa2c15b\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"outputs\": [\n    {\n     \"data\": {\n      \"image/png\": \"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",
        "id": "187d509b1a28aa9bc206a9e04330f5bc"
    },
    {
        "text": "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",
        "id": "9657e840494e89339f20ffd88eacdde9"
    },
    {
        "text": "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",
        "id": "8824ddbefa36452f916c167faf2817d6"
    },
    {
        "text": "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",
        "id": "ee3729af6e1625c1a5ab7678744d7af5"
    },
    {
        "text": "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",
        "id": "06a9ee663f73fd3ac96c31ac826f778b"
    },
    {
        "text": "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",
        "id": "159c8e99c3e38167fb5d7c34854d8cbe"
    },
    {
        "text": "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",
        "id": "8172597f3e50795a742d10ec415e9099"
    },
    {
        "text": "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",
        "id": "4d9de8c59d3a35e48f573a5540a2fab6"
    },
    {
        "text": "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",
        "id": "d5e530e908637eeb92d765f24a61b5cb"
    },
    {
        "text": "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",
        "id": "585887167db913da0042a06a830b9285"
    },
    {
        "text": "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",
        "id": "66928f6531c54aaf4cc094f50fc8b5c9"
    },
    {
        "text": "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\",\n      \"text/plain\": [\n       \" \"\n      ]\n     },\n     \"metadata\": {},\n     \"output_type\": \"display_data\"\n    }\n   ],\n   \"source\": [\n    \"import matplotlib.pyplot as plt\\n\",\n    \"\\n\",\n    \"# Plotting each metric\\n\",\n    \"for metric_name in [\\\"precision\\\", \\\"recall\\\", \\\"ndcg\\\"]:\\n\",\n    \"    y = [evaluate_results.metrics[f\\\"{metric_name}_at_{k}/mean\\\"] for k in range(1, 4)]\\n\",\n    \"    plt.plot([1, 2, 3], y, label=f\\\"{metric_name}@k\\\")\\n\",\n    \"\\n\",\n    \"# Adding labels and title\\n\",\n    \"plt.xlabel(\\\"k\\\")\\n\",\n    \"plt.ylabel(\\\"Metric Value\\\")\\n\",\n    \"plt.title(\\\"Metrics Comparison at Different Ks\\\")\\n\",\n    \"# Setting x-axis ticks\\n\",\n    \"plt.xticks([1, 2, 3])\\n\",\n    \"plt.legend()\\n\",\n    \"\\n\",\n    \"# Display the plot\\n\",\n    \"plt.show()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {},\n     \"inputWidgets\": {},\n     \"nuid\": \"cac23d4b-bece-4274-836f-9ca2b7c3860d\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"source\": [\n    \"### Corner case handling\\n\",\n    \"\\n\",\n    \"There are a few corner cases handle specially for each built-in metric.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {},\n     \"inputWidgets\": {},\n     \"nuid\": \"e05a4ede-db44-46d2-bce8-752b0ce5d807\",\n     \"showTitle\"",
        "id": "94777ca0f8d5570c3d738eb90e516071"
    },
    {
        "text": " \"cellMetadata\": {},\n     \"inputWidgets\": {},\n     \"nuid\": \"cac23d4b-bece-4274-836f-9ca2b7c3860d\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"source\": [\n    \"### Corner case handling\\n\",\n    \"\\n\",\n    \"There are a few corner cases handle specially for each built-in metric.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {},\n     \"inputWidgets\": {},\n     \"nuid\": \"e05a4ede-db44-46d2-bce8-752b0ce5d807\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"source\": [\n    \"#### Empty retrieved document IDs\\n\",\n    \"\\n\",\n    \"When no relevant docs are retrieved:\\n\",\n    \"\\n\",\n    \"- `mlflow.metrics.precision_at_k(k)` is defined as:\\n\",\n    \"  * 0 if the ground-truth doc IDs is non-empty\\n\",\n    \"  * 1 if the ground-truth doc IDs is also empty\\n\",\n    \"\\n\",\n    \"- `mlflow.metrics.ndcg_at_k(k)` is defined as:\\n\",\n    \"  * 0 if the ground-truth doc IDs is non-empty\\n\",\n    \"  * 1 if the ground-truth doc IDs is also empty\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {},\n     \"inputWidgets\": {},\n     \"nuid\": \"931a32e7-29cb-4a22-b94e-ea2bf4f0b1a7\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"source\": [\n    \"#### Empty ground-truth document IDs\\n\",\n    \"\\n\",\n    \"When no ground-truth document IDs are provided:\\n\",\n    \"\\n\",\n    \"- `mlflow.metrics.recall_at_k(k)` is defined as:\\n\",\n    \"  * 0 if the retrieved doc IDs is non-empty\\n\",\n    \"  * 1 if the retrieved doc IDs is also empty\\n\",\n    \"\\n\",\n    \"- `mlflow.metrics.ndcg_at_k(k)` is defined as:\\n\",\n    \"  * 0 if the retrieved doc IDs is non-empty\\n\",\n    \"  * 1 if the retrieved doc IDs is also empty\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {},\n     \"inputWidgets\": {},\n     \"nuid\": \"5a1453f6-a62d-43da-b230-955841c66651\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"source\": [\n    \"#### Duplicate retreived document IDs\\n\",\n    \"\\n\",\n    \"It is a common case for the retriever in a RAG system to retrieve multiple chunks in the same document for a given query. In this case, `mlflow.metrics.ndcg_at_k(k)` is calculated as follows:\\n\",\n    \"\\n\",\n    \"If the duplicate doc IDs are in the ground truth,\\n\",\n    \"       they will be treated as different docs. For example, if the ground truth doc IDs are\\n\",\n    \"       [1, 2] and the retrieved doc IDs are [1, 1, 1, 3], the score will be equavalent to\\n\",\n    \"       ground truth doc IDs [10, 11, 12, 2] and retrieved doc IDs [10, 11, 12, 3].\\n\",\n    \"\\n\",\n    \"If the duplicate doc IDs are not in the ground truth, the ndcg score is calculated as normal.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {},\n     \"inputWidgets\": {},\n     \"nuid\": \"525ccc10-3a60-4dc9-804e-083cfa313349\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"source\": [\n    \"## Step 4: Result Analysis and Visualization\\n\",\n    \"\\n\",\n    \"You can view the per-row scores in the logged \\\"eval_results_table.json\\\" in artifacts by either loading it to a pandas dataframe (shown below) or visiting the MLflow run comparison UI.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {\n      \"byteLimit\": 2048000,\n      \"rowLimit\": 10000\n     },\n     \"inputWidgets\": {},\n     \"nuid\": \"32f3d5b3-245c-46b7-87ce-d85e261eac28\",\n     \"showTitle\": true,\n     \"title\": \"\"\n    }\n   },\n   \"outputs\": [\n    {\n     \"data\": {\n      \"application/vnd.jupyter.widget-view+json\": {\n       \"model_id\": \"ee4bfb1998174c558e537ebb1dd737d9\",\n       \"version_major\": 2,\n       \"version_minor\": 0\n      },\n      \"text/plain\": [\n       \"Downloading artifacts:   0%|          | 0/1 [00:00 \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" question \\n\",\n       \" source \\n\",\n       \" retrieved_doc_ids \\n\",\n       \" precision_at_1/s",
        "id": "0be98a6fbfb92f0fea24a8b2776a9a3a"
    },
    {
        "text": " \"showTitle\": true,\n     \"title\": \"\"\n    }\n   },\n   \"outputs\": [\n    {\n     \"data\": {\n      \"application/vnd.jupyter.widget-view+json\": {\n       \"model_id\": \"ee4bfb1998174c558e537ebb1dd737d9\",\n       \"version_major\": 2,\n       \"version_minor\": 0\n      },\n      \"text/plain\": [\n       \"Downloading artifacts:   0%|          | 0/1 [00:00 \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" question \\n\",\n       \" source \\n\",\n       \" retrieved_doc_ids \\n\",\n       \" precision_at_1/score \\n\",\n       \" precision_at_2/score \\n\",\n       \" precision_at_3/score \\n\",\n       \" recall_at_1/score \\n\",\n       \" recall_at_2/score \\n\",\n       \" recall_at_3/score \\n\",\n       \" ndcg_at_1/score \\n\",\n       \" ndcg_at_2/score \\n\",\n       \" ndcg_at_3/score \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" 0 \\n\",\n       \" What is the purpose of the MLflow Model Registry? \\n\",\n       \" [model-registry.html] \\n\",\n       \" [model-registry.html, introduction/index.html,... \\n\",\n       \" 1 \\n\",\n       \" 0.5 \\n\",\n       \" 0.333333 \\n\",\n       \" 1 \\n\",\n       \" 1 \\n\",\n       \" 1 \\n\",\n       \" 1 \\n\",\n       \" 1.0 \\n\",\n       \" 0.919721 \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" 1 \\n\",\n       \" What is the purpose of registering a model wit... \\n\",\n       \" [model-registry.html] \\n\",\n       \" [model-registry.html, models.html, introductio... \\n\",\n       \" 1 \\n\",\n       \" 0.5 \\n\",\n       \" 0.333333 \\n\",\n       \" 1 \\n\",\n       \" 1 \\n\",\n       \" 1 \\n\",\n       \" 1 \\n\",\n       \" 1.0 \\n\",\n       \" 1.000000 \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" 2 \\n\",\n       \" What can you do with registered models and mod... \\n\",\n       \" [model-registry.html] \\n\",\n       \" [model-registry.html, models.html, deployment/... \\n\",\n       \" 1 \\n\",\n       \" 0.5 \\n\",\n       \" 0.333333 \\n\",\n       \" 1 \\n\",\n       \" 1 \\n\",\n       \" 1 \\n\",\n       \" 1 \\n\",\n       \" 1.0 \\n\",\n       \" 1.000000 \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" 3 \\n\",\n       \" How can you add, modify, update, or delete a m... \\n\",\n       \" [model-registry.html] \\n\",\n       \" [model-registry.html, models.html, deployment/... \\n\",\n       \" 1 \\n\",\n       \" 0.5 \\n\",\n       \" 0.333333 \\n\",\n       \" 1 \\n\",\n       \" 1 \\n\",\n       \" 1 \\n\",\n       \" 1 \\n\",\n       \" 1.0 \\n\",\n       \" 1.000000 \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" 4 \\n\",\n       \" How can you deploy and organize models in the ... \\n\",\n       \" [model-registry.html] \\n\",\n       \" [model-registry.html, deployment/index.html, d... \\n\",\n       \" 1 \\n\",\n       \" 0.5 \\n\",\n       \" 0.333333 \\n\",\n       \" 1 \\n\",\n       \" 1 \\n\",\n       \" 1 \\n\",\n       \" 1 \\n\",\n       \" 1.0 \\n\",\n       \" 0.919721 \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \\n\",\n       \" \"\n      ],\n      \"text/plain\": [\n       \"                                            question                 source  \\\\\\n\",\n       \"0  What is the purpose of the MLflow Model Registry?  [model-registry.html]   \\n\",\n       \"1  What is the purpose of registering a model wit...  [model-registry.html]   \\n\",\n       \"2  What can you do with registered models and mod...  [model-registry.html]   \\n\",\n       \"3  How can you add, modify, update, or delete a m...  [model-registry.html]   \\n\",\n       \"4  How can you deploy and organize models in the ...  [model-registry.html]   \\n\",\n       \"\\n\",\n       \"                                   retrieved_doc_ids  precision_at_1/score  \\\\\\n\",\n       \"0  [model-registry.html, introduction/index.html,...                     1   \\n\",\n       \"1  [model-registry.html, models.html, introductio...                     1   \\n\",\n       \"2  [model-registry.html, models.html, deployment/...                     1   \\n\",\n       \"3  [model-registry.html, models.html, deployment/...                     1   \\n\",\n       \"4  [model-registry.html, deployment/index.html, d...                     1   \\n\",\n       \"\\n\",\n       \"   precision_at_2/score  precision_at_3/score  recall_at_1/score  \\\\\\n\",\n       \"0                   0.5              0.333333                  1   \\n\",\n       \"1                   0.5              0.333333                  1   \\n\",\n ",
        "id": "70d68ae4f71df6e56c4d10674d143f50"
    },
    {
        "text": "odel-registry.html, models.html, deployment/...                     1   \\n\",\n       \"3  [model-registry.html, models.html, deployment/...                     1   \\n\",\n       \"4  [model-registry.html, deployment/index.html, d...                     1   \\n\",\n       \"\\n\",\n       \"   precision_at_2/score  precision_at_3/score  recall_at_1/score  \\\\\\n\",\n       \"0                   0.5              0.333333                  1   \\n\",\n       \"1                   0.5              0.333333                  1   \\n\",\n       \"2                   0.5              0.333333                  1   \\n\",\n       \"3                   0.5              0.333333                  1   \\n\",\n       \"4                   0.5              0.333333                  1   \\n\",\n       \"\\n\",\n       \"   recall_at_2/score  recall_at_3/score  ndcg_at_1/score  ndcg_at_2/score  \\\\\\n\",\n       \"0                  1                  1                1              1.0   \\n\",\n       \"1                  1                  1                1              1.0   \\n\",\n       \"2                  1                  1                1              1.0   \\n\",\n       \"3                  1                  1                1              1.0   \\n\",\n       \"4                  1                  1                1              1.0   \\n\",\n       \"\\n\",\n       \"   ndcg_at_3/score  \\n\",\n       \"0         0.919721  \\n\",\n       \"1         1.000000  \\n\",\n       \"2         1.000000  \\n\",\n       \"3         1.000000  \\n\",\n       \"4         0.919721  \"\n      ]\n     },\n     \"metadata\": {},\n     \"output_type\": \"display_data\"\n    }\n   ],\n   \"source\": [\n    \"eval_results_table = evaluate_results.tables[\\\"eval_results_table\\\"]\\n\",\n    \"eval_results_table.head(5)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {},\n     \"inputWidgets\": {},\n     \"nuid\": \"cf18dd29-1017-4245-9f3b-923dbd46f742\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"source\": [\n    \"With the evaluate results table, you can further visualize the well-answered questions and poorly-answered questions using topical analysis techniques.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {\n      \"byteLimit\": 2048000,\n      \"rowLimit\": 10000\n     },\n     \"inputWidgets\": {},\n     \"nuid\": \"b1d9e40a-ccf6-4d6a-b24c-8cf41bbfa005\",\n     \"showTitle\": true,\n     \"title\": \"Utilitity functions\"\n    }\n   },\n   \"outputs\": [\n    {\n     \"name\": \"stderr\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"[nltk_data] Downloading package punkt to\\n\",\n      \"[nltk_data]     /Users/liang.zhang/nltk_data...\\n\",\n      \"[nltk_data]   Package punkt is already up-to-date!\\n\",\n      \"[nltk_data] Downloading package stopwords to\\n\",\n      \"[nltk_data]     /Users/liang.zhang/nltk_data...\\n\",\n      \"[nltk_data]   Package stopwords is already up-to-date!\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"import nltk\\n\",\n    \"import pyLDAvis.gensim_models as gensimvis\\n\",\n    \"from gensim import corpora, models\\n\",\n    \"from nltk.corpus import stopwords\\n\",\n    \"from nltk.tokenize import word_tokenize\\n\",\n    \"\\n\",\n    \"# Initialize NLTK resources\\n\",\n    \"nltk.download(\\\"punkt\\\")\\n\",\n    \"nltk.download(\\\"stopwords\\\")\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"def topical_analysis(questions: list[str]):\\n\",\n    \"    stop_words = set(stopwords.words(\\\"english\\\"))\\n\",\n    \"\\n\",\n    \"    # Tokenize and remove stop words\\n\",\n    \"    tokenized_data = []\\n\",\n    \"    for question in questions:\\n\",\n    \"        tokens = word_tokenize(question.lower())\\n\",\n    \"        filtered_tokens = [word for word in tokens if word not in stop_words and word.isalpha()]\\n\",\n    \"        tokenized_data.append(filtered_tokens)\\n\",\n    \"\\n\",\n    \"    # Create a dictionary and corpus\\n\",\n    \"    dictionary = corpora.Dictionary(tokenized_data)\\n\",\n    \"    corpus = [dictionary.doc2bow(text) for text in tokenized_data]\\n\",\n    \"\\n\",\n    \"    # Apply LDA model\\n\",\n    \"    lda_model = models.LdaModel(corpus, num_topics=5, id2wor",
        "id": "311937cd23c32590770ecd8c74ee3794"
    },
    {
        "text": "      tokens = word_tokenize(question.lower())\\n\",\n    \"        filtered_tokens = [word for word in tokens if word not in stop_words and word.isalpha()]\\n\",\n    \"        tokenized_data.append(filtered_tokens)\\n\",\n    \"\\n\",\n    \"    # Create a dictionary and corpus\\n\",\n    \"    dictionary = corpora.Dictionary(tokenized_data)\\n\",\n    \"    corpus = [dictionary.doc2bow(text) for text in tokenized_data]\\n\",\n    \"\\n\",\n    \"    # Apply LDA model\\n\",\n    \"    lda_model = models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\\n\",\n    \"\\n\",\n    \"    # Get topic distribution for each question\\n\",\n    \"    topic_distribution = []\\n\",\n    \"    for i, ques in enumerate(questions):\\n\",\n    \"        bow = dictionary.doc2bow(tokenized_data[i])\\n\",\n    \"        topics = lda_model.get_document_topics(bow)\\n\",\n    \"        topic_distribution.append(topics)\\n\",\n    \"        print(f\\\"Question: {ques}\\\\nTopic: {topics}\\\")\\n\",\n    \"\\n\",\n    \"    # Print all topics\\n\",\n    \"    print(\\\"\\\\nTopics found are:\\\")\\n\",\n    \"    for idx, topic in lda_model.print_topics(-1):\\n\",\n    \"        print(f\\\"Topic: {idx} \\\\nWords: {topic}\\\\n\\\")\\n\",\n    \"    return lda_model, corpus, dictionary\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {\n      \"byteLimit\": 2048000,\n      \"rowLimit\": 10000\n     },\n     \"inputWidgets\": {},\n     \"nuid\": \"e892d804-a4d8-468c-93e2-acc4a5fbcf2c\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"filtered_df = eval_results_table[eval_results_table[\\\"precision_at_1/score\\\"] == 1]\\n\",\n    \"hit_questions = filtered_df[\\\"question\\\"].tolist()\\n\",\n    \"filtered_df = eval_results_table[eval_results_table[\\\"precision_at_1/score\\\"] == 0]\\n\",\n    \"miss_questions = filtered_df[\\\"question\\\"].tolist()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {\n      \"byteLimit\": 2048000,\n      \"rowLimit\": 10000\n     },\n     \"inputWidgets\": {},\n     \"nuid\": \"7c178b69-37d4-4a6b-9737-b93e7f3d75c5\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"Question: What is the purpose of the MLflow Model Registry?\\n\",\n      \"Topic: [(0, 0.0400703), (1, 0.040002838), (2, 0.040673085), (3, 0.04075462), (4, 0.8384991)]\\n\",\n      \"Question: What is the purpose of registering a model with the Model Registry?\\n\",\n      \"Topic: [(0, 0.0334267), (1, 0.033337697), (2, 0.033401005), (3, 0.033786207), (4, 0.8660484)]\\n\",\n      \"Question: What can you do with registered models and model versions?\\n\",\n      \"Topic: [(0, 0.04019648), (1, 0.04000775), (2, 0.040166058), (3, 0.8391777), (4, 0.040452003)]\\n\",\n      \"Question: How can you add, modify, update, or delete a model in the Model Registry?\\n\",\n      \"Topic: [(0, 0.025052568), (1, 0.025006149), (2, 0.025024023), (3, 0.025236268), (4, 0.899681)]\\n\",\n      \"Question: How can you deploy and organize models in the Model Registry?\\n\",\n      \"Topic: [(0, 0.033460867), (1, 0.033337582), (2, 0.033362914), (3, 0.8659808), (4, 0.033857808)]\\n\",\n      \"Question: What method do you use to create a new registered model?\\n\",\n      \"Topic: [(0, 0.028867528), (1, 0.028582651), (2, 0.882546), (3, 0.030021703), (4, 0.029982116)]\\n\",\n      \"Question: How can you deploy and organize models in the Model Registry?\\n\",\n      \"Topic: [(0, 0.033460878), (1, 0.033337586), (2, 0.033362918), (3, 0.8659798), (4, 0.03385884)]\\n\",\n      \"Question: How can you fetch a list of registered models in the MLflow registry?\\n\",\n      \"Topic: [(0, 0.0286206), (1, 0.028577656), (2, 0.02894385), (3, 0.88495284), (4, 0.028905064)]\\n\",\n      \"Question: What is the default channel logged for models using MLflow v1.18 and above?\\n\",\n      \"Topic: [(0, 0.02862059), (1, 0.028577654), (2, 0.028883327), (3, 0.8851736), (4, 0.028744776)]\\n\",\n      \"Question: What information is stored in the conda.yaml file?\\n\",\n      \"To",
        "id": "4e4f26f48b56128fc9ea3334a4c37033"
    },
    {
        "text": ".8659798), (4, 0.03385884)]\\n\",\n      \"Question: How can you fetch a list of registered models in the MLflow registry?\\n\",\n      \"Topic: [(0, 0.0286206), (1, 0.028577656), (2, 0.02894385), (3, 0.88495284), (4, 0.028905064)]\\n\",\n      \"Question: What is the default channel logged for models using MLflow v1.18 and above?\\n\",\n      \"Topic: [(0, 0.02862059), (1, 0.028577654), (2, 0.028883327), (3, 0.8851736), (4, 0.028744776)]\\n\",\n      \"Question: What information is stored in the conda.yaml file?\\n\",\n      \"Topic: [(0, 0.050020963), (1, 0.051287953), (2, 0.051250603), (3, 0.7968765), (4, 0.05056402)]\\n\",\n      \"Question: How can you save a model with a manually specified conda environment?\\n\",\n      \"Topic: [(0, 0.02862434), (1, 0.02858204), (2, 0.02886313), (3, 0.8851747), (4, 0.028755778)]\\n\",\n      \"Question: What are inference params and how are they used during model inference?\\n\",\n      \"Topic: [(0, 0.86457103), (1, 0.03353862), (2, 0.033417325), (3, 0.034004394), (4, 0.034468662)]\\n\",\n      \"Question: What is the purpose of model signatures in MLflow?\\n\",\n      \"Topic: [(0, 0.040070876), (1, 0.04000346), (2, 0.040688124), (3, 0.040469088), (4, 0.8387685)]\\n\",\n      \"Question: What is the API used to set signatures on models?\\n\",\n      \"Topic: [(0, 0.033873636), (1, 0.033508822), (2, 0.033337757), (3, 0.035357967), (4, 0.8639218)]\\n\",\n      \"Question: What components are used to generate the final time series?\\n\",\n      \"Topic: [(0, 0.028693806), (1, 0.8853218), (2, 0.028573763), (3, 0.02862714), (4, 0.0287835)]\\n\",\n      \"Question: What functionality does the configuration DataFrame submitted to the pyfunc flavor provide?\\n\",\n      \"Topic: [(0, 0.02519801), (1, 0.025009492), (2, 0.025004204), (3, 0.025004204), (4, 0.8997841)]\\n\",\n      \"Question: What is a common configuration for lowering the total memory pressure for pytorch models within transformers pipelines?\\n\",\n      \"Topic: [(0, 0.93316424), (1, 0.016669936), (2, 0.016668117), (3, 0.016788227), (4, 0.016709473)]\\n\",\n      \"Question: What does the save_model() function do?\\n\",\n      \"Topic: [(0, 0.10002145), (1, 0.59994656), (2, 0.10001026), (3, 0.10001026), (4, 0.10001151)]\\n\",\n      \"Question: What is an MLflow Project?\\n\",\n      \"Topic: [(0, 0.06667001), (1, 0.06667029), (2, 0.7321751), (3, 0.06711196), (4, 0.06737265)]\\n\",\n      \"Question: What are the entry points in a MLproject file and how can you specify parameters for them?\\n\",\n      \"Topic: [(0, 0.02857626), (1, 0.88541776), (2, 0.02868285), (3, 0.028626908), (4, 0.02869626)]\\n\",\n      \"Question: What are the project environments supported by MLflow?\\n\",\n      \"Topic: [(0, 0.040009078), (1, 0.040009864), (2, 0.839655), (3, 0.040126894), (4, 0.040199146)]\\n\",\n      \"Question: What is the purpose of specifying a Conda environment in an MLflow project?\\n\",\n      \"Topic: [(0, 0.028579442), (1, 0.028580135), (2, 0.8841217), (3, 0.028901232), (4, 0.029817443)]\\n\",\n      \"Question: What is the purpose of the MLproject file?\\n\",\n      \"Topic: [(0, 0.05001335), (1, 0.052611485), (2, 0.050071735), (3, 0.05043289), (4, 0.7968705)]\\n\",\n      \"Question: How can you pass runtime parameters to the entry point of an MLflow Project?\\n\",\n      \"Topic: [(0, 0.025007373), (1, 0.025498485), (2, 0.8993807), (3, 0.02504522), (4, 0.025068246)]\\n\",\n      \"Question: How does MLflow run a Project on Kubernetes?\\n\",\n      \"Topic: [(0, 0.04000677), (1, 0.040007353), (2, 0.83931196), (3, 0.04012452), (4, 0.04054937)]\\n\",\n      \"Question: What fields are replaced when MLflow creates a Kubernetes Job for an MLflow Project?\\n\",\n      \"Topic: [(0, 0.022228329), (1, 0.022228856), (2, 0.023192631), (3, 0.02235802), (4, 0.90999216)]\\n\",\n      \"Question: What is the syntax for searching runs using the MLflow UI and API?\\n\",\n      \"Topic: [(0, 0.025003674), (1, 0.02500399), (2, 0.02527212), (3, 0.89956146), (4, 0.025158761)]\\n\",\n      \"Question: What is the syntax for searching runs using the MLflow UI and API?\\n\",\n      \"Topic: [(0, 0.025003672), (1, 0.025003988), (2, 0.025272164), (3, 0.8995614), (4, 0.025158769)]\\n\",\n      \"Question: W",
        "id": "460ac78d86dd83c219286b679442f8f0"
    },
    {
        "text": "    \"Topic: [(0, 0.022228329), (1, 0.022228856), (2, 0.023192631), (3, 0.02235802), (4, 0.90999216)]\\n\",\n      \"Question: What is the syntax for searching runs using the MLflow UI and API?\\n\",\n      \"Topic: [(0, 0.025003674), (1, 0.02500399), (2, 0.02527212), (3, 0.89956146), (4, 0.025158761)]\\n\",\n      \"Question: What is the syntax for searching runs using the MLflow UI and API?\\n\",\n      \"Topic: [(0, 0.025003672), (1, 0.025003988), (2, 0.025272164), (3, 0.8995614), (4, 0.025158769)]\\n\",\n      \"Question: What are the key parts of a search expression in MLflow?\\n\",\n      \"Topic: [(0, 0.03334423), (1, 0.03334517), (2, 0.8662702), (3, 0.033611353), (4, 0.033429127)]\\n\",\n      \"Question: What are the key attributes for the model with the run_id 'a1b2c3d4' and run_name 'my-run'?\\n\",\n      \"Topic: [(0, 0.05017508), (1, 0.05001634), (2, 0.05058142), (3, 0.7985237), (4, 0.050703418)]\\n\",\n      \"Question: What information does each run record in MLflow Tracking?\\n\",\n      \"Topic: [(0, 0.03333968), (1, 0.033340227), (2, 0.86639804), (3, 0.03349555), (4, 0.033426523)]\\n\",\n      \"Question: What are the two components used by MLflow for storage?\\n\",\n      \"Topic: [(0, 0.0334928), (1, 0.033938777), (2, 0.033719826), (3, 0.03357158), (4, 0.86527705)]\\n\",\n      \"Question: What interfaces does the MLflow client use to record MLflow entities and artifacts when running MLflow on a local machine with a SQLAlchemy-compatible database?\\n\",\n      \"Topic: [(0, 0.014289577), (1, 0.014289909), (2, 0.94276434), (3, 0.014325481), (4, 0.014330726)]\\n\",\n      \"Question: What is the default backend store used by MLflow?\\n\",\n      \"Topic: [(0, 0.033753525), (1, 0.03379533), (2, 0.033777602), (3, 0.86454684), (4, 0.0341267)]\\n\",\n      \"Question: What information does autologging capture when launching short-lived MLflow runs?\\n\",\n      \"Topic: [(0, 0.028579954), (1, 0.02858069), (2, 0.8851724), (3, 0.029027484), (4, 0.028639426)]\\n\",\n      \"Question: What is the purpose of the --serve-artifacts flag?\\n\",\n      \"Topic: [(0, 0.06670548), (1, 0.066708855), (2, 0.067003354), (3, 0.3969311), (4, 0.40265122)]\\n\",\n      \"\\n\",\n      \"Topics found are:\\n\",\n      \"Topic: 0 \\n\",\n      \"Words: 0.059*\\\"inference\\\" + 0.032*\\\"models\\\" + 0.032*\\\"used\\\" + 0.032*\\\"configuration\\\" + 0.032*\\\"common\\\" + 0.032*\\\"transformers\\\" + 0.032*\\\"total\\\" + 0.032*\\\"within\\\" + 0.032*\\\"pytorch\\\" + 0.032*\\\"pipelines\\\"\\n\",\n      \"\\n\",\n      \"Topic: 1 \\n\",\n      \"Words: 0.036*\\\"file\\\" + 0.035*\\\"mlproject\\\" + 0.035*\\\"used\\\" + 0.035*\\\"components\\\" + 0.035*\\\"entry\\\" + 0.035*\\\"parameters\\\" + 0.035*\\\"specify\\\" + 0.035*\\\"final\\\" + 0.035*\\\"points\\\" + 0.035*\\\"time\\\"\\n\",\n      \"\\n\",\n      \"Topic: 2 \\n\",\n      \"Words: 0.142*\\\"mlflow\\\" + 0.066*\\\"project\\\" + 0.028*\\\"information\\\" + 0.028*\\\"use\\\" + 0.028*\\\"record\\\" + 0.028*\\\"run\\\" + 0.015*\\\"key\\\" + 0.015*\\\"running\\\" + 0.015*\\\"artifacts\\\" + 0.015*\\\"client\\\"\\n\",\n      \"\\n\",\n      \"Topic: 3 \\n\",\n      \"Words: 0.066*\\\"models\\\" + 0.066*\\\"model\\\" + 0.066*\\\"mlflow\\\" + 0.041*\\\"using\\\" + 0.041*\\\"registry\\\" + 0.028*\\\"api\\\" + 0.028*\\\"registered\\\" + 0.028*\\\"runs\\\" + 0.028*\\\"syntax\\\" + 0.028*\\\"searching\\\"\\n\",\n      \"\\n\",\n      \"Topic: 4 \\n\",\n      \"Words: 0.089*\\\"model\\\" + 0.074*\\\"purpose\\\" + 0.074*\\\"mlflow\\\" + 0.046*\\\"registry\\\" + 0.031*\\\"used\\\" + 0.031*\\\"signatures\\\" + 0.017*\\\"kubernetes\\\" + 0.017*\\\"fields\\\" + 0.017*\\\"job\\\" + 0.017*\\\"replaced\\\"\\n\",\n      \"\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"lda_model, corpus, dictionary = topical_analysis(hit_questions)\\n\",\n    \"vis_data = gensimvis.prepare(lda_model, corpus, dictionary)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 3,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {},\n     \"inputWidgets\": {},\n     \"nuid\": \"a0587a0f-b35d-488d-9054-55435a9585bf\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Uncomment the following line to render the interactive widget\\n\",\n    \"# pyLDAvis.display(vis_data)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell",
        "id": "c5f34006c46c5bc3c1593e9f00ac0bec"
    },
    {
        "text": "ode\",\n   \"execution_count\": 3,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {},\n     \"inputWidgets\": {},\n     \"nuid\": \"a0587a0f-b35d-488d-9054-55435a9585bf\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Uncomment the following line to render the interactive widget\\n\",\n    \"# pyLDAvis.display(vis_data)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {\n      \"byteLimit\": 2048000,\n      \"rowLimit\": 10000\n     },\n     \"inputWidgets\": {},\n     \"nuid\": \"1375250d-9818-4503-87ec-f14020d87c81\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"Question: What is the purpose of the mlflow.sklearn.log_model() method?\\n\",\n      \"Topic: [(0, 0.0669118), (1, 0.06701085), (2, 0.06667974), (3, 0.73235476), (4, 0.06704286)]\\n\",\n      \"Question: How can you fetch a specific model version?\\n\",\n      \"Topic: [(0, 0.83980393), (1, 0.040003464), (2, 0.04000601), (3, 0.040101767), (4, 0.040084846)]\\n\",\n      \"Question: How can you fetch the latest model version in a specific stage?\\n\",\n      \"Topic: [(0, 0.88561153), (1, 0.028575428), (2, 0.028578365), (3, 0.0286214), (4, 0.028613236)]\\n\",\n      \"Question: What can you do to promote MLflow Models across environments?\\n\",\n      \"Topic: [(0, 0.8661927), (1, 0.0333396), (2, 0.03362743), (3, 0.033428304), (4, 0.033411972)]\\n\",\n      \"Question: What is the name of the model and its version details?\\n\",\n      \"Topic: [(0, 0.83978903), (1, 0.04000637), (2, 0.04001106), (3, 0.040105395), (4, 0.040088095)]\\n\",\n      \"Question: What is the purpose of saving the model in pickled format?\\n\",\n      \"Topic: [(0, 0.033948876), (1, 0.03339717), (2, 0.033340737), (3, 0.86575514), (4, 0.033558063)]\\n\",\n      \"Question: What is an MLflow Model and what is its purpose?\\n\",\n      \"Topic: [(0, 0.7940762), (1, 0.05068333), (2, 0.050770763), (3, 0.053328265), (4, 0.05114142)]\\n\",\n      \"Question: What are the flavors defined in the MLmodel file for the mlflow.sklearn library?\\n\",\n      \"Topic: [(0, 0.86628276), (1, 0.033341788), (2, 0.03334801), (3, 0.03368498), (4, 0.033342462)]\\n\",\n      \"Question: What command can be used to package and deploy models to AWS SageMaker?\\n\",\n      \"Topic: [(0, 0.89991224), (1, 0.025005225), (2, 0.025009066), (3, 0.025006713), (4, 0.025066752)]\\n\",\n      \"Question: What is the purpose of the --build-image flag when running mlflow run?\\n\",\n      \"Topic: [(0, 0.033957016), (1, 0.033506736), (2, 0.034095332), (3, 0.034164555), (4, 0.86427635)]\\n\",\n      \"Question: What is the relative path to the python_env YAML file within the MLflow project's directory?\\n\",\n      \"Topic: [(0, 0.02243), (1, 0.02222536), (2, 0.022470985), (3, 0.9105873), (4, 0.02228631)]\\n\",\n      \"Question: What are the additional local volume mounted and environment variables in the docker container?\\n\",\n      \"Topic: [(0, 0.022225259), (1, 0.9110914), (2, 0.02222932), (3, 0.022227468), (4, 0.022226628)]\\n\",\n      \"Question: What are some examples of entity names that contain special characters?\\n\",\n      \"Topic: [(0, 0.028575381), (1, 0.88568854), (2, 0.02858065), (3, 0.028578246), (4, 0.028577149)]\\n\",\n      \"Question: What type of constant does the RHS need to be if LHS is a metric?\\n\",\n      \"Topic: [(0, 0.028575381), (1, 0.8856886), (2, 0.028580645), (3, 0.028578239), (4, 0.028577147)]\\n\",\n      \"Question: How can you get all active runs from experiments IDs 3, 4, and 17 that used a CNN model with 10 layers and had a prediction accuracy of 94.5% or higher?\\n\",\n      \"Topic: [(0, 0.015563371), (1, 0.015387185), (2, 0.015389071), (3, 0.015427767), (4, 0.9382326)]\\n\",\n      \"Question: What is the purpose of the 'experimentIds' variable in the given paragraph?\\n\",\n      \"Topic: [(0, 0.040206533), (1, 0.8384999), (2, 0.040013183), (3, 0.040967643), (4, 0.040312726)]\\n\",\n      \"Question: What is the MLflow Tracking component used for?\\n\",",
        "id": "1f8a3271bc4930f25bfa4c947e13b50f"
    },
    {
        "text": "ve runs from experiments IDs 3, 4, and 17 that used a CNN model with 10 layers and had a prediction accuracy of 94.5% or higher?\\n\",\n      \"Topic: [(0, 0.015563371), (1, 0.015387185), (2, 0.015389071), (3, 0.015427767), (4, 0.9382326)]\\n\",\n      \"Question: What is the purpose of the 'experimentIds' variable in the given paragraph?\\n\",\n      \"Topic: [(0, 0.040206533), (1, 0.8384999), (2, 0.040013183), (3, 0.040967643), (4, 0.040312726)]\\n\",\n      \"Question: What is the MLflow Tracking component used for?\\n\",\n      \"Topic: [(0, 0.8390845), (1, 0.04000697), (2, 0.040462855), (3, 0.04014182), (4, 0.040303845)]\\n\",\n      \"Question: How can you create an experiment in MLflow?\\n\",\n      \"Topic: [(0, 0.050333958), (1, 0.0500024), (2, 0.7993825), (3, 0.050153885), (4, 0.05012722)]\\n\",\n      \"Question: How can you create an experiment using MLflow?\\n\",\n      \"Topic: [(0, 0.04019285), (1, 0.04000254), (2, 0.8396381), (3, 0.040091105), (4, 0.04007539)]\\n\",\n      \"Question: What is the architecture depicted in this example scenario?\\n\",\n      \"Topic: [(0, 0.04000523), (1, 0.040007014), (2, 0.040012203), (3, 0.04000902), (4, 0.83996654)]\\n\",\n      \"\\n\",\n      \"Topics found are:\\n\",\n      \"Topic: 0 \\n\",\n      \"Words: 0.078*\\\"model\\\" + 0.059*\\\"mlflow\\\" + 0.059*\\\"version\\\" + 0.041*\\\"models\\\" + 0.041*\\\"fetch\\\" + 0.041*\\\"specific\\\" + 0.041*\\\"used\\\" + 0.022*\\\"command\\\" + 0.022*\\\"deploy\\\" + 0.022*\\\"sagemaker\\\"\\n\",\n      \"\\n\",\n      \"Topic: 1 \\n\",\n      \"Words: 0.030*\\\"local\\\" + 0.030*\\\"container\\\" + 0.030*\\\"variables\\\" + 0.030*\\\"docker\\\" + 0.030*\\\"mounted\\\" + 0.030*\\\"environment\\\" + 0.030*\\\"volume\\\" + 0.030*\\\"additional\\\" + 0.030*\\\"special\\\" + 0.030*\\\"names\\\"\\n\",\n      \"\\n\",\n      \"Topic: 2 \\n\",\n      \"Words: 0.096*\\\"experiment\\\" + 0.096*\\\"create\\\" + 0.096*\\\"mlflow\\\" + 0.051*\\\"using\\\" + 0.009*\\\"purpose\\\" + 0.009*\\\"model\\\" + 0.009*\\\"method\\\" + 0.009*\\\"file\\\" + 0.009*\\\"version\\\" + 0.009*\\\"used\\\"\\n\",\n      \"\\n\",\n      \"Topic: 3 \\n\",\n      \"Words: 0.071*\\\"purpose\\\" + 0.039*\\\"file\\\" + 0.039*\\\"mlflow\\\" + 0.039*\\\"yaml\\\" + 0.039*\\\"directory\\\" + 0.039*\\\"relative\\\" + 0.039*\\\"within\\\" + 0.039*\\\"path\\\" + 0.039*\\\"project\\\" + 0.039*\\\"format\\\"\\n\",\n      \"\\n\",\n      \"Topic: 4 \\n\",\n      \"Words: 0.032*\\\"purpose\\\" + 0.032*\\\"used\\\" + 0.032*\\\"model\\\" + 0.032*\\\"prediction\\\" + 0.032*\\\"get\\\" + 0.032*\\\"accuracy\\\" + 0.032*\\\"active\\\" + 0.032*\\\"layers\\\" + 0.032*\\\"higher\\\" + 0.032*\\\"experiments\\\"\\n\",\n      \"\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"lda_model, corpus, dictionary = topical_analysis(miss_questions)\\n\",\n    \"vis_data = gensimvis.prepare(lda_model, corpus, dictionary)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 4,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {},\n     \"inputWidgets\": {},\n     \"nuid\": \"724db985-5382-43a6-ada5-0ac1c2d49c18\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# Uncomment the following line to render the interactive widget\\n\",\n    \"# pyLDAvis.display(vis_data)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"application/vnd.databricks.v1+cell\": {\n     \"cellMetadata\": {},\n     \"inputWidgets\": {},\n     \"nuid\": \"31945151-7cf9-4f25-af30-d9b9bd526e7b\",\n     \"showTitle\": false,\n     \"title\": \"\"\n    }\n   },\n   \"outputs\": [],\n   \"source\": []\n  }\n ],\n \"metadata\": {\n  \"application/vnd.databricks.v1+notebook\": {\n   \"dashboards\": [],\n   \"language\": \"python\",\n   \"notebookMetadata\": {\n    \"pythonIndentUnit\": 4\n   },\n   \"notebookName\": \"retriever-evaluation-tutorial\",\n   \"widgets\": {}\n  },\n  \"kernelspec\": {\n   \"display_name\": \"Python 3 (ipykernel)\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.17\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 1\n}",
        "id": "0e2b943dce02044fb8983a04df453a57"
    },
    {
        "text": "   \"display_name\": \"Python 3 (ipykernel)\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.17\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 1\n}",
        "id": "cbb7975bc1047fa07bffafde27dbd64d"
    },
    {
        "text": "Configuring and Starting the gateway server 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs MLflow AI Gateway (Experimental) Getting Started with MLflow Deployments for LLMs Tutorials and Guides Quickstart Concepts Configuring the gateway server Querying the AI Gateway server Plugin LLM Provider (Experimental) MLflow AI Gateway API Documentation OpenAI Compatibility Unity Catalog Integration gateway server Security Considerations Benefits of the MLflow AI Gateway Explore the Native Providers of the MLflow AI Gateway LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow AI Gateway (Experimental) Getting Started with MLflow Deployments for LLMs Configuring and Starting the gateway server  Configuring and Starting the gateway server  Step 1: Install First, install MLflow along with the genai extras to get access to a range of serving-related\ndependencies, including uvicorn and fastapi . Note that direct dependencies on OpenAI are\nunnecessary, as all supported providers are abstracted from the developer.  pip install 'mlflow[genai]'  Step 2: Set the OpenAI Token as an Environment Variable Next, set the OpenAI API key as an environment variable in your CLI. This approach allows the MLflow AI Gateway to read the sensitive API key safely, reducing the risk\nof leaking the token in code. The gateway server, when started, will read the value set by this environment\nvariable without any additional action required.  export OPENAI_API_KEY = your_api_key_here  Step 3: Configure the gateway server Third, set up several routes for the gateway server to host. The configuration of the gateway server is done through\nediting a YAML file that is read by the server initialization command (covered in step 4). Notably, the gateway server allows real-time updates to an active server through the YAML configuration;\nservice restart is not required for changes to take effect and can instead be done simply by editing the\nconfiguration file that is defined at server start, permitting dynamic route creation without downtime of the service.  endpoints : - name : completions endpoint_type : llm/v1/completions model : provider : openai name : gpt-4o-mini config : openai_api_key : $OPENAI_API_KEY - name : chat endpoint_type : llm/v1/chat model : provider : openai name : gpt-4 config : openai_api_key : $OPENAI_API_KEY - name : chat_3.5 endpoint_type : llm/v1/chat model : provider : openai name : gpt-4o-mini config : openai_api_key : $OPENAI_API_KEY - name : embeddings endpoint_type : llm/v1/embeddings model : provider : openai name : text-embedding-ada-002 config : openai_api_key : $OPENAI_API_KEY  Step 4: Start the Server Fourth, let\u00e2\u0080\u0099s test the gateway server! To launch the gateway server using a YAML config file, use the deployments CLI command. The gateway server will automatically start on localhost at port 5000 , accessible via\nthe URL: http://localhost:5000 . To modify these default settings, use the mlflow gateway start --help command to view additional configuration options.  mlflow gateway start --config-path config.yaml Note MLflow AI Gateway automatically creates API docs. You can validate your deployment server\nis running by viewing the docs. Go to http://{host}:{port} in your web browser. Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "b5dbd587b836555fe7384bbdce48c6f7"
    },
    {
        "text": "Querying endpoints in the MLflow Deployment Server 2.20.0rc0  MLflow MLflow Overview Getting Started with MLflow New Features LLMs Tutorials and Use Case Guides for GenAI applications in MLflow MLflow Tracing MLflow AI Gateway for LLMs MLflow AI Gateway (Experimental) Getting Started with MLflow Deployments for LLMs Tutorials and Guides Quickstart Concepts Configuring the gateway server Querying the AI Gateway server Plugin LLM Provider (Experimental) MLflow AI Gateway API Documentation OpenAI Compatibility Unity Catalog Integration gateway server Security Considerations Benefits of the MLflow AI Gateway Explore the Native Providers of the MLflow AI Gateway LLM Evaluation Prompt Engineering UI Native MLflow Flavors for LLMs LLM Tracking in MLflow MLflow Tracing Model Evaluation Deep Learning Traditional ML Deployment MLflow Tracking System Metrics MLflow Projects MLflow Models MLflow Model Registry MLflow Recipes MLflow Plugins MLflow Authentication Command-Line Interface Search Runs Search Experiments Python API R API Java API REST API Official MLflow Docker Image Community Model Flavors Tutorials and Examples  Contribute Documentation LLMs MLflow AI Gateway (Experimental) Getting Started with MLflow Deployments for LLMs Querying endpoints in the MLflow Deployment Server  Querying endpoints in the MLflow Deployment Server Now that the deployment server is operational, it\u00e2\u0080\u0099s time to send it some data. You can interact with the\ngateway server using the deployments APIs or REST APIs. In this instance, we\u00e2\u0080\u0099ll utilize the deployments APIs for simplicity. Let\u00e2\u0080\u0099s elaborate on the three types of supported models: 1. Completions : This type of model is used to generate predictions or suggestions based on the\ninput provided, helping to \u00e2\u0080\u009ccomplete\u00e2\u0080\u009d a sequence or pattern. 2. Chat : These models facilitate interactive conversations, capable of understanding and responding\nto user inputs in a conversational manner. 3. Embeddings : Embedding models transform input data (like text or images) into a numerical vector\nspace, where similar items are positioned closely in the space, facilitating various machine learning tasks. In the following steps, we will explore how to query the gateway server using these model types.  Example 1: Completions Completion models are designed to finish sentences or respond to prompts. To query these models via the MLflow AI Gateway, you need to provide a prompt parameter,\nwhich is the string the Language Model (LLM) will respond to. The gateway server also accommodates\nvarious other parameters. For detailed information, please refer to the documentation.  from mlflow.deployments import get_deploy_client client = get_deploy_client ( \"http://localhost:5000\" ) name = \"completions\" data = dict ( prompt = \"Name three potions or spells in harry potter that sound like an insult. Only show the names.\" , n = 2 , temperature = 0.2 , max_tokens = 1000 , ) response = client . predict ( endpoint = name , inputs = data ) print ( response )  Example 2: Chat Chat models facilitate interactive conversations with users, gradually accumulating context over time. Creating a chat payload is slightly more complex compared to the other model types since it accommodates an\nunlimited number of messages from three distinct personas: system , user , and assistant . To set up\na chat payload through the MLflow AI Gateway, you\u00e2\u0080\u0099ll need to specify a messages parameter. This parameter\ntakes a list of dictionaries formatted as follows: {\"role\": \"system/user/assistant\", \"content\": \"user-specified content\"} For further details, please consult the documentation. ",
        "id": "1e9fc4b950d2af07627f071874bbc548"
    },
    {
        "text": " from mlflow.deployments import get_deploy_client client = get_deploy_client ( \"http://localhost:5000\" ) name = \"chat_3.5\" data = dict ( messages = [ { \"role\" : \"system\" , \"content\" : \"You are the sorting hat from harry potter.\" }, { \"role\" : \"user\" , \"content\" : \"I am brave, hard-working, wise, and backstabbing.\" }, { \"role\" : \"user\" , \"content\" : \"Which harry potter house am I most likely to belong to?\" } ], n = 3 , temperature = .5 , ) response = client . predict ( endpoint = name , inputs = data ) print ( response )  Example 3: Embeddings Embedding models transform tokens into numerical vectors. To use embedding models via the MLflow AI Gateway, supply a text parameter, which can be a\nstring or a list of strings. The gateway server then processes these strings and returns their\nrespective numerical vectors. Let\u00e2\u0080\u0099s proceed with an example\u00e2\u0080\u00a6  from mlflow.deployments import get_deploy_client client = get_deploy_client ( \"http://localhost:5000\" ) name = \"embeddings\" data = dict ( input = [ \"Gryffindor: Values bravery, courage, and leadership.\" , \"Hufflepuff: Known for loyalty, a strong work ethic, and a grounded nature.\" , \"Ravenclaw: A house for individuals who value wisdom, intellect, and curiosity.\" , \"Slytherin: Appreciates ambition, cunning, and resourcefulness.\" ], ) response = client . predict ( endpoint = name , inputs = data ) print ( response ) And there you have it! You\u00e2\u0080\u0099ve successfully set up your first gateway server and served three OpenAI models. Previous Next \u00a9 MLflow Project, a Series of LF Projects, LLC. All rights reserved.",
        "id": "f3191902e0ea964f9de86bb3d62965e8"
    }
]